%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
%% turn off hyperref patch of \index as sphinx.xdy xindy module takes care of
%% suitable \hyperpage mark-up, working around hyperref-xindy incompatibility
\PassOptionsToPackage{hyperindex=false}{hyperref}

\PassOptionsToPackage{warn}{textcomp}

\catcode`^^^^00a0\active\protected\def^^^^00a0{\leavevmode\nobreak\ }
\usepackage{cmap}
\usepackage{fontspec}
\defaultfontfeatures[\rmfamily,\sffamily,\ttfamily]{}
\usepackage{amsmath,amssymb,amstext}
\usepackage{polyglossia}
\setmainlanguage{english}



\setmainfont{FreeSerif}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Italic,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldItalic
]
\setsansfont{FreeSans}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]
\setmonofont{FreeMono}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]


\usepackage[Bjarne]{fncychap}
\usepackage[,numfigreset=1,mathnumfig]{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}
\addto\captionsenglish{\renewcommand{\contentsname}{Notes}}

\usepackage{sphinxmessages}




\title{MA346 Course Notes}
\date{Dec 17, 2020}
\release{}
\author{Nathan Carter}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{intro::doc}}


These course notes are for \sphinxhref{http://www.bentley.edu}{Bentley University}’s Data Science course (MA346) that will be taught by \sphinxhref{http://nathancarter.github.io}{Nathan Carter} in Fall 2020.

You can download a PDF of these course notes here, in case you prefer to read it that way.

This page summarizes the course schedule.  Each day is a link to the appropriate section in the {\hyperref[\detokenize{course-schedule::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{detailed course schedule with course notes, slides, and all assignments}}}}.


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|}
\hline
\sphinxstyletheadfamily 
Week
&\sphinxstyletheadfamily 
Date
&\sphinxstyletheadfamily 
Content for the day
\\
\hline
1
&
9/3/2020
&
\sphinxhref{course-schedule.html\#week-1-9-3-2020-introduction-and-mathematical-foundations}{Introduction and mathematical foundations}
\\
\hline
2
&
9/10/2020
&
\sphinxhref{course-schedule.html\#week-2-9-10-2020-jupyter-and-a-review-of-python-and-pandas}{Jupyter and a review of Python and pandas}
\\
\hline
3
&
9/17/2020
&
\sphinxhref{course-schedule.html\#week-3-9-17-2020-before-and-after-single-table-verbs}{Before and after, single\sphinxhyphen{}table verbs}
\\
\hline
4
&
9/24/2020
&
\sphinxhref{course-schedule.html\#week-4-9-24-2020-abstraction-and-version-control}{Abstraction and version control}
\\
\hline
5
&
10/1/2020
&
\sphinxhref{course-schedule.html\#week-5-10-1-2020-math-and-stats-in-python-plus-visualization}{Math and stats in Python, plus Visualization}
\\
\hline
6
&
10/8/2020
&
\sphinxhref{course-schedule.html\#week-6-10-8-2020-processing-the-rows-of-a-dataframe}{Processing the Rows of a \sphinxcode{\sphinxupquote{DataFrame}}}
\\
\hline
7
&
10/15/2020
&
\sphinxhref{course-schedule.html\#week-7-10-15-2020-concatenation-and-merging}{Concatenation and Merging}
\\
\hline
8
&
10/22/2020
&
\sphinxhref{course-schedule.html\#week-8-10-22-2020-miscellaneous-munging-methods-etl}{Miscellaneous Munging Methods (ETL)}
\\
\hline
9
&
10/29/2020
&
\sphinxhref{course-schedule.html\#week-9-10-29-2020-dashboards}{Dashboards}
\\
\hline
10
&
11/5/2020
&
\sphinxhref{course-schedule.html\#week-10-11-5-2020-relations-graphs-and-networks}{Relations, graphs, and networks}
\\
\hline
11
&
11/12/2020
&
\sphinxhref{course-schedule.html\#week-11-11-12-2020-relations-as-matrices}{Relations as matrices}
\\
\hline
12
&
11/19/2020
&
\sphinxhref{course-schedule.html\#week-12-11-19-2020-introduction-to-machine-learning}{Introduction to machine learning}
\\
\hline
13
&
11/26/2020
&
Thanksgiving break, no class
\\
\hline
14
&
12/3/2020
&
Final Exam Review and Final Project Workshop
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}


\chapter{Introduction to Data Science}
\label{\detokenize{chapter-1-intro-to-data-science:introduction-to-data-science}}\label{\detokenize{chapter-1-intro-to-data-science::doc}}
See also the slides that summarize a portion of this content.


\section{What is data science?}
\label{\detokenize{chapter-1-intro-to-data-science:what-is-data-science}}
The term “data science” was coined in 2001, attempting to describe a new field.  Some argue that it’s nothing more than the natural evolution of statistics, and shouldn’t be called a new field at all.  But others argue that it’s more interdisciplinary.  For example, in \sphinxstyleemphasis{The Data Science Design Manual} (2017), Steven Skiena says the following.
\begin{quote}

I think of data science as lying at the intersection of computer science, statistics, and substantive application domains. From computer science comes machine learning and high\sphinxhyphen{}performance computing technologies for dealing with scale. From statistics comes a long tradition of exploratory data analysis, significance testing, and visualization. From application domains in business and the sciences comes challenges worthy of battle, and evaluation standards to assess when they have been adequately conquered.
\end{quote}

This echoes a famous blog post by Drew Conway in 2013, called \sphinxhref{http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram}{The Data Science Venn Diagram}, in which he drew the following diagram to indicate the various fields that come together to form what we call “data science.”

\sphinxincludegraphics{{Data_Science_VD}.png}

Regardless of whether data science is just a part of statistics, and regardless of the domain to which we’re applying data science, the goal is the same: \sphinxstylestrong{to turn data into actionable value.}  The professional society INFORMS defines the related field of analytics as “the scientific process of transforming data into insight for making better decisions.”


\section{What do data scientists do?}
\label{\detokenize{chapter-1-intro-to-data-science:what-do-data-scientists-do}}
Turning data into actionable value usually involves answering questions using data.  Here’s a typical workflow for how that plays out in practice.
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Obtain data that you hope will help answer the question.

\item {} 
Explore the data to understand it.

\item {} 
Clean and prepare the data for analysis.

\item {} 
Perform analysis, model building, testing, etc.

(The analysis is the step most people think of as data science, but it’s just one step!  Notice how much more there is that surrounds it.)

\item {} 
Draw conclusions from your work.

\item {} 
Report those conclusions to the relevant stakeholders.

\end{enumerate}

Our course focuses on all the steps \sphinxstyleemphasis{except for} the analysis.  You’ve learned some introductory statistical analysis in one of the course prerequisites (GB213), and we will leverage that.  (Later in our course we will review simple linear regression and hypothesis testing.)  If you have taken other relevant courses in statistics, mathematical modeling, econometrics, etc., and want to bring that knowledge in to use in this course, great, but it’s not a requirement.  Other advanced statistics and modeling courses you take later will essentially plug into step 4 in this data science workflow.


\section{What’s in our course?}
\label{\detokenize{chapter-1-intro-to-data-science:what-s-in-our-course}}
Our course covers the following four foundational aspects of data science.
\begin{itemize}
\item {} 
\sphinxstylestrong{Mathematics:} We will cover foundational mathematical concepts, such as functions, relations, assumptions, conclusions, and abstraction, so that we can use these concepts to define and understand many aspects of data manipulation.  We will also make use of statistics from GB213 (and optionally other statistics courses you may have taken) in course projects, and we will briefly review this material as well.  We will also see small previews of other mathematics and statistics courses and their connections to data science, including graphs for social network analysis, matrices for finding themes in relations, and supervised machine learning.

\item {} 
\sphinxstylestrong{Technology:} We will extend your Python knowledge from CS230 with more advanced table manipulation functions, extended practice with data cleaning and manipulation tasks, computational notebooks (such as Jupyter), and GitHub for version control and project publishing.

\item {} 
\sphinxstylestrong{Visualization:} We will learn new types of plots for a wide variety of data types and what you intend to communicate about them.  We will also study the general principles that govern when and how to use visualizations and will learn how to build and publish interactive online visualizations (dashboards).

\item {} 
\sphinxstylestrong{Communication:} We will study how to write comments in code, documentation for code, motivations in computational notebooks, interpretation of results in computational notebooks, and technical reports about the results of analyses.  We will prioritize clarity, brevity, and knowing the target audience.  Many of these same principles will arise when creating presentations or videos as well.  Each of these modes of communication is required at some point in our course.

\end{itemize}

Details about specific topics and their order appears in the course syllabus, and is summarized on {\hyperref[\detokenize{intro::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{the main page of these course notes}}}}.


\section{Will this course make me a data scientist?}
\label{\detokenize{chapter-1-intro-to-data-science:will-this-course-make-me-a-data-scientist}}
This course is an introduction to data science.  Learning more math, stats, and technology will make you more qualified than just this one course can.  (Bentley University has both a \sphinxhref{https://www.bentley.edu/academics/undergraduate-programs/data-analytics}{Data Analytics major} and a \sphinxhref{https://catalog.bentley.edu/undergraduate/programs/minors-arts-sciences/minor-in-data-technologies/?\_ga=2.60446893.187969798.1590419787-1233038147.1564259120}{Data Technologies minor}, if you’re curious which courses are relevant.)

But there are two focuses of our course that will make a big difference.


\subsection{Learning on your own (LOYO)}
\label{\detokenize{chapter-1-intro-to-data-science:learning-on-your-own-loyo}}
\begin{sphinxadmonition}{note}{Big Picture \sphinxhyphen{} The importance of life\sphinxhyphen{}long learning}

I once heard a director of informatics in the health care industry describe how quickly the field of data science changes by saying, “There aren’t any experts; it’s just who’s the fastest learner.”  For that reason, it’s essential to cultivate the skill of being able to learn new tools and concepts on your own.
\end{sphinxadmonition}

Thus our course requires you to do so.  Twice during the course you must partner up with some classmates to research a topic outside of class (from an extensive list the instructor will provide) and report on it to the class, through writing, presenting, video, or whatever modality makes sense for the content.

If you’re interested in a career in this space, I encourage you to follow data scientists on platforms like Twitter and Medium so that you’re kept abreast of the newest innovations and can learn those that are relevant to your work.


\subsection{Excellent communication}
\label{\detokenize{chapter-1-intro-to-data-science:excellent-communication}}
This was already mentioned earlier, but I will re\sphinxhyphen{}emphasize it here, because of how important it is.  In a meeting between the Bentley University Career Services office and about a dozen employers of our graduates, the employers were asked whether they preferred technical knowledge or what some call “soft skills” and others call “power skills,” which include communication perhaps first and foremost.  Unanimously every employer chose the latter.

\begin{sphinxadmonition}{note}{Big Picture \sphinxhyphen{} The importance of communication}

Data science is about turning data into actionable knowledge.  If a data scientist cannot take the results of their analysis and effectively communicate them to decision makers, they have not turned data into actionable knowledge, and have therefore failed at their goal.  Even if the insights are brilliant, if they are never shared with those who need them, they achieve nothing.  Good communication is essential for data work.
\end{sphinxadmonition}

Consequently our course will contain several opportunities for you to exercise your communication skills and receive feedback from the instructor on doing so.  See the comments under the “communication” bullet above, and the course outline on {\hyperref[\detokenize{intro::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{the main page}}}}.  The first such opportunities appear immediately below.


\section{Where should I start?}
\label{\detokenize{chapter-1-intro-to-data-science:where-should-i-start}}
There are several topics you can investigate on your own that will help you get a leg up in our course.  All of these topics are optional, and in our course are available for teams to investigate outside of class and report back, as described above.

\begin{sphinxadmonition}{note}{Learning on Your Own \sphinxhyphen{} File Explorers and Shell Commands}

On Windows, the file explorer is called Windows Explorer; on Mac, it is called Finder.  It is essential that every computer\sphinxhyphen{}literate person knows how to use these tools.  Most of the actions you can take with your mouse in Windows Explorer or OS X Finder can also be taken using commands at a command prompt.  On Windows, this prompt can be found by running command.exe; on Mac, it can be found in Terminal.app.  It is very useful to know how to do at least basic file manipulation tools with the command prompt, because it enables you to take such action in cloud computing evironments where a file explorer is not always available.
\end{sphinxadmonition}

A report on file explorers and shell commands would address all of the following points.
\begin{itemize}
\item {} 
What the folder tree/hierarchy is

\item {} 
What a file path is and how to express it on both Windows and OS X

\item {} 
From either the file explorer or command prompt:
\begin{itemize}
\item {} 
How to navigate to your home folder

\item {} 
How to move up/down the folder hierarchy by one step

\item {} 
How to copy or move a file

\end{itemize}

\item {} 
From the file explorer:
\begin{itemize}
\item {} 
What happens when you double\sphinxhyphen{}click a file in a file explorer

\item {} 
What file extensions do and when it is acceptable to change them

\end{itemize}

\item {} 
From the command prompt:
\begin{itemize}
\item {} 
How to list all files in the current folder from the command prompt

\item {} 
How to view the contents of a text file

\end{itemize}

\end{itemize}

\begin{sphinxadmonition}{note}{Learning on Your Own \sphinxhyphen{} Python IDEs}

One of the prerequisites for MA346 is CS230, which introduces the Python language.  I assume that you know from that course how to install Python on your own computer and use at least one Python Integrated Development Environment (IDE), such as \sphinxhref{https://www.jetbrains.com/pycharm/}{PyCharm}, \sphinxhref{https://code.visualstudio.com/}{VS Code}, \sphinxhref{https://realpython.com/python-idle/}{IDLE}, \sphinxhref{http://www.pydev.org/}{Eclipse (through PyDev)}, \sphinxhref{https://atom.io/packages/ide-python}{Atom (through the ide\sphinxhyphen{}python package)}, and others.
\end{sphinxadmonition}

A report on Python IDEs would cover:
\begin{itemize}
\item {} 
a list of the most commonly used Python IDEs for data science

\item {} 
very brief installation instructions for each

\item {} 
a comparison of the major features that distinguish these from one another

\item {} 
a list of the features that IDEs have that computational notebooks typically do not have

\item {} 
a demonstration of a few of the most useful distinguishing features of these tools

\end{itemize}

\begin{sphinxadmonition}{note}{Learning on Your Own \sphinxhyphen{} Numerical Analysis}

One valuable contribution that computers make to mathematics is the ability to get excellent approximations to mathematical questions without needing to do extensive by\sphinxhyphen{}hand calculations.  For instance, recall the trapezoidal rule for estimating the result of an integral (covered in the courses MA126 and MA139).  It says that we can estimate the value of
\(\int_a^b f(x)\;dx\)
by computing the area of a sequence of trapezoids.  Choose some points \(x_0,x_1,\ldots,x_n\) evenly spaced between \(a\) and \(b\), with \(x_0=a\) and \(x_n=b\), each one a distance of \(\Delta x\) from the previous.  Then the integral is approximately equal to
\(\frac{\Delta x}{2}\left(f(x_0)+2f(x_1)+2f(x_2)+\cdots+2f(x_{n-1})+f(x_n)\right)\).
\end{sphinxadmonition}

A computational notebook reporting on this numerical technique would cover:
\begin{itemize}
\item {} 
how to implement the trapezoidal rule in Python, given as input some function \sphinxcode{\sphinxupquote{f}}, some real numbers \sphinxcode{\sphinxupquote{a}} and \sphinxcode{\sphinxupquote{b}}, and some positive integer \sphinxcode{\sphinxupquote{n}}

\item {} 
at least one example of how to apply it to a simple mathematical function \sphinxcode{\sphinxupquote{f}} where we know the precise answer from calculus, comparing the result for various values of \sphinxcode{\sphinxupquote{n}}

\item {} 
at least one example of how to apply it to a set of data, when a smooth function \sphinxcode{\sphinxupquote{f}} is not available

\end{itemize}


\chapter{Mathematical Foundations}
\label{\detokenize{chapter-2-mathematical-foundations:mathematical-foundations}}\label{\detokenize{chapter-2-mathematical-foundations::doc}}
See also the slides that summarize a portion of this content.

\begin{sphinxadmonition}{note}{Big Picture \sphinxhyphen{} Functions and relations}

The contents of this page are extremely foundational to the course.  We will be weaving these foundations through every lesson in the course after this one.
\end{sphinxadmonition}


\section{Functions}
\label{\detokenize{chapter-2-mathematical-foundations:functions}}
\sphinxstylestrong{Definition:} A \sphinxstyleemphasis{function} is any method for taking a list of inputs and determining the corresponding output.


\subsection{Examples of functions}
\label{\detokenize{chapter-2-mathematical-foundations:examples-of-functions}}
\sphinxstylestrong{Math:} We can write functions with the usual notation from an algebra or calculus course:
\begin{itemize}
\item {} 
\(f(x)=x^2-5\)

\item {} 
\(g(x,y,z)=\frac{x^2-y^2}{z}\)

\end{itemize}

How is this a method for turning inputs into outputs?  Given an input like \(x=2\), a function like \(f\) can find an output through the usual mechanism of substitution, more commonly called “plugging it in.”  Just substitute \(2\) into \(f(x)=x^2-5\) to get \(f(2)=2^2-5=-1\).  There are also computer programs into which you can type mathematical notation and ask it to apply the function for you.

\sphinxstylestrong{English:} We can write functions in plain English (or any other natural language, but we’ll use English).  To do so, we write a \sphinxstyleemphasis{noun phrase,} and include blanks where the inputs belong:
\begin{itemize}
\item {} 
the capitol of        

\item {} 
the difference in ages between         and        

\end{itemize}

How is this a method for turning inputs into outputs?  Given an input like France, I can substitute it into “the capitol of        ” to get “the capitol of France” and use my knowledge to get Paris.  If it were a capitol I didn’t know, I could use the Internet to find out.

\sphinxstylestrong{Python:} We can write functions in Python (or other programming languages, but this course focuses on Python), like this:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{square} \PYG{p}{(} \PYG{n}{x} \PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{n}{x}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}

\PYG{k}{def} \PYG{n+nf}{is\PYGZus{}a\PYGZus{}long\PYGZus{}word} \PYG{p}{(} \PYG{n}{word} \PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{word}\PYG{p}{)} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{8}
\end{sphinxVerbatim}

How is this a method for turning inputs into outputs?  I can ask Python to do it for me!

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{square}\PYG{p}{(}\PYG{l+m+mi}{50}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
2500
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{is\PYGZus{}a\PYGZus{}long\PYGZus{}word}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Hello}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
False
\end{sphinxVerbatim}

\sphinxstylestrong{Tables:} Any two\sphinxhyphen{}column table can work as a function, if we follow a few conventions.
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
The left column will list the possible inputs to the function.

\item {} 
The right column will list the corresponding outputs.

\item {} 
Each input must show up only once in the table, so there’s no ambiguity about what its corresponding output is.

\end{enumerate}

Here’s an example, which converts Bentley email IDs to real names for a few members of the Mathematical Sciences Department:


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\sphinxstyletheadfamily 
User ID
&\sphinxstyletheadfamily 
Name
\\
\hline
aaltidor
&
Alina Altidor
\\
\hline
mbhaduri
&
Moinak Bhaduri
\\
\hline
wbuckley
&
Winston Buckley
\\
\hline
ncarter
&
Nathan Carter
\\
\hline
lcherveny
&
Luke Cherveny
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

(We could add more names, but it’s just an example.)

How is this a method for turning inputs into outputs?  We use the familiar and fundamental operation of \sphinxstyleemphasis{lookup,} something that shows up in numerous places when working with data.  (We’ll return to the concept of lookup at the end of this chapter.)  Given a User ID as input, we look for it in the first column of the table, and once it’s found, the appropriate output is right next to it in the right column.

\sphinxstylestrong{Others:} Later in the course we will see other ways to represent functions, but the ones above are the most common.


\subsection{Which way is best?}
\label{\detokenize{chapter-2-mathematical-foundations:which-way-is-best}}
The examples above show that you can express functions using math, English, Python, tables, and more.  Although none of these ways is always better than the others, we will typically give functions names and refer to them by those names.  Examples:
\begin{itemize}
\item {} 
In Math:  Rather than writing out \(x=\frac{-b\pm\sqrt{b^2-4ac}}{2a}\) all the time, people just use the short name “the quadratic formula.”

\item {} 
In Python:  The \sphinxcode{\sphinxupquote{def}} keyword in Python is for giving names to functions so that you can use them later by just typing their name.

\end{itemize}


\subsection{Why care about functions?}
\label{\detokenize{chapter-2-mathematical-foundations:why-care-about-functions}}
The concept of a function was invented because it represents an important component of how humans think about the processing of information.  As you’ve seen above, functions show up in ordinary language, in mathematics, in tables of data, and code that processes data.  Even people who don’t do data work use functions unknowingly all the time when they talk about information, as in:
\begin{itemize}
\item {} 
I don’t know all the state capitols. (In other words, I haven’t memorized the function that gives the capitol for a state.)

\item {} 
You better learn your times tables. (In other words, you should memorize the function that gives the product of two small whole numbers.)

\item {} 
What’s Kayla’s phone number? (In other words, please apply the phone\sphinxhyphen{}number\sphinxhyphen{}of\sphinxhyphen{}person function to Kayla for me.)

\end{itemize}

Unsurprisingly, functions show up all over the place in data science.  In particular, when working with a pandas DataFrame, we use functions often to summarize columns (such as compute the max, min, or mean) or to compute new columns, as in this example using Python’s built in \sphinxcode{\sphinxupquote{/}} function:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Per capita cost}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Cost}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{/} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Population}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\end{sphinxVerbatim}


\section{Writing functions in Python}
\label{\detokenize{chapter-2-mathematical-foundations:writing-functions-in-python}}
In MA346, we’ll almost always want the functions we use to be written in Python, so that we can run them on data.  Let’s practice writing some functions in Python.

\begin{sphinxadmonition}{note}{Exercise 1 \sphinxhyphen{} from mathematics}

Write a function \sphinxcode{\sphinxupquote{solve\_quadratic}} that takes as input three real numbers \(a\), \(b\), and \(c\), and gives as output a list of all real number solutions to the equation \(ax^2+bx+c=0\).  (It can return an empty list if there are no real number solutions.)

Example: \sphinxcode{\sphinxupquote{solve\_quadratic(1,0,\sphinxhyphen{}4)}} would yield \sphinxcode{\sphinxupquote{{[}\sphinxhyphen{}2,2{]}}} because \(1x^2+0x+(-4)=0\) is the same equation as \(x^2=4\).
\end{sphinxadmonition}

The above exercise requires only the basic arithmetic built into Python, but when we do more advanced mathematics and statistics, we will import tools like \sphinxhref{https://numpy.org/}{numpy} and \sphinxhref{https://www.scipy.org/}{scipy}.

\begin{sphinxadmonition}{note}{Exercise 2 \sphinxhyphen{} from English}

Write a function \sphinxcode{\sphinxupquote{last\_closing\_price}} that takes as input a NYSE ticker symbol and gives as output the price of one share at the last closing time of the NYSE.  Hints:
\begin{itemize}
\item {} 
The URL \sphinxurl{https://finance.yahoo.com/quote/GOOG} gives data for Alphabet, Inc.  A similar URL works for any ticker symbol.

\item {} 
You can extract all tables from a web page as pandas DataFrames as follows:

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{data\PYGZus{}frames} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}html}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{put the URL here}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
That page has only one table, so it will be \sphinxcode{\sphinxupquote{data\_frames{[}0{]}}}.

\end{itemize}

Example: \sphinxcode{\sphinxupquote{last\_closing\_price('GOOG')}} yielded something like \sphinxcode{\sphinxupquote{1465.85}} in mid\sphinxhyphen{}June 2020.
\end{sphinxadmonition}

It is not always guaranteed that you can turn an idea expressed in English, like “look up the last closing price of a stock,” into Python code.  For instance, no one knows how to write code that answers the question, “given a digital photo as input, return the year in which the photo was taken.”  But coming up with creative ways to answer important questions in code is a very valuable skill we will work to develop.

\begin{sphinxadmonition}{note}{Exercise 3 \sphinxhyphen{} from a table}

Write a function \sphinxcode{\sphinxupquote{country\_capitol}} that takes as input a string containing a country name and gives as output a string containing the name of the country’s capitol.  Hints:
\begin{itemize}
\item {} 
A list of countries and capitols appears here: \sphinxurl{https://www.boldtuesday.com/pages/alphabetical-list-of-all-countries-and-capitals-shown-on-list-of-countries-poster}

\item {} 
To convert two columns of a pandas DataFrame into a Python \sphinxcode{\sphinxupquote{dict}} for easy lookup, try the following.

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{D} \PYG{o}{=} \PYG{n+nb}{dict}\PYG{p}{(} \PYG{n+nb}{zip}\PYG{p}{(} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{input column name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{output column name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{p}{)} \PYG{p}{)}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
You can then look items up using \sphinxcode{\sphinxupquote{D{[}item\_to\_look\_up{]}}}, as in \sphinxcode{\sphinxupquote{D{[}'ZIMBABWE'{]}}}.

\end{itemize}

Example: \sphinxcode{\sphinxupquote{country\_capitol('JORDAN')}} would yield \sphinxcode{\sphinxupquote{'AMMAN'}}.
\end{sphinxadmonition}

Why do you think the \sphinxcode{\sphinxupquote{dict(zip())}} trick given above works?  What exactly is it doing?


\section{Terminology}
\label{\detokenize{chapter-2-mathematical-foundations:terminology}}
The following terminology is used throughout computing when discussing functions.

\sphinxstylestrong{Definition:} A \sphinxstyleemphasis{data type} is a category of values.

For instance, \sphinxcode{\sphinxupquote{int}} is a Python data type for integers (that is, positive and negative whole numbers).  Each number is a value in that data type.  Other Python data types include \sphinxcode{\sphinxupquote{bool}} (with the values \sphinxcode{\sphinxupquote{True}} and \sphinxcode{\sphinxupquote{False}}), \sphinxcode{\sphinxupquote{str}} (short for “string” and containing text), and more.

\sphinxstylestrong{Definition:} A function’s \sphinxstyleemphasis{input type} is the type of values you can pass as inputs when calling the function.  If a function has multiple inputs, we might speak of its \sphinxstyleemphasis{input types} instead.

In Python, we are not required to write the input types of functions into our code, so we can only know them by reading a function’s documentation or by inspecting the function’s code and reasoning it out.

For example, the \sphinxcode{\sphinxupquote{square}} function defined above probably has input type \sphinxcode{\sphinxupquote{float}} (any number).  The \sphinxcode{\sphinxupquote{is\_a\_long\_word}} function has input type \sphinxcode{\sphinxupquote{str}}.

\sphinxstylestrong{Definition:} A function’s \sphinxstyleemphasis{output type} is the type of values the function returns as outputs.  Not all functions have a single return type, but many do.

For example, the \sphinxcode{\sphinxupquote{square}} function always produces a \sphinxcode{\sphinxupquote{float}} output and the \sphinxcode{\sphinxupquote{is\_a\_long\_word}} function alwayds produces a \sphinxcode{\sphinxupquote{bool}} output.

These ides of input type and output type are a bit related to the ideas of domain and range of functions in mathematics, but they are not precisely the same.  The difference is not important here.

\sphinxstylestrong{Definition:} A function is sometimes called a \sphinxstyleemphasis{map} from its input type to its output type.  We say that a function \sphinxstyleemphasis{maps} its inputs to its outputs.

For instance, the \sphinxcode{\sphinxupquote{is\_a\_long\_word}} function maps strings to booleans.

\sphinxstylestrong{Definition:} A function that takes a single input is called a \sphinxstyleemphasis{unary} function.  If it takes two inputs, it is a \sphinxstyleemphasis{binary} function.  If it takes three inputs it is a \sphinxstyleemphasis{ternary} function.  The number of inputs is called the \sphinxstyleemphasis{arity} of the function.

Although there are related words that go beyond three inputs (quaternary!) almost nobody uses them; instead, we would probably just say “a four\sphinxhyphen{}parameter function.”


\section{Relations}
\label{\detokenize{chapter-2-mathematical-foundations:relations}}
\sphinxstylestrong{Definition:} A \sphinxstyleemphasis{relation} is a function whose output type is \sphinxcode{\sphinxupquote{bool}}, that is, the outputs are always either true or false.


\subsection{Examples of relations}
\label{\detokenize{chapter-2-mathematical-foundations:examples-of-relations}}
\sphinxstylestrong{Math:} Any equation or inequality in mathematics is a relation, such as \(x^2+y^2\ge z^2\) or \(x\ge 0\).

Consider \(x\ge 0\).  Given any input of the appropriate type, say \(x=15\), we can determine a true or false value by substitution.  In this case, substituting \(x=15\) into \(x\ge0\) gives \(15\ge0\), which we know is true.  We could do a similar thing with \(x^2+y^2\ge z^2\) if given three numerical inputs instead of just one.

\sphinxstylestrong{English:} Any declarative sentence with blanks in it is a relation, such as “        is the capitol of        ” or “        is a fruit.”

Given any input, you can use it to fill in the blank in the sentence and then judge (using your ordinary knowledge of the world and English) whether the sentence is true.  For instance, if we’re working with the sentence “        is a fruit” and I provide the input “Python,” then I get the sentence “Python is a fruit,” which is obviously false, because it’s a programming language, not a fruit.

\sphinxstylestrong{Python:} Any Python function with output type \sphinxcode{\sphinxupquote{bool}} is a relation.

You can evaluate such relations by running them in Python, just as we did with functions earlier.  In fact, the \sphinxcode{\sphinxupquote{is\_a\_long\_word}} function from earlier is not only a function, but also a relation.  Here are two other examples:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{R} \PYG{p}{(} \PYG{n}{a}\PYG{p}{,} \PYG{n}{b} \PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{n}{a} \PYG{o+ow}{in} \PYG{n}{b}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{p}{]}

\PYG{k}{def} \PYG{n+nf}{is\PYGZus{}a\PYGZus{}primary\PYGZus{}color} \PYG{p}{(} \PYG{n}{c} \PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{n}{c} \PYG{o+ow}{in} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{green}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{blue}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\end{sphinxVerbatim}

Although the first relation is an example with no clear purpose, the second one has a clear meaning.  We can test it out like so:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{is\PYGZus{}a\PYGZus{}primary\PYGZus{}color}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{blue}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}\PYG{p}{,} \PYG{n}{is\PYGZus{}a\PYGZus{}primary\PYGZus{}color}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{orange}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(True, False)
\end{sphinxVerbatim}

\sphinxstylestrong{Lists:} A very common way of defining a relation is to just list all the inputs for which the relation is true, and then we know that everything else makes it false.

In data science, we often do this using tables.  For example, consider the table on the webpage mentioned in Exercise 3, above.  That table lists all the pairs of inputs that make the “        is the capitol of        ” relation true.

If you want to check whether, for example, “Bangalore is the capitol of India” is true, you can look to see if any row of the table is \sphinxcode{\sphinxupquote{('India','Bangalore')}}.  Since there is no such row, the relation is false for that input.  (The capitol is actually New Delhi.)

\begin{sphinxadmonition}{note}{Big Picture \sphinxhyphen{} Every table represents a relation.}

Every table is a relation.  Each row represents a set of inputs that would make the relation true, and any inputs that don’t appear as a row in the table make it false.

Thus every pandas DataFrame is a relation, every SQL table is a relation, and every table you see printed in a book or on a webpage is a relation.  This is why SQL is the language for querying \sphinxstyleemphasis{relational} databases.
\end{sphinxadmonition}

The above big picture concept is almost 100\% true.  Technically, a pandas DataFrame or an SQL table can have repeated rows, which is unnecessary if you’re defining a relation.  And technically pandas DataFrames and SQL tables also have an extra layer of data called the “index” which we’re ignoring for now, just concentrating on the contents of the table’s columns.

\sphinxstylestrong{Others:} Later in the class we’ll see even other ways to represent functions.


\subsection{Which way is best?}
\label{\detokenize{chapter-2-mathematical-foundations:id1}}
Although we can express relations in all the ways just mentioned—in math, English, Python, or with lists—we typically \sphinxstyleemphasis{talk about} relations by using simple phrases.  For instance, it’s awkward to say “the ‘        is a fruit’ relation,” so I would probably instead say something like “being a fruit.”  And instead of \(x<y\), I might say something like “the usual less\sphinxhyphen{}than relation for numbers.”

Sometimes we just use the central phrase to describe a binary relation.  So to discuss the “        has more employees than        ” relation, I might just use the phrase “has more employees than” when talking about it, or perhaps just “more employees.”  Usually it’s clear what we mean.


\subsection{Why care about relations?}
\label{\detokenize{chapter-2-mathematical-foundations:why-care-about-relations}}
The mathematical concept of a relation was invented because humans use it all the time when we think and speak, even though we don’t precisely define it in everyday life.  Every time we say a declarative sentence, this idea comes up.  Here are some examples:
\begin{itemize}
\item {} 
If I say, “George isn’t friends with Mia,” then I’m relying on your familiarity with the being\sphinxhyphen{}friends\sphinxhyphen{}with relation, which you’ve known since Kindergarten.

\item {} 
If I say, “Dell acquired EMC in 2015,” then I’m relying on your familiarity with the “acquired” relation among companies, which you might not have been very familiar with before coming to Bentley.

\end{itemize}

The above examples are from binary relations, which are possibly the most common type.  Just as a function can be binary (that is, take two inputs), so can a relation, because it’s just a special type of function.  But of course we can have unary functions as well (taking one input only), like the \sphinxcode{\sphinxupquote{is\_a\_long\_word}} and \sphinxcode{\sphinxupquote{is\_a\_primary\_color}} examples above, and we can have relations with three or more inputs as well.

A very important use of relations in data science is for \sphinxstyleemphasis{filtering} a dataset.  We often want to focus our attention on just the section of a dataset we’re interested in, which we describe as “filtering” to keep the rows we want (or “filtering out” the rows we don’t want).  In pandas, you can select a subset of a DataFrame \sphinxcode{\sphinxupquote{df}} and return it as a new DataFrame (or, rather, a view on the original), like so:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} To filter the rows of a DataFrame, index the DataFrame with the relation:}
\PYG{n}{df}\PYG{p}{[}\PYG{n}{put\PYGZus{}any\PYGZus{}relation\PYGZus{}here}\PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} Here\PYGZsq{}s an example, which uses the \PYGZgt{}= relation to filter for adults:}
\PYG{n}{df}\PYG{p}{[}\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{age}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{\PYGZgt{}}\PYG{o}{=} \PYG{l+m+mi}{18}\PYG{p}{]}
\end{sphinxVerbatim}


\section{Relations and functions in data}
\label{\detokenize{chapter-2-mathematical-foundations:relations-and-functions-in-data}}
\begin{sphinxadmonition}{note}{Exercise 4 \sphinxhyphen{} food inspections}

The table below shows a sample of data taken from \sphinxhref{https://data.world/cityofchicago/food-inspections}{a larger dataset on data.world about Chicago city food inspections}.  Imagine the entire dataset of over 150,000 rows based on the sample of the first 10 rows shown below.
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Name at least two relations expressed by the contents of this table.  (You need not use all the columns.)

\item {} 
What are the input types, output type, and arity of each of your relations?

\item {} 
Does the table contain any sets of columns that define a function?

\item {} 
If so, what are the input types, output type, and arity of the function(s)?

\end{enumerate}
\end{sphinxadmonition}


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
Business
&\sphinxstyletheadfamily 
Address
&\sphinxstyletheadfamily 
Inspection Date
&\sphinxstyletheadfamily 
Inspection Type
&\sphinxstyletheadfamily 
Results
\\
\hline
ZAM ZAM MIDDLE EASTERN GRILL
&
3461 N CLARK ST
&
11/07/2017
&
Complaint
&
Pass
\\
\hline
SPINZER RESTAURANT
&
2331 W DEVON AVE
&
11/07/2017
&
Complaint Re\sphinxhyphen{}Inspection
&
Pass
\\
\hline
THAI THANK YOU RICE \& NOODLES
&
3248 N LINCOLN AVE
&
11/07/2017
&
License Re\sphinxhyphen{}Inspection
&
Pass
\\
\hline
SOUTH OF THE BORDER
&
1416 W MORSE AVE
&
11/07/2017
&
License
&
Pass
\\
\hline
BEAVERS COFFEE \& DONUTS
&
131 N CLINTON ST
&
11/07/2017
&
License
&
Not Ready
\\
\hline
BEAVERS COFFEE \& DONUTS
&
131 N CLINTON ST
&
11/07/2017
&
License
&
Not Ready
\\
\hline
BEAVERS COFFEE \& DONUTS
&
131 N CLINTON ST
&
11/07/2017
&
License
&
Not Ready
\\
\hline
FAT CAT
&
4840 N BROADWAY
&
11/07/2017
&
Complaint Re\sphinxhyphen{}Inspection
&
Pass
\\
\hline
SAFARI SOMALI CUISINE
&
6319 N RIDGE AVE
&
11/07/2017
&
License
&
Fail
\\
\hline
DATA RESTAURANT
&
2306 W DEVON AVE
&
11/06/2017
&
Complaint
&
Out of Business
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\begin{sphinxadmonition}{note}{Exercise 5 \sphinxhyphen{} tech companies}

The table below shows a sample of data taken from \sphinxhref{https://data.world/datanerd/2016-technology-fast-500}{a larger dataset on data.world about the 2016 Technology Fast 500}.  Imagine the entire dataset of 500 rows based on the sample of the first 10 rows shown below.
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Name at least two relations expressed by the contents of this table.  (You need not use all the columns.)

\item {} 
What are the input types, output type, and arity of each of your relations?

\item {} 
Does the table contain any sets of columns that define a function?

\item {} 
If so, what are the input types, output type, and arity of the function(s)?

\end{enumerate}
\end{sphinxadmonition}


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
CEO Name
&\sphinxstyletheadfamily 
City
&\sphinxstyletheadfamily 
Company Name
&\sphinxstyletheadfamily 
Country
&\sphinxstyletheadfamily 
Market
&\sphinxstyletheadfamily 
State
\\
\hline
Charles Deguire
&
Boisbriand
&
Kinova Inc.
&
Canada
&
Canada
&
QC
\\
\hline
Greg Malpass
&
Burnaby
&
Traction on Demand
&
Canada
&
Canada
&
BC
\\
\hline
Jack Newton
&
Burnaby
&
Clio
&
Canada
&
Canada
&
BC
\\
\hline
Jory Lamb
&
Calgary
&
VistaVu Solutions Inc.
&
Canada
&
Canada
&
AB
\\
\hline
Wayne Sim
&
Calgary
&
Enersight
&
Canada
&
Canada
&
AB
\\
\hline
Bryan de Lottinville
&
Calgary
&
Benevity, Inc.
&
Canada
&
Canada
&
AB
\\
\hline
J. Paul Haynes
&
Cambridge
&
eSentire
&
Canada
&
Canada
&
ON
\\
\hline
Jason Flick
&
Kanata
&
You.i TV
&
Canada
&
Canada
&
ON
\\
\hline
Matthew Rendall
&
Kitchener
&
Clearpath
&
Canada
&
Canada
&
ON
\\
\hline
Dan Latendre
&
Kitchener
&
Igloo Software
&
Canada
&
Canada
&
ON
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}


\section{Some technical notes}
\label{\detokenize{chapter-2-mathematical-foundations:some-technical-notes}}

\subsection{Connections between functions and relations}
\label{\detokenize{chapter-2-mathematical-foundations:connections-between-functions-and-relations}}
As you’ve probably noticed, there are some close relationships between relations and functions.  Let’s state them explicitly.
\begin{itemize}
\item {} 
Our definitions say that a relation is \sphinxstyleemphasis{a special kind of function;} that is, it’s one whose output type has to be bool.  So every relation is really also a function.

\item {} 
But in the last two exercises, we’ve been thinking about relations and functions in tables.  There we saw that we can think of a function as \sphinxstyleemphasis{a special kind of relation;} that is, it’s one in which one column has all unique values, so that it can be used for input lookup in an unambiguous way.

\end{itemize}


\subsection{Applying functions and relations}
\label{\detokenize{chapter-2-mathematical-foundations:applying-functions-and-relations}}
This idea of “input lookup” is called \sphinxstyleemphasis{applying} a function.  For example, we apply the \sphinxcode{\sphinxupquote{country\_capitol}} function by looking up the country in the table and giving the corresponding capitol as output.

But we can actually do lookup in a relation as well, as long as we don’t mind the possibility of getting more than one output.  For instance, if we use the Technology Fast 500 table shown above and look up a city name, and ask for the corresponding company name, we won’t always get just one answer.  Even in just the small sample of the data we have, we can see that Calgary houses at least three different companies.

In short, functions let you apply them and get a unique answer, while relations let you apply them and get any number of answers.


\subsection{Inverses}
\label{\detokenize{chapter-2-mathematical-foundations:inverses}}
As mentioned above, a function is a relation in which \sphinxstyleemphasis{for each} input, \sphinxstyleemphasis{there is exactly one} output.  But for \sphinxstyleemphasis{some} functions, the reverse is also true:  For each \sphinxstyleemphasis{output}, there is exactly one \sphinxstyleemphasis{input.}

For example, consider the Technology Fast 500 table again, and let’s assume that each company and CEO name is unique (i.e., there are not two CEOs name Jack Newton, or two companies named Clearpath, etc.).  Consider the function that maps a company name to the corresponding CEO name; let’s call it \sphinxcode{\sphinxupquote{find\_ceo\_for\_company}}.
\begin{itemize}
\item {} 
As with every function, for each input company, there is exactly one CEO output.

\item {} 
But in this case, also, for each CEO output, there is exactly one input company.

\end{itemize}

While we chose to use the company as input and provide the CEO name as output, we could also have done it in the other order.  That is, we could have created a function \sphinxcode{\sphinxupquote{find\_company\_for\_ceo}} that takes a CEO name as input and provides the corresponding company name as output.  It just depends on which column you chose to use as the input and which you choose to use as the output.

This concept is probably familiar from mathematics, where we speak of \sphinxstyleemphasis{inverting} a function.  In mathematical notation, we write the inverse of \(f\) as \(f^{-1}\), but in computing, we can use more descriptive names, like the example of \sphinxcode{\sphinxupquote{find\_ceo\_for\_company}} and \sphinxcode{\sphinxupquote{find\_company\_for\_ceo}}.

In summary:  For a relation to be a function, it has to provide just one output for each input.  For it to be invertible, it has to have just one input for each output.


\section{An extremely common data operation: Lookup}
\label{\detokenize{chapter-2-mathematical-foundations:an-extremely-common-data-operation-lookup}}
When working with data and writing code, we “look up” values in many different ways.  We’ve already discussed above how applying a function expressed in a table is done by looking up the input and finding the corresponding output.

Let’s review the most common ways that lookup operations show up in Python coding.  Almost all of them use square brackets, because that’s the common coding notation for looking up an item in a larger structure.
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
If we have a Python list \sphinxcode{\sphinxupquote{L}} then we can look up the fourth item in it using the syntax \sphinxcode{\sphinxupquote{L{[}3{]}}}, for example.  In this way, you can think of a list as a function from numbers to the contents of the list.

\item {} 
If we have a Python dictionary \sphinxcode{\sphinxupquote{D}} then we can look up an item in it using the syntax \sphinxcode{\sphinxupquote{D{[}my\_item{]}}}.  So a dictionary is very much like a function; it maps its keys to their corresponding values.

\item {} 
If we have a pandas DataFrame, there are many ways to look up items in it, including:
\begin{itemize}
\item {} 
filtering for just some rows, as discussed earlier, using syntax like \sphinxcode{\sphinxupquote{df{[}df.X==Y{]}}}, and then selecting the column to use as the result, as in \sphinxcode{\sphinxupquote{df{[}df.Name=='Smith'{]}.Employer}}

\item {} 
choosing one or more rows and/or columns by their names, using \sphinxcode{\sphinxupquote{df.loc{[}rows,cols{]}}}, as in \sphinxcode{\sphinxupquote{df.loc{[}'May':'June','Rainfall'{]}}}

\item {} 
choosing one or more rows and/or columns by their zero\sphinxhyphen{}based index, using \sphinxcode{\sphinxupquote{df.iloc{[}rows,cols{]}}}, as in \sphinxcode{\sphinxupquote{df.iloc{[}:,5{]}}}

\end{itemize}

\end{enumerate}

Some of the lookup operations shown above act like functions and some act like relations.  For instance, a Python list always returns one value when you use square brackets for lookup, so that behaves like a function.  But a pandas DataFrame might yield multiple values when you execute code like \sphinxcode{\sphinxupquote{df{[}df.Name=='Smith'{]}.Employer}}, because there may be many Smiths in the dataset.  If you don’t care about getting \sphinxstyleemphasis{all} the results, but want to just choose one of them, you can always add \sphinxcode{\sphinxupquote{.iloc{[}0{]}}} on the end of the code to select just the first result from the list, as in \sphinxcode{\sphinxupquote{df{[}df.Name=='Smith'{]}.Employer.iloc{[}0{]}}}.

Later in the course we will see that SQL joins (called by various names in pandas, including merge, concat, and join) are highly related to all the lookup concepts just discussed.  A SQL or pandas join is like doing many lookups all at once, which is why it is such a common operation.


\chapter{Jupyter}
\label{\detokenize{chapter-3-jupyter:jupyter}}\label{\detokenize{chapter-3-jupyter::doc}}
See also the slides that summarize a portion of this content.


\section{What’s Jupyter?}
\label{\detokenize{chapter-3-jupyter:what-s-jupyter}}
The Jupyter project makes it possible to use code to experiment with and process data in your web browser.  It lets you do all of these things in one page (or browser tab):
\begin{itemize}
\item {} 
write and run code

\item {} 
write explanations of code and data, including with mathematical formulas

\item {} 
view tables, plots, and other visualizations of data

\item {} 
interact with certain types of data visualizations

\end{itemize}

It’s pronounced just like Jupiter, but has the funny spelling because it was originally built for Python, so they wanted to work a “py” in there somewhere.

You may prefer to use another tool to accomplish these tasks; your MA346 instructor won’t force you to use Jupyter.  But you should still know about Jupyter for the following reasons:
\begin{itemize}
\item {} 
Lots of people in data science and analytics use Jupyter notebooks, so you’ll definitely encounter them and want to be familiar with how to read them, edit them, and run them.

\item {} 
It’s becoming the \sphinxstyleemphasis{lingua franca} for how to share your data research online, so you may want to know how to publish Jupyter notebooks, something our course will cover.

\item {} 
It was a big enough deal to win one of the highest awards in the computer science profession, the \sphinxhref{https://blog.jupyter.org/jupyter-receives-the-acm-software-system-award-d433b0dfe3a2}{2017 ACM Software System Award}.

\end{itemize}

In fact, these course notes were written in Jupyter.  That’s why you’ll see code inputs and outputs interspersed among them, because Jupyter lets you write documents with code built in, and it runs the code for you and shows you the output.  Here’s an example:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(} \PYG{p}{[}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{9}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{chapter-3-jupyter_2_0}.png}

Okay, sounds great, so where do we point our browser to start using this thing?  Well, you’ve got lots of options, so let’s see what they are first.


\section{How does Jupyter work?}
\label{\detokenize{chapter-3-jupyter:how-does-jupyter-work}}
\begin{sphinxadmonition}{note}{Big Picture \sphinxhyphen{} The structure of Jupyter}

Jupyter is made of two pieces:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
The notebook interface, which shows you a document with code and visualizations in it, called “a Jupyter notebook.”

\item {} 
The engine behind the notebook, which runs your code, and is doing its work invisibly in the background; this engine is called the “kernel.”

\end{enumerate}

How you interact with each of these two pieces is important, and comes with some pitfalls to avoid.
\end{sphinxadmonition}


\subsection{Jupyter in the cloud}
\label{\detokenize{chapter-3-jupyter:jupyter-in-the-cloud}}
The easiest way to start using Jupyter is to just point your browser at a website that offers you access to Jupyter in the cloud.  In such a situation, Jupyter’s two pieces work like this:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
The notebook interface runs in your browser on your computer

\item {} 
The kernel runs in the cloud on a server provided by someone else

\end{enumerate}

Here are three examples of where you can use Jupyter notebooks in the cloud:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Best choice: \sphinxhref{https://deepnote.com/}{Deepnote}\sphinxhref{https://deepnote.com/}{}
\begin{itemize}
\item {} 
They took the standard Jupyter interface and added more goodies

\item {} 
You can read and write files to/from your Google Drive

\item {} 
You can use multiple languages (though we’ll stick to Python)

\item {} 
The amount of computing power they give you for free is pretty good

\item {} 
It’s extremely easy to share your work with anyone

\item {} 
But you have access to Deepnote only because I signed our class up; they’re a startup and they’re not letting everyone in yet

\end{itemize}

\item {} 
Next best choice: \sphinxhref{https://colab.research.google.com/}{Google Colab}\sphinxhref{https://colab.research.google.com/}{}
\begin{itemize}
\item {} 
All the same positives as Deepnote except without as many interface goodies

\item {} 
Supports more languages than Deepnote does, but that’s not relevant in MA346

\end{itemize}

\item {} 
Third choice: \sphinxhref{https://cocalc.com/}{CoCalc}\sphinxhref{https://cocalc.com/}{}
\begin{itemize}
\item {} 
Many features that the previous two don’t have, including a nice palette of common code snippets you can insert

\item {} 
But the notebook interface is nonstandard and different from Jupyter’s in several ways

\item {} 
Perhaps the most limited in terms of how much computing you get for free, though this is not very important for MA346

\end{itemize}

\end{enumerate}

Obviously, none of these is going to give you access to a supercomputer for free.  If you want to do any intense or lengthy computing in the cloud, you have to pay for them to let the kernel you’re using run on big hardware.


\subsection{Jupyter on your machine}
\label{\detokenize{chapter-3-jupyter:jupyter-on-your-machine}}
You can also choose to run Jupyter on your own machine.  In contrast to accessing Jupyter in the cloud, when you run it on your own machine, Jupyter’s two pieces work like this:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
The notebook interface still runs in your browser on your computer

\item {} 
The kernel now also runs on your computer, which has both advantages and disadvantages

\end{enumerate}

Let’s consider the major tradeoffs in each of these approaches.


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\sphinxstyletheadfamily 
Why put Jupyter on my computer?
&\sphinxstyletheadfamily 
Why choose the cloud instead?
\\
\hline
1. Not limited by how much power a cloud company will give you for free.
&
1. You don’t have to install anything on your computer.
\\
\hline
2. Even if I don’t have good wifi access, I can still use it.
&
2. You can use it on a phone/tablet.
\\
\hline
3. May be easier to add specific Python packages you need for your work.
&
3. Avoid accidentally leaving a kernel running invisibly. (See below.)
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

If you want to go this route, there are several ways to install Jupyter on your machine.

\sphinxstylestrong{Easiest way:}  {\hyperref[\detokenize{anaconda-installation::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Install Anaconda}}}}
\begin{itemize}
\item {} 
This is by far the easiest method, so start here.

\item {} 
Follow the link above for detailed instructions within these course notes; it is not necessary to also install VS Code, which the instructions make optional.

\item {} 
Once Anaconda is installed, you can launch it from the Windows Start menu or Mac Applications folder, then choose to launch either Jupyter Lab or the Jupyter Notebook.

\end{itemize}

Before we discuss the other methods of installing Jupyter, let’s discuss the difference between Jupyter Lab and the Jupyter Noteboook.  Here’s a summary:


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\sphinxstyletheadfamily 
Jupyter Notebook
&\sphinxstyletheadfamily 
Jupyter Lab
\\
\hline
The original Jupyter project
&
Its newer successor
\\
\hline
Uses multiple browser tabs
&
Does everything in one tab
\\
\hline
Supports many extensions
&
Doesn’t yet support all extensions
\\
\hline
Has no console/terminal access
&
Has both console and terminal access
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

Both technologies let you edit Jupyter notebooks.  (Yes, it’s confusing that one app is called “the Jupyter Notebook” and the files are also called “Jupyter notebooks.”  Sorry.)

\begin{sphinxadmonition}{note}{Big Picture \sphinxhyphen{} How to shut down Jupyter}

When you launch either the Jupyter Notebook or Jupyter Lab, you launch both the user interface (which you see in your browser) and the kernel (which you don’t!).  \sphinxstylestrong{Just closing the browser tab DOES NOT CLOSE THE KERNEL.}  Doing this repeatedly (e.g., each day in class) will clog up your computer with many kernels running invisibly in the background.

Instead, do one of these things EVERY TIME you’re done coding:
\begin{itemize}
\item {} 
In Jupyter Notebook: File Menu > Close and Halt

\item {} 
In Jupyter Lab: File Menu > Shut down

\end{itemize}

These close the (invisible) kernel first, then let you close the user interface after that.
\end{sphinxadmonition}

But that’s a hassle!  Wouldn’t it be easier if Jupyter were just an app I could run on my machine, like every other app?  In fact, because Jupyter is \sphinxstyleemphasis{not} an app, you can’t even double\sphinxhyphen{}click Jupyter notebook files (which end with the \sphinxcode{\sphinxupquote{.ipynb}} extension) and have them automatically open in Jupyter.  Another hassle!  How can we fix these things?

\sphinxstylestrong{Another option:}  Install \sphinxhref{https://nteract.io/}{nteract} (pronounced “interact”)
\begin{itemize}
\item {} 
This assumes you have a working Python installation.  The easiest way to do that is to install Anaconda, using the instructions up above.  That’s why this one is listed second; it assumes you’ve done that first.

\item {} 
Then visit the website linked to above and follow the very easy process of installing the nteract app.

\item {} 
When you run nteract, it shows you a new, blank Jupyter notebook.  It has already launched the invisible kernel behind the scenes for you.  (No need to go to Anaconda Navigator first!)

\item {} 
You can also double\sphinxhyphen{}click notebooks to open them in nteract.  Easy, just like every other app on your machine.

\item {} 
When you quit nteract, it quits not only the user interface you see, but the invisible kernel as well.  Nothing to remember.

\end{itemize}

The only disadvantage here is that some Jupyter notebook extensions don’t work in nteract.  But we won’t be using many of those in MA346 anyway.


\section{Closing comments}
\label{\detokenize{chapter-3-jupyter:closing-comments}}
There are many websites that make it easy to view Jupyter notebooks online.  This is very useful for sharing the results of your work when you’re done.  Examples include \sphinxhref{https://nbviewer.jupyter.org/}{NbViewer} and \sphinxhref{https://github.com/}{GitHub}, but there are others.  Notebooks are often shared in nerdy places on the Internet, with websites supporting viewing them with all their plots, tables, and math displayed nicely.  We will learn how to use GitHub in a future week.

There are various pros and cons to using Jupyter notebooks vs. plain old Python scripts, as you probably did in CS230.  There are also some hybrid technologies that exist to make notebooks more like scripts, or scripts more like notebooks (such as \sphinxhref{https://papermill.readthedocs.io/en/latest/}{Papermill}, \sphinxhref{https://code.visualstudio.com/docs/python/jupyter-support}{VSCode notebook support}, and others).  In this class, you can usually use whatever technology you prefer.  The instructor will use notebooks because they are good for communicating, and communicating is your instructor’s job.

\begin{sphinxadmonition}{note}{Learning on Your Own \sphinxhyphen{} Problems with Notebooks}

Some folks \sphinxhref{https://twitter.com/joelgrus/status/1033035196428378113?lang=en}{really don’t like Jupyter notebooks}.  And they have good points!  Study what pitfalls notebooks have, based on the presentation at that link, and report on them to the class.
\end{sphinxadmonition}

Such a report would include:
\begin{itemize}
\item {} 
From the many problems the presentation lists, choose the 4\sphinxhyphen{}6 that are most relevant to MA346 students.

\item {} 
For each such problem:
\begin{itemize}
\item {} 
Explain it carefully.

\item {} 
Show how a tool other than Jupyter doesn’t have the same problem.

\item {} 
Suggest specific ways that MA346 students can avoid pitfalls surrounding that problem.

\end{itemize}

\end{itemize}

\begin{sphinxadmonition}{note}{Learning on Your Own \sphinxhyphen{} Math in Notebooks}

You can add mathematics to Jupyter notebooks and it looks very nice.  Here’s an example of the quadratic formula:
\begin{equation*}
\begin{split}x=\frac{-b\pm\sqrt{b^2-4ac}}{2a}\end{split}
\end{equation*}
This can be useful for explaining mathematical and statistical concepts in your work clearly, without resorting to ugly text attempts to look sort of like math.
\end{sphinxadmonition}

Such a report would include:
\begin{itemize}
\item {} 
An explanation of what a student would type into a Markdown cell to make some simple mathematics

\item {} 
A list of the 5\sphinxhyphen{}10 most common math notation or symbols the student will want to know how to create (particularly those relevant to statistics and/or data science)

\item {} 
Suggestions for where the student can go to learn more symbols or notation if they need it

\end{itemize}


\chapter{Review of Python and pandas}
\label{\detokenize{chapter-4-review-of-python-and-pandas:review-of-python-and-pandas}}\label{\detokenize{chapter-4-review-of-python-and-pandas::doc}}
Unlike most chapters, there are no slides corresponding to this chapter, because they consist mostly of in\sphinxhyphen{}class exercises.  They aim to help you remember the Python and pandas you learned in CS230 and be sure they’re refreshed and at the front of your mind, so that we can build on them in future weeks.


\section{Python review 1: Remembering pandas}
\label{\detokenize{chapter-4-review-of-python-and-pandas:python-review-1-remembering-pandas}}
This first set of exercises works with a database from the U.S. Consumer Financial Protection Bureau.  The dataset recorded all mortgage applications in the U.S. in 2018, over 15 million of them.  Here we will work with a sample of about 0.1\% of that data, just over 15 thousand entries.  These 15 thousand entries are randomly sampled from just those applications that were for a conventional loan for a typical home purchase of a principal residence (i.e., not a rental property, not an office building, etc., just standard house or condo for an individual or family).

Download the dataset as a CSV file here.  If you have questions about the meanings of any column in the dataset, they are fully documented \sphinxhref{https://ffiec.cfpb.gov/documentation/2018/lar-data-fields/}{on the government website} from which I got the original (much larger) dataset.

\begin{sphinxadmonition}{note}{Exercise 1}

In class, we will work independently to perform the following tasks, using a cloud Jupyter provider such as Deepnote or Colab.
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Create a new project and name it something sensible, such as “MA346 Practice Project 1.”

\item {} 
Upload the data file into the project.

\item {} 
Start a new Jupyter notebook in the same project and load the data file into a pandas DataFrame in that notebook.

\item {} 
Explore the data using pandas’s built\sphinxhyphen{}in \sphinxcode{\sphinxupquote{info}} and/or \sphinxcode{\sphinxupquote{head}} methods.

\item {} 
The dataset has many columns we won’t use.  Drop all columns except for \sphinxcode{\sphinxupquote{interest\_rate}}, \sphinxcode{\sphinxupquote{property\_value}}, \sphinxcode{\sphinxupquote{state\_code}}, \sphinxcode{\sphinxupquote{tract\_minority\_population\_percent}}, \sphinxcode{\sphinxupquote{derived\_race}}, \sphinxcode{\sphinxupquote{derived\_sex}}, and \sphinxcode{\sphinxupquote{applicant\_age}}.

\item {} 
Reading a CSV file does not always ensure that columns are assigned the correct data type.  Use pandas’s built\sphinxhyphen{}in \sphinxcode{\sphinxupquote{astype}} function to correct any columns that have the wrong data type.

\item {} 
Practice selecting just a subset of the DataFrame by trying each of these things:
\begin{itemize}
\item {} 
Define a new variable \sphinxcode{\sphinxupquote{women}} that contains just the rows of the dataset containing mortgage applications from females.  How many are there?  What are the mean and median loan amounts for that group?

\item {} 
Repeat the previous bullet point, but for Asian applicants, stored in a variable named \sphinxcode{\sphinxupquote{asians}}.

\item {} 
Repeat the previous bullet point, but for applicants whose age is 75 or over, stored in a variable \sphinxcode{\sphinxupquote{age75andup}}.

\end{itemize}

\item {} 
Make your notebook presentable, using appropriate Markdown comments between cells to explain your code.  (Chapter 5 will cover best practices for how to write such comments, but do what you think is best for now.)

\item {} 
Use Deepnote or Colab’s publishing feature to create a shareable link to your notebook.  Paste that link into our class’s Microsoft Teams chat, so that we can share our work with one another and learn from each other’s work.

\end{enumerate}
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Learning on Your Own \sphinxhyphen{} Basic pandas work in Excel}

Investigate the following questions.  A report on this topic would give complete answers to each.
\begin{itemize}
\item {} 
Which of the tasks in Exercise 1 are possible to do in Excel and which are not?

\item {} 
For those that are possible in Excel, what steps does the user take to do them?

\item {} 
Will the resulting Excel workbook continue to function correctly if the original data changes?

\item {} 
Which steps are more convenient in Excel and which are more convenient in Python and pandas, and why?

\end{itemize}
\end{sphinxadmonition}


\section{Adding a new column}
\label{\detokenize{chapter-4-review-of-python-and-pandas:adding-a-new-column}}
As you may recall from CS230, you can add new columns to a pandas DataFrame using code like the example below.  This example calculates how much interest the loan would accrue in the first year.  (This is not fully accurate, since of course the borrower would make some payments that year, but it’s just an example.)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{interest\PYGZus{}first\PYGZus{}year}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{property\PYGZus{}value}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{*} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{interest\PYGZus{}rate}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{/} \PYG{l+m+mi}{100}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

Running this code in the notebook you’ve created would work just fine, and would create that new column.  It would have missing values for any rows that had missing property values or interest rates, naturally, but it would compute correct numerical values in all other rows.

But what happens if you try to run the same code, but just on the \sphinxcode{\sphinxupquote{women}} DataFrame (or \sphinxcode{\sphinxupquote{asians}} or \sphinxcode{\sphinxupquote{age75andup}})?

\begin{sphinxadmonition}{note}{Big Picture \sphinxhyphen{} Writing to a slice of a DataFrame}

The warning message you see when you attempt to run the code described above is an important one!  It relates to the difference between a DataFrame and a \sphinxstyleemphasis{view} of that DataFrame.  You can add columns to a DataFrame, but if you add to just a view, you’ll receive a warning.  We will discuss the details of this in class.
\end{sphinxadmonition}


\section{What if you don’t remember CS230 very well?}
\label{\detokenize{chapter-4-review-of-python-and-pandas:what-if-you-don-t-remember-cs230-very-well}}
I have several recommendations of resources you can use:


\subsection{DataCamp}
\label{\detokenize{chapter-4-review-of-python-and-pandas:datacamp}}
I will regularly be assigning you exercises from \sphinxhref{http://www.datacamp.com}{DataCamp}, some of which will review CS230 materials.  If you remember everything from CS230, the first few weeks of these exercises should be easy and quick for you.  If not, you will need to put in more time, but it will help you catch up.


\subsection{Bentley faculty}
\label{\detokenize{chapter-4-review-of-python-and-pandas:bentley-faculty}}
I’m glad to meet with students who need help catching up on material from CS230 they may not remember.  Please feel free to come to office hours!

I know that Prof. Masloff, who teaches CS230, made an extensive set of course notes available to her students.  You may wish to review key portions of that document to help you stay caught up in MA346.  If you did not have Prof. Masloff, you might consider \sphinxhref{https://faculty.bentley.edu/details.asp?uname=jmasloff}{contacting her} and asking for her course notes anyway.


\subsection{Stack Overflow}
\label{\detokenize{chapter-4-review-of-python-and-pandas:stack-overflow}}
The premiere question and answer website for technical subjects is \sphinxhref{https://stackoverflow.com/}{Stack Overflow}.  You don’t need to visit the site, though; if you do a good Google search for any specific Python or pandas question, one of the top hits will almost alway be from Stack Overflow.  Here are a few tips to using it well:
\begin{itemize}
\item {} 
When you do a search, put as many specific words related to your question as possible.
\begin{itemize}
\item {} 
Be sure to mention Python, pandas, or whatever other libraries your question might touch upon.

\item {} 
If your question is about an error message, put the specific key words from the error message in your search.

\end{itemize}

\item {} 
When viewing questions and answers on Stack Overflow, don’t just accept the top answer; see if later answers might be better suited to you.

\end{itemize}


\subsection{O’Reilly books}
\label{\detokenize{chapter-4-review-of-python-and-pandas:o-reilly-books}}
You have free access to O’Reilly Online Learning through the Bentley Library.  They are one of the top publishers of high\sphinxhyphen{}quality tutorial books on technical subjects.  To get started, \sphinxhref{https://www.oreilly.com/online-learning/try-now.html}{visit this page and at the bottom choose to download a mobile app for your phone or tablet.}

Then browse their book catalog and see what looks like it might be good for you.  I recommend starting here:
\begin{itemize}
\item {} 
Python Data Science Handbook by Jake VanderPlas, chapter 3 (or perhaps start earlier if you need to)

\item {} 
Python for Data Analysis by Wes McKinney, chapter 5 (or perhaps start earlier if you need to)

\end{itemize}


\subsection{Official documentation}
\label{\detokenize{chapter-4-review-of-python-and-pandas:official-documentation}}
Official documentation is used mostly for reference.  It does not make a good tutorial or lesson.  But it is the definitive reference, so I mention it here.
\begin{itemize}
\item {} 
\sphinxhref{https://docs.python.org/3/}{Python official documentation}

\item {} 
\sphinxhref{https://pandas.pydata.org/docs/}{pandas official documentation}

\end{itemize}


\section{Python review 2: mathematical exercises}
\label{\detokenize{chapter-4-review-of-python-and-pandas:python-review-2-mathematical-exercises}}
As before, do these exercises in a new notebook in Deepnote or Colab, and when you’re done, share the link to the published version into our class’s Teams chat.

\begin{sphinxadmonition}{note}{Exercise 2}

If \(r\) is the annual interest rate and \(P\) is the principal, we’re all familiar with the standard formula for the present value after \(n\) periods, \(P(1+r)^n\).  Write this as a Python function.  Also consider:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
How many inputs does it take and what are their data types?

\item {} 
What is the data type of its output?

\item {} 
Evaluate your function on \(P=1,000\), \(r=0.01\), and \(n=7\).  Ensure you get approximately \$1,072.14.

\end{enumerate}
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Exercise 3}

Create a pandas DataFrame with two columns.  The first column should be entitled F for Fahrenheit, and should contain the numbers from 0 to 100, counting by fives.  The next column should be entitled C for Celsius, and contain the corresponding temperature in degrees Celsius for the number in the first column.  Display the resulting table in the notebook.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Exercise 4}

The NumPy function \sphinxcode{\sphinxupquote{np.random.randint(a,b)}} picks a random integer between \(a\) and \(b-1\).  Use that to create a function that behaves as follows:
\begin{itemize}
\item {} 
Your function takes as input a positive integer \(n\), how many times to “roll the dice.”

\item {} 
Each roll of the dice simulates two dice being rolled (each with a number from 1 to 6) and adds the results together (thus generating a number between 2 and 12).

\item {} 
After all \(n\) rolls, return a pandas DataFrame with three columns:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
the numbers 2 through 12

\item {} 
the number of times that number showed up

\item {} 
the percentage of the time that number showed up

\end{enumerate}

\item {} 
Ensure the resulting DataFrame is sorted by its first column.

\end{itemize}
\end{sphinxadmonition}


\section{Functional\sphinxhyphen{}style code vs. imperative\sphinxhyphen{}style code}
\label{\detokenize{chapter-4-review-of-python-and-pandas:functional-style-code-vs-imperative-style-code}}
As you wrote the functions above, you might have found yourself falling into one of two styles.  To see examples of each style, let’s consider the definition of the statistical concept of \sphinxstyleemphasis{variance.}  The variance of a list of data \(x_1,\ldots,x_n\) is defined to be
\begin{equation*}
\begin{split}\frac{\sum_{i=1}^{n} (x_i-\bar{x})^2}{n-1},\end{split}
\end{equation*}
where we write \(\bar{x}\) to mean the mean of the data, and we pronounce it “\(x\) bar.”  If we take that function and convert it directly into Python, we might write it as follows.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}

\PYG{k}{def} \PYG{n+nf}{variance\PYGZus{}style\PYGZus{}1} \PYG{p}{(} \PYG{n}{data} \PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{n+nb}{sum}\PYG{p}{(} \PYG{p}{[} \PYG{p}{(} \PYG{n}{x} \PYG{o}{\PYGZhy{}} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{data}\PYG{p}{)} \PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2} \PYG{k}{for} \PYG{n}{x} \PYG{o+ow}{in} \PYG{n}{data} \PYG{p}{]} \PYG{p}{)} \PYG{o}{/} \PYG{p}{(} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{data}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{1} \PYG{p}{)}

\PYG{n}{test\PYGZus{}data} \PYG{o}{=} \PYG{p}{[} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{9}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{1} \PYG{p}{]}
\PYG{n}{variance\PYGZus{}style\PYGZus{}1}\PYG{p}{(} \PYG{n}{test\PYGZus{}data} \PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
13.982142857142858
\end{sphinxVerbatim}

Although this function computes the variance of a list of data correctly, it piles up a lot of parentheses and brackets that some readers find unnecessarily confusing when reading code.  We can make the function less compact and more explanatory by breaking the nested parentheses into several different lines of code, each storing its result in a variable.  Here is an example.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{variance\PYGZus{}style\PYGZus{}2} \PYG{p}{(} \PYG{n}{data} \PYG{p}{)}\PYG{p}{:}
    \PYG{n}{n} \PYG{o}{=} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{data}\PYG{p}{)}
    \PYG{n}{xbar} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(} \PYG{n}{data} \PYG{p}{)}
    \PYG{n}{squared\PYGZus{}differences} \PYG{o}{=} \PYG{p}{[} \PYG{p}{(} \PYG{n}{x} \PYG{o}{\PYGZhy{}} \PYG{n}{xbar} \PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2} \PYG{k}{for} \PYG{n}{x} \PYG{o+ow}{in} \PYG{n}{data} \PYG{p}{]}
    \PYG{k}{return} \PYG{n+nb}{sum}\PYG{p}{(} \PYG{n}{squared\PYGZus{}differences} \PYG{p}{)} \PYG{o}{/} \PYG{p}{(} \PYG{n}{n} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{1} \PYG{p}{)}

\PYG{n}{variance\PYGZus{}style\PYGZus{}2}\PYG{p}{(} \PYG{n}{test\PYGZus{}data} \PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
13.982142857142858
\end{sphinxVerbatim}

I call the first one \sphinxstyleemphasis{functional style} because we’re composing a lot of functions, each inside another.  I call the second one \sphinxstyleemphasis{imperative style} because this is a programming term used to describe a line of code that gives a command; here we’ve broken the formula out into three separate commands to create variables, followed by the final formula.

Neither of these is always right or always wrong.  For a short formula, you probably just want to use functional style.  But for a long formula, imperative style has these advantages:
\begin{itemize}
\item {} 
You can use good, descriptive variable names to clarify for the reader of your code what it’s computing in each step.

\item {} 
If the code you’re writing isn’t inside a function, you can split imperative\sphinxhyphen{}style code over multiple cells, and put explanations in between.

\item {} 
If you know the reader of your code is new to coding (such as a new teammate in your organization) then imperative style gives them small pieces of code to digest one at a time, rather than a big pile of code they must understand all at once.

\end{itemize}

So consider using each style for those situations that it fits best.


\chapter{Before and After}
\label{\detokenize{chapter-5-before-and-after:before-and-after}}\label{\detokenize{chapter-5-before-and-after::doc}}
See also the slides that summarize a portion of this content.

The phrase “before and after” has two meanings for us in MA346.
\begin{itemize}
\item {} 
First, it relates to code:  What \sphinxstyleemphasis{requirements} do we need to satisfy \sphinxstyleemphasis{before} doing something with data, and what \sphinxstyleemphasis{guarantees} do the math and stats techniques we use provide \sphinxstyleemphasis{after} we’ve used them?

\item {} 
Second, it relates to communicating about code:  When we’re writing explanations about our code, how do we know what kind of explanations to insert \sphinxstyleemphasis{before and after} a piece of code?

\end{itemize}

Let’s look at each of these meanings separately.


\section{Requirements and Guarantees}
\label{\detokenize{chapter-5-before-and-after:requirements-and-guarantees}}

\subsection{Requirements}
\label{\detokenize{chapter-5-before-and-after:requirements}}
Almost nobody ever writes a piece of code with no clear purpose in mind.  You can’t write code the way you can doodle in the margins of a notebook, aimless, purposeless, spacing out.  Code almost always accomplishes something; that’s what it was built for and that’s why we use it.  So when we’re coding, it’s helpful to think about our code in a purposeful way.  It helps to do so in a “before and after” way.

Before writing a piece of code, you need to know what situation you’re currently in (including your data, variables, files, etc.).  This is because the code you write will almost certainly have requirements that need to be true before that code can be run.  Here are some examples:
\begin{itemize}
\item {} 
If I’m going to sort a health care DataFrame by the “heart rate” column, the DataFrame had better have a “heart rate” column, not a “heart\_rate” column, or a “HeartRate” column, etc.  (This is a requirement imposed by the sorting routine.  It can’t guess the column name’s correct spelling; you have to provide it.)

\item {} 
If I’m going to fit a linear model to the relationship between the “heart rate” variable and the “oxygen replacement” variable, I should be sure that the relationship between those two variables appears to be approximately linear.  (This is a requirement imposed by the nature of linear models.  It isn’t always a smart idea to use a linear model if that doesn’t reflect the actual relationship in the data.)

\end{itemize}

Any code I’m about to run has \sphinxstyleemphasis{requirements} that must be true in order for that code to work, and if those requirements aren’t satisfied, the code will either give you an error or silently do the wrong thing.  Sometimes these are called “assumptions” instead of requirements, because the code assumes you’re running it in a situation where it makes sense to do so.

For instance, in the “heart rate” example above, we would get an error, because the column we tried to sort by didn’t exist.  But in the linear model example above, we would get no error, just a linear model that probably wasn’t very useful, or might produce poor predictions.

You can think of these requirements as \sphinxstylestrong{what to know before running your code} (or what to check if you don’t yet know it).  They are almost always phrased in terms of the inputs to the function you’re about to run, such as the data type the input must have, or the size/shape it must have, or the contents it must have.

How do we avoid messing this up?  \sphinxstyleemphasis{Know what the relevant requirements are} for the code you’re about to run and \sphinxstyleemphasis{check them before you run the code.}  In some cases, the requirements are so small that it doesn’t make sense to waste time checking them, as in the “heart rate” example above.  (If we get it wrong, the error message will tell us, and we’ll fix it, nice and easy.)  But in other cases, the requirements are important and take time to check, as in the linear model example above.  In fact, let’s see how that would work:

Let’s say we’ve loaded a dataset of mortgages, with columns for \sphinxcode{\sphinxupquote{property\_value}} and \sphinxcode{\sphinxupquote{total\_loan\_costs}}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZus{}static/practice\PYGZhy{}project\PYGZhy{}dataset\PYGZhy{}1.csv}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\end{sphinxVerbatim}

I’m suspecting \sphinxcode{\sphinxupquote{total\_loan\_costs}} can be estimated pretty reliably with a linear model from \sphinxcode{\sphinxupquote{property\_value}}.  But before I go and fit such a model, I had better check to be sure that the relationship between those variables actually seems to be linear.  The code below does so.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{n}{two\PYGZus{}cols} \PYG{o}{=} \PYG{n}{df}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{property\PYGZus{}value}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{total\PYGZus{}loan\PYGZus{}costs}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{replace}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Exempt}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{nan} \PYG{p}{)}
\PYG{n}{two\PYGZus{}cols} \PYG{o}{=} \PYG{n}{two\PYGZus{}cols}\PYG{o}{.}\PYG{n}{dropna}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(} \PYG{n+nb}{float} \PYG{p}{)}
\PYG{n}{two\PYGZus{}cols} \PYG{o}{=} \PYG{n}{two\PYGZus{}cols}\PYG{p}{[}\PYG{n}{two\PYGZus{}cols}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{property\PYGZus{}value}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{\PYGZlt{}} \PYG{l+m+mi}{2000000}\PYG{p}{]}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(} \PYG{n}{two\PYGZus{}cols}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{property\PYGZus{}value}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{two\PYGZus{}cols}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{total\PYGZus{}loan\PYGZus{}costs}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.25} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sample of over 15,000 U.S. mortgage applications in 2018}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Property value (USD)}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Total loan costs (USD)}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{chapter-5-before-and-after_4_0}.png}

Hmm…While some portions of that picture are linear (such as the top and bottom edges, as well as a thick strip at about \(y=4000\)), it’s pretty clear that the whole shape is not at all close to a straight line.  Any model that predicts total costs just based on property value is going to be an unreliable predictor.  I almost certainly don’t want to make a linear model for this after all (unless I’m in a situation in which I just need an \sphinxstyleemphasis{extremely} rough estimate).  Good thing I checked the requirements before making the model!


\subsection{Guarantees}
\label{\detokenize{chapter-5-before-and-after:guarantees}}
Each piece of code you run also provides certain guarantees that it will do for you (as long as you took care to ensure that the assumptions it required held true).  Here are some examples:
\begin{itemize}
\item {} 
If you have a pandas DataFrame \sphinxcode{\sphinxupquote{df}} containing numeric data and you call \sphinxcode{\sphinxupquote{df.mean()}}, you will get a list of the mean value of each column in the data, computed separately, using the standard definition of mean from your intro stats class.

\item {} 
If you fit a linear model to data using the standard method (ordinary least squares), then you know that the resulting model is the one that minimizes the sum of the squared residuals.  In other words, the expected estimation error on your data is as small as possible.

\end{itemize}

These guarantees are, in fact, the reason we run the code in the first place.  We have goals for our data work, and someone has provided us some Python\sphinxhyphen{}based tools that help us achieve our goals.  We trust the guarantees their software provides, and so we use it.

It’s important to be familiar with the guarantees provided by your math and stats software, for two reasons.  First, obviously, you can’t choose which code to run unless you know what it’s going to do when you run it!  But secondly, you’re going to want to be able to write good explanations to go along with your code, and you can’t do that unless you can articulate the guarantees your code makes.  Let’s talk about good explanations next.


\section{Communication}
\label{\detokenize{chapter-5-before-and-after:communication}}
\begin{sphinxadmonition}{note}{Big Picture \sphinxhyphen{} Explanations before and after code}

The best code notebooks explain their contents according to two rules:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Before each piece of code, explain the motivation for the code.

\item {} 
After each piece of code, explain what the output means.

\end{enumerate}

\sphinxstylestrong{Connect the two!} Your output explanation should directly address your motivation for running the code.
\end{sphinxadmonition}

This is so important that we should see some examples.


\subsection{Example 1}
\label{\detokenize{chapter-5-before-and-after:example-1}}
Imagine that you just came across the following code, all by itself.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{state\PYGZus{}code}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{value\PYGZus{}counts}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(} \PYG{l+m+mi}{10} \PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
CA    1684
FL    1136
TX    1119
PA     564
GA     558
OH     542
NY     535
NC     524
IL     508
MI     469
Name: state\PYGZus{}code, dtype: int64
\end{sphinxVerbatim}

Seeing this code naturally causes us to ask questions like:  Why are we running this code?  What is this output saying?  Who cares?  What are the numbers next to the state codes?  Why just these 10 states?

If instead the writer of the code had followed the two rules in the “Big Picture” lesson from earlier in the chapter, none of those questions would arise.  Here’s how they could have done it:


\bigskip\hrule\bigskip


Which states have the most mortgage applications in our dataset?

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{state\PYGZus{}code}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{value\PYGZus{}counts}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(} \PYG{l+m+mi}{10} \PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
CA    1684
FL    1136
TX    1119
PA     564
GA     558
OH     542
NY     535
NC     524
IL     508
MI     469
Name: state\PYGZus{}code, dtype: int64
\end{sphinxVerbatim}

Each state is shown next to the number of applications from that state in our dataset, largest first, then descending.  Here we show just the top 10.


\bigskip\hrule\bigskip


Even with just a small piece of code, notice how easy it is to understand when we have the two explanations.  The sentence before the code asks an easy\sphinxhyphen{}to\sphinxhyphen{}understand question that shows the writer’s motivation for the code.  The two sentences after the code explain what the output shows and why we can trust it.

We help the reader out (and ourselves later when we come back to this code!) by following those two simple rules of explanation.


\subsection{Example 2}
\label{\detokenize{chapter-5-before-and-after:example-2}}
Imagine encountering this code:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{rates} \PYG{o}{=} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{interest\PYGZus{}rate}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{rates}\PYG{o}{.}\PYG{n}{describe}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
count     10061
unique      500
top        4.75
freq        912
Name: interest\PYGZus{}rate, dtype: object
\end{sphinxVerbatim}

Although in this case, you might know what’s going on because \sphinxcode{\sphinxupquote{.describe()}} is so common in pandas, it still doesn’t tell us why the code was run, or what we’re supposed to pay attention to in the output.

Imagine instead that the writer of the code had done this:


\bigskip\hrule\bigskip


We’d like to use the interest rates in the dataset to do some computation.  What format are they currently stored in?

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{rates} \PYG{o}{=} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{interest\PYGZus{}rate}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{rates}\PYG{o}{.}\PYG{n}{describe}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
count     10061
unique      500
top        4.75
freq        912
Name: interest\PYGZus{}rate, dtype: object
\end{sphinxVerbatim}

The interest rates are written as percentages, since we see the most common one was 4.75 (instead of 0.0475, for example).  However, they are currently stored as text (what pandas calls “dtype: object”), so we must convert them before using them.  We stored them in the \sphinxcode{\sphinxupquote{rates}} variable so we can manipulate it further later.


\bigskip\hrule\bigskip


Now we know why the original coder cared about this output (and perhaps why we should).  Also, if we didn’t know what “dtype: object” meant, or why we might pay attention to that, now we know.  Also, we know not to multiply anything by these interest rates without also dividing by 100, because they’re percentages.  Much more helpful than just the code alone!

Poor or missing explanations decrease productivity.  When you work on a project that takes more than one day to do (and you will definitely have that experience in MA346), you’re guaranteed to come back and look at some code that you wrote in the past and scratch your head, wondering why it doesn’t look familiar.  This happens to everyone.  Help yourself out by adding explanations about each piece of code you write.  This is a requirement for the projects you do in this class; you’ll se more about this when you read the specific grading requirements for each project.

If one day you find yourself coding in a professional environment, you’ll definitely want to document your work with comments and explanations.  You’re sure to share your work with teammates at some point.  You may even use your work to show new people who join the team how to get started.  A pile of code without explanations is far less useful than code interspersed with careful explanations.


\subsection{Knowing your target audience}
\label{\detokenize{chapter-5-before-and-after:knowing-your-target-audience}}
When you’re considering adding explanations to your code, imagine yourself explaining the code to a future reader.
\begin{itemize}
\item {} 
If you suspect it’s a teammate that will read your code, write what you would say to them if you had to explain the code in person.

\item {} 
If you know it’s your MA346 instructor who will read your code, write in such a way that you prove you know what your code does and can articulate why you wrote it.

\item {} 
If you know it’s a new coder who will read your code, be more thorough and don’t take any knowledge for granted.  Think about what might confuse them and address it.

\end{itemize}


\subsection{Professionalism}
\label{\detokenize{chapter-5-before-and-after:professionalism}}
In a business context, taking the time required to make your writing as brief as possible has many benefits.  It enhances productivity because your writing is faster to read.  It reduces confusion because long writing makes people space out.  It shows respect because you’ve invested the time required to make sure your writing doesn’t waste your reader’s time.  Short, simple writing doesn’t make you look unintelligent; it makes you look like a clear writer.

It is also essential to proofread what you’ve written.  Code explanations that don’t make sense because of typos, missing words, spelling errors, or enormous paragraphs are helpful to almost no one.  Take the time to ensure your writing would make your EXP101 professor proud.  In particular, any sufficiently long text (over one page, or one computer screen) needs headings to help the reader see the big picture.

\begin{sphinxadmonition}{note}{Learning on Your Own \sphinxhyphen{} Technical Writing Tips}

Interview a professor in the English and Media Studies department.  Ask what their top 5 tips are for technical and/or business writing.  Create a report, video, or presentation on this for your MA346 peers.  Is it possible, for each tip, to show a realistic example of how bad things can be when someone disobeys the tip, compared side\sphinxhyphen{}by\sphinxhyphen{}side with a realistic example of how good things can be when the tip is followed?
\end{sphinxadmonition}


\subsection{Choosing a medium}
\label{\detokenize{chapter-5-before-and-after:choosing-a-medium}}
Should I put my code explanations as comments in the code, or as Markdown cells, or what?  Here are some brief guidelines, but there are no set rules.
\begin{itemize}
\item {} 
A Python script with comments in it is best if:
\begin{itemize}
\item {} 
you’re writing a Python module that other software developers will read (which we won’t do in this class), or

\item {} 
the code is short enough that it doesn’t warrant a full Jupyter notebook.

\end{itemize}

\item {} 
A Jupyter notebook with Markdown cells is best if:
\begin{itemize}
\item {} 
the code will generate tables and graphs that are a key part of what you’re trying to communicate, and

\item {} 
the readers are other coders, who may want to see the code along with the tables and graphs,

\item {} 
but it’s okay to also insert comments within code cells \sphinxstyleemphasis{in addition to} the before\sphinxhyphen{}and\sphinxhyphen{}after explanations between cells.

\end{itemize}

\item {} 
A report (such as a Word doc) or slide deck is best if:
\begin{itemize}
\item {} 
your audience is nontechnical and therefore will be disconcerted to see your code, or

\item {} 
your audience is technical but in this particular instance they just want your results, or

\item {} 
the amount of writing and pictures in what you need to share is high, and the amount of code very small.

\item {} 
Showing code in slides is rarely welcome in a business context.

\end{itemize}

\item {} 
A code repository (which we’ll learn about in future weeks) is best if:
\begin{itemize}
\item {} 
you have several files you want to share together, such as one or more notebooks and one or more data files, and

\item {} 
you know that your audience may want to have access not just to your results, but to your code and data as well, and

\item {} 
you know that your audience is comfortable accessing a code repository.

\end{itemize}

\end{itemize}


\chapter{Single\sphinxhyphen{}Table Verbs}
\label{\detokenize{chapter-6-single-table-verbs:single-table-verbs}}\label{\detokenize{chapter-6-single-table-verbs::doc}}
See also the slides that summarize a portion of this content.

The function we’ll discuss today got the name “verbs” because coders in the R community developed what they call \sphinxhref{https://beanumber.github.io/sds192/lab-single\_table.html}{a “grammar” for data transformation}, and the function we’ll look at today are some of that grammar’s “verbs.”  The origins in R are unimportant for our course; what matters is that verbs are things you can \sphinxstyleemphasis{do} with tables of data.


\section{Tall and Wide Form}
\label{\detokenize{chapter-6-single-table-verbs:tall-and-wide-form}}
The following two tables show the same data, but in different forms.  One is tall while the other is wide.

\sphinxstylestrong{Tall form:}


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
First
&\sphinxstyletheadfamily 
Last
&\sphinxstyletheadfamily 
Day
&\sphinxstyletheadfamily 
Sales
\\
\hline
Amy
&
Smith
&
Monday
&
39
\\
\hline
Amy
&
Smith
&
Tuesday
&
68
\\
\hline
Amy
&
Smith
&
Wednesday
&
10
\\
\hline
Bob
&
Jones
&
Monday
&
93
\\
\hline
Bob
&
Jones
&
Tuesday
&
85
\\
\hline
Bob
&
Jones
&
Wednesday
&
0
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxstylestrong{Wide form:}


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
First
&\sphinxstyletheadfamily 
Last
&\sphinxstyletheadfamily 
Monday
&\sphinxstyletheadfamily 
Tuesday
&\sphinxstyletheadfamily 
Wednesday
\\
\hline
Amy
&
Smith
&
39
&
68
&
10
\\
\hline
Bob
&
Jones
&
93
&
85
&
0
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

Although it’s not part of MA346, it’s worth mentioning that: In the famous paper \sphinxhref{https://vita.had.co.nz/papers/tidy-data.pdf}{Tidy Data}, data scientist and R developer Hadley Wickham calls tall form “tidy data” and defines it as having exactly one “observation” per row.  (What an observation is depends on what you’ve gathered data about.  In the first table above, an observation seems to be the amount of sales by a particular person on a particular day.)  His rationale comes from people who’ve studied databases, and if you’ve taken CS350 at Bentley, you may be familiar with the related concept of database normal forms.  The \sphinxhref{https://www.tidyverse.org/}{tidyverse} is a collection of R packages that help you work smoothly with data if you organize it in tidy form.

\begin{sphinxadmonition}{note}{Big Picture \sphinxhyphen{} The relationship between tall and wide data}

\sphinxstylestrong{The tall form is typically more useful when computing with data,} because we often want to filter for just the rows we care about.  So the more separated the data is into rows, the easier it is to select just the data we need.

\sphinxstylestrong{The wide form is typically more useful when presenting data to humans.}  Although this tiny table is just an example, data in the real world has far more rows, meaning that the tall form will not fit on a page.  Reshaping it into a rectangle that does fit on one page is usually preferred.

\sphinxstylestrong{Pivot} is the verb that converts tall form to wide form.

\sphinxstylestrong{Melt} is the verb that converts wide form to tall form.
\end{sphinxadmonition}

Let’s investigate them.


\section{Pivot}
\label{\detokenize{chapter-6-single-table-verbs:pivot}}
As just stated, pivot is the verb for converting tall\sphinxhyphen{}form data to wide\sphinxhyphen{}form data.  We’ll give a precise definition later on.  Let’s first get some intuition for how it works.


\subsection{The general idea}
\label{\detokenize{chapter-6-single-table-verbs:the-general-idea}}
The big picture idea of the pivot operation is illustrated here:

\sphinxincludegraphics{{table-verb-pivot}.png}

We will make that more precise later, but it can serve as a reference for the general idea.

 

The table below shows the same table from above, in “tall” form.  Drag the slider back and forth to watch the transition from tall to wide form.  While you do so, watch each of these parts of the table:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
The gray cells:
\begin{itemize}
\item {} 
These are the unique IDs used in both shapes, tall or wide.

\item {} 
They function like row headers.

\item {} 
In pandas, we call them the \sphinxcode{\sphinxupquote{index}} of the pivot.

\end{itemize}

\item {} 
The blue cells:
\begin{itemize}
\item {} 
The most important change happens here.

\item {} 
In tall form they’re data, but in wide form they’re column headers.

\item {} 
In pandas, we call them the \sphinxcode{\sphinxupquote{columns}} of the pivot (because they turn into columns when we pivot).

\end{itemize}

\item {} 
The green cells:
\begin{itemize}
\item {} 
These contain the values, typically numbers.

\item {} 
They do not change, but merely move to sit in the appropriate place in each table.

\item {} 
In pandas, we call them the \sphinxcode{\sphinxupquote{values}} of the pivot.

\end{itemize}

\end{enumerate}

(This animation can be viewed \sphinxhref{https://nathancarter.github.io/dataframe-animations/pivot.html}{in its own page here}.)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{IPython}\PYG{n+nn}{.}\PYG{n+nn}{display} \PYG{k+kn}{import} \PYG{n}{IFrame}
\PYG{n}{IFrame}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://nathancarter.github.io/dataframe\PYGZhy{}animations/pivot.html?hide\PYGZhy{}title=true}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
        \PYG{n}{width}\PYG{o}{=}\PYG{l+m+mi}{700}\PYG{p}{,} \PYG{n}{height}\PYG{o}{=}\PYG{l+m+mi}{800} \PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}IPython.lib.display.IFrame at 0x11b48c6a0\PYGZgt{}
\end{sphinxVerbatim}


\subsection{The precise definition}
\label{\detokenize{chapter-6-single-table-verbs:the-precise-definition}}
We can state precisely what \sphinxcode{\sphinxupquote{df.pivot()}} does by building on what we’ve learned in previous chapters.  We can describe both the \sphinxstyleemphasis{requirements} and \sphinxstyleemphasis{guarantees} of the pivot function, and can do so in terms of functions and relations.
\begin{itemize}
\item {} 
Requirements of \sphinxcode{\sphinxupquote{df.pivot()}}:
\begin{itemize}
\item {} 
The table to be pivoted must express a \sphinxstyleemphasis{function} from at least two input columns (called \sphinxcode{\sphinxupquote{index}} and \sphinxcode{\sphinxupquote{columns}}, above) to one output column (called \sphinxcode{\sphinxupquote{values}}, above).

\item {} 
It is acceptable for the \sphinxcode{\sphinxupquote{index}} to comprise more than one column, as in the example above.

\item {} 
Recall that for it to be a function, inputs cannot be repeated, because that could connect them with more than one output.

\end{itemize}

\item {} 
Guarantees of \sphinxcode{\sphinxupquote{df.pivot()}}:
\begin{itemize}
\item {} 
Each value from the \sphinxcode{\sphinxupquote{index}} columns will appear only once in the resulting table.

\item {} 
A new column will be created for each unique value in the old \sphinxcode{\sphinxupquote{columns}} column.

\item {} 
The \sphinxcode{\sphinxupquote{values}} column will have been removed.

\item {} 
For each \sphinxcode{\sphinxupquote{index}} entry \(i\) in the original DataFrame and each \sphinxcode{\sphinxupquote{columns}} entry \(c\), if \(v\) is the unique value associated with it, then the new table will contain a row with \sphinxcode{\sphinxupquote{index}} \(i\) and with \(v\) in the column entitled \(c\).

\end{itemize}

\end{itemize}

You can think of \sphinxcode{\sphinxupquote{df.pivot()}} as turning one function into many.  In the example above, it worked like this:
\begin{itemize}
\item {} 
Original table
\begin{itemize}
\item {} 
One function
\begin{itemize}
\item {} 
Inputs: first name, last name, day

\item {} 
Output: sales

\end{itemize}

\end{itemize}

\item {} 
Result of pivoting
\begin{itemize}
\item {} 
First function
\begin{itemize}
\item {} 
Inputs: first name, last name

\item {} 
Output: Monday sales

\end{itemize}

\item {} 
Second function
\begin{itemize}
\item {} 
Inputs: first name, last name

\item {} 
Output: Tuesday sales

\end{itemize}

\item {} 
Third function
\begin{itemize}
\item {} 
Inputs: first name, last name

\item {} 
Output: Wednesday sales

\end{itemize}

\end{itemize}

\end{itemize}


\subsection{Purpose of pivoting}
\label{\detokenize{chapter-6-single-table-verbs:purpose-of-pivoting}}
Recall that pivoting just turns “tall” data into “wide” data.  And tall form is how you typically store data when doing an analysis, because of the ease of processing tall data using code, while wide form is often more attractive for a human reading data from a table.  \sphinxstylestrong{So the purpose of pivoting is typically when you’re generating reports for human consumption.}


\section{Melt}
\label{\detokenize{chapter-6-single-table-verbs:melt}}
The reverse operation to a pivot is called “melt.”  This comes from the fact that wide data “falls down” (like the drips of a melting icicle perhaps?) into tall form.  The idea is summarized in the following picture, but you can watch it happen in the animation further below.


\subsection{The genreal idea}
\label{\detokenize{chapter-6-single-table-verbs:the-genreal-idea}}
The big picture idea of the pivot operation is illustrated here:

\sphinxincludegraphics{{table-verb-melt}.png}

We will make that more precise later, but it can serve as a reference for the general idea.

 

Just as pivoting was usually to turn data stored for computers into data readable by humans, melting is for the reverse.  If you’re given data in wide form, but you want to prepare it for analysis, you often want to convert it into tall form to make subsequent data processing code easier.

For example, let’s say we were given the table below of students’ performance on various exams.  (Obviously, this is fake data.)  If we would rather view each exam as a separate observation, so that each row is a single exam score, we can melt the table.

Drag the slider to see the melting in action.  While you do so, watch the following parts of the table:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
The gray cells:
\begin{itemize}
\item {} 
Because we’ll be spreading a student’s data out over more than one row, these will be copied.

\item {} 
These function as unique IDs for each row, so pandas calls these columns the \sphinxcode{\sphinxupquote{id\_vars}}.

\end{itemize}

\item {} 
The blue cells:
\begin{itemize}
\item {} 
These are the titles for each of several different functions.

\item {} 
Each function takes a student as input and gives a type of exam score as output.

\item {} 
They will change from being column headers to being values in the table, so pandas calls them the \sphinxcode{\sphinxupquote{value\_vars}}.

\end{itemize}

\item {} 
The green cells:
\begin{itemize}
\item {} 
Each column represents a separate function (the first maps students to SAT score, the second maps students to ACT score, and the third maps students to GPA).

\item {} 
Because we’re collecting all scores into a single column, these will stack up to become just one column.

\end{itemize}

\end{enumerate}

(This animation can be viewed \sphinxhref{https://nathancarter.github.io/dataframe-animations/melt.html}{in its own page here}.)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{IPython}\PYG{n+nn}{.}\PYG{n+nn}{display} \PYG{k+kn}{import} \PYG{n}{IFrame}
\PYG{n}{IFrame}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://nathancarter.github.io/dataframe\PYGZhy{}animations/melt.html?hide\PYGZhy{}title=true}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
        \PYG{n}{width}\PYG{o}{=}\PYG{l+m+mi}{700}\PYG{p}{,} \PYG{n}{height}\PYG{o}{=}\PYG{l+m+mi}{900} \PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}IPython.lib.display.IFrame at 0x11b376358\PYGZgt{}
\end{sphinxVerbatim}


\subsection{The precise definition}
\label{\detokenize{chapter-6-single-table-verbs:id1}}
Unsurprisingly, the requirements and guarantees of the melt operation are the reverse of those from the pivot operation.
\begin{itemize}
\item {} 
Requirements of \sphinxcode{\sphinxupquote{df.melt()}}
\begin{itemize}
\item {} 
The \sphinxcode{\sphinxupquote{id\_vars}} are one or more columns that contain unique identifiers for each row.

\item {} 
The \sphinxcode{\sphinxupquote{value\_vars}} columns are each a function from the \sphinxcode{\sphinxupquote{id\_vars}}.  (That is, no value in \sphinxcode{\sphinxupquote{id\_vars}} appears twice.)

\end{itemize}

\item {} 
Guarantees of \sphinxcode{\sphinxupquote{df.melt()}}
\begin{itemize}
\item {} 
For each value \(i\) in the \sphinxcode{\sphinxupquote{id\_vars}} column and for each column \(c\) in the \sphinxcode{\sphinxupquote{value\_vars}}, if we write \(f\) for the function that column represents, then the new table will contain a row with ID \(i\) and values \(c\) and \(f(c)\).

\item {} 
This new table will therefore be a function from the \(i\) and \(c\) columns to the \(f(c)\) column.  (By default, pandas calls those two new columns “variable” and “value” but you can give them more meaningful names.)

\item {} 
There are no other rows in the resulting table besides those just described.

\end{itemize}

\end{itemize}


\section{Pivot tables}
\label{\detokenize{chapter-6-single-table-verbs:pivot-tables}}
All this talk of pivoting should remind you of the very common Excel operation called “pivot table.”  It is very much like the pivot operation, with two differences.  First, it doesn’t require the table to represent a function.  Second, it does require you to explain how values will be summarized or combined.  Naturally, pandas suppoorts this operation as well, and it’s extremely useful.

If \sphinxcode{\sphinxupquote{df.pivot()}} makes a tall table wide, then \sphinxcode{\sphinxupquote{df.pivot\_table()}} makes a tall table sort of wide.  We’ll see why below.


\subsection{The general idea}
\label{\detokenize{chapter-6-single-table-verbs:id2}}
The big picture idea of the pivot operation is illustrated here:

\sphinxincludegraphics{{table-verb-pivot-table}.png}

We will make that more precise later, but it can serve as a reference for the general idea.

 

In the table shown below, notice that if we try to consider the gray and blue columns as inputs and the green column as outputs, the relationship is \sphinxstyleemphasis{not} a function.  If it were, we could pivot on the blue column, and the green cells would rearrange themselves just as they did in the first animation up above.  But try dragging the slider below \sphinxstyleemphasis{slowly} and you will see that some green cells collide.

For instance, Amy Smith has two different sales to the same customer, Facebook, and Bob Jones has two different sales to the same customer, Amazon.  So we cannot simply create a Facebook column and an Amazon column and rearrange the sales data into them.  When two sales figures need to be placed under the same customer heading, we need some way to combine them.

The way the table below combines cells is by adding, which is a very sensible thing to do with sales data for a customer.  You can see that the code asks this by specifying the aggregation function (or \sphinxcode{\sphinxupquote{aggfunc}}) to be “sum.”

This is why a \sphinxcode{\sphinxupquote{pivot\_table}} operation doesn’t make a table that’s as wide as a \sphinxcode{\sphinxupquote{pivot}} might, because some cells are combined, meaning that the overall table reduces in size.

(This animation can be viewed \sphinxhref{https://nathancarter.github.io/dataframe-animations/pivot-table.html}{in its own page here}.)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{IPython}\PYG{n+nn}{.}\PYG{n+nn}{display} \PYG{k+kn}{import} \PYG{n}{IFrame}
\PYG{n}{IFrame}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://nathancarter.github.io/dataframe\PYGZhy{}animations/pivot\PYGZhy{}table.html?hide\PYGZhy{}title=true}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
        \PYG{n}{width}\PYG{o}{=}\PYG{l+m+mi}{800}\PYG{p}{,} \PYG{n}{height}\PYG{o}{=}\PYG{l+m+mi}{800} \PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}IPython.lib.display.IFrame at 0x118ede2e8\PYGZgt{}
\end{sphinxVerbatim}


\subsection{The precise definition}
\label{\detokenize{chapter-6-single-table-verbs:id3}}
I will alter the precise definition of \sphinxcode{\sphinxupquote{df.pivot()}} as little as possible when creating this definition of \sphinxcode{\sphinxupquote{df.pivot\_table().}}
\begin{itemize}
\item {} 
Requirements of \sphinxcode{\sphinxupquote{df.pivot\_table()}}:
\begin{itemize}
\item {} 
The table to be pivoted can express any \sphinxstyleemphasis{relation} among least three columns (called \sphinxcode{\sphinxupquote{index}}, \sphinxcode{\sphinxupquote{columns}}, and \sphinxcode{\sphinxupquote{values}}, above).

\item {} 
It is acceptable for the \sphinxcode{\sphinxupquote{index}} to comprise more than one column, as in the example above.  (Same as for \sphinxcode{\sphinxupquote{df.pivot()}}.)

\item {} 
We must have some aggregation function (called \sphinxcode{\sphinxupquote{aggfunc}}, above) that can combine many entries from the \sphinxcode{\sphinxupquote{values}} column into one.  In the example above, we used “sum.”  Let’s call this function \(A\).

\end{itemize}

\item {} 
Guarantees of \sphinxcode{\sphinxupquote{df.pivot\_table()}}:
\begin{itemize}
\item {} 
Each value from the \sphinxcode{\sphinxupquote{index}} columns will appear only once in the resulting table.  (Same as for \sphinxcode{\sphinxupquote{df.pivot()}}.)

\item {} 
A new column will be created for each unique value in the old \sphinxcode{\sphinxupquote{columns}} column.  (Same as for \sphinxcode{\sphinxupquote{df.pivot()}}.)

\item {} 
The \sphinxcode{\sphinxupquote{values}} column will have been removed.  (Same as for \sphinxcode{\sphinxupquote{df.pivot()}}.)

\item {} 
For each \sphinxcode{\sphinxupquote{index}} entry \(i\) in the original DataFrame and each \sphinxcode{\sphinxupquote{columns}} entry \(c\), if \(v_1,v_2,\ldots,v_n\) are the various values associated with it, then the new table will contain a row with \sphinxcode{\sphinxupquote{index}} \(i\) and with \(A(v_1,v_2,\ldots,v_n)\) in the column entitled \(c\).

\end{itemize}

\end{itemize}


\section{Stack and unstack}
\label{\detokenize{chapter-6-single-table-verbs:stack-and-unstack}}
There are two other single\sphinxhyphen{}table verbs that you studied in the DataCamp review before today’s reading.  These are less common because they apply only in the context where there is a multi\sphinxhyphen{}index, either on rows or columns.  But we give animations of each below to help the reader visualize them.

The stack operation takes nested column indices (which are arranged horizontally) and makes them nested row indices (which are arranged vertically).  This is why it’s called “stack,” because it arranges the headings vertically.  Unstack is the same operation in reverse.

When applying these operations, it is possible to choose which level of a multi\sphinxhyphen{}index gets stacked or unstacked.  The two animations below use two different levels, so that you can compare the differences.


\subsection{Animation for unstack/stack at level 1}
\label{\detokenize{chapter-6-single-table-verbs:animation-for-unstack-stack-at-level-1}}
The level of a stack/unstack operation refers to which level of the multi\sphinxhyphen{}index will be moved.  The animation below shows \sphinxcode{\sphinxupquote{df.unstack( level=1 )}} when you move the slider from left to right, so level 1 of the row multi\sphinxhyphen{}index (the weeks) moves up to become part of the column index.  It is always placed as an inner index, but this can be changed afterwards with \sphinxcode{\sphinxupquote{df.swaplevel()}}.

The reverse operation is exactly \sphinxcode{\sphinxupquote{df.stack( level=1 )}}, because it moves level 1 from the column headings back to be inside the row headings instead.

(This animation can be viewed \sphinxhref{https://nathancarter.github.io/dataframe-animations/stack-1.html}{in its own page here}.)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{IPython}\PYG{n+nn}{.}\PYG{n+nn}{display} \PYG{k+kn}{import} \PYG{n}{IFrame}
\PYG{n}{IFrame}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://nathancarter.github.io/dataframe\PYGZhy{}animations/stack\PYGZhy{}1.html?hide\PYGZhy{}title=true}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
        \PYG{n}{width}\PYG{o}{=}\PYG{l+m+mi}{800}\PYG{p}{,} \PYG{n}{height}\PYG{o}{=}\PYG{l+m+mi}{900} \PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}IPython.lib.display.IFrame at 0x11b49b9e8\PYGZgt{}
\end{sphinxVerbatim}


\subsection{Animation for unstack/stack at level 0}
\label{\detokenize{chapter-6-single-table-verbs:animation-for-unstack-stack-at-level-0}}
The level of a stack/unstack operation refers to which level of the multi\sphinxhyphen{}index will be moved.  The animation below shows \sphinxcode{\sphinxupquote{df.unstack( level=0 )}} when you move the slider from left to right, so level 0 of the row multi\sphinxhyphen{}index (the months) moves up to become part of the column index.  It is always placed as an inner index, but this can be changed afterwards with \sphinxcode{\sphinxupquote{df.swaplevel()}}.

The reverse operation is therefore actually a combination of \sphinxcode{\sphinxupquote{df.stack()}} (which would put the months inside the weeks) and \sphinxcode{\sphinxupquote{df.swaplevel()}} (which would fix that) all in one.

(This animation can be viewed \sphinxhref{https://nathancarter.github.io/dataframe-animations/stack-0.html}{in its own page here}.)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{IPython}\PYG{n+nn}{.}\PYG{n+nn}{display} \PYG{k+kn}{import} \PYG{n}{IFrame}
\PYG{n}{IFrame}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://nathancarter.github.io/dataframe\PYGZhy{}animations/stack\PYGZhy{}0.html?hide\PYGZhy{}title=true}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
        \PYG{n}{width}\PYG{o}{=}\PYG{l+m+mi}{700}\PYG{p}{,} \PYG{n}{height}\PYG{o}{=}\PYG{l+m+mi}{900} \PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}IPython.lib.display.IFrame at 0x107f714e0\PYGZgt{}
\end{sphinxVerbatim}


\chapter{Abstraction}
\label{\detokenize{chapter-7-abstraction:abstraction}}\label{\detokenize{chapter-7-abstraction::doc}}
See also the slides that summarize a portion of this content.


\section{Abstract vs. concrete}
\label{\detokenize{chapter-7-abstraction:abstract-vs-concrete}}
Abstract/concrete are opposite ends of a spectrum:


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|}
\hline


&\sphinxstyletheadfamily 
Concrete (or specific)
&\sphinxstyletheadfamily 
Abstract (or general)
\\
\hline
Example from science:
&
When we drop things, they fall to earth.
&
\(G_{\mu v}+\Lambda g_{\mu v}=\frac{8\pi G}{c^4}T_{\mu v}\) (Einstein’s field equation)
\\
\hline
Example from business:
&
That startup failed because each partner tried to pull it in a different direction.
&
Organizations need a clearly stated vision.
\\
\hline
Example from ethics:
&
The Nazis’ attacks on the Jews were a great evil.
&
Systematically disadvantaging any racial group is wrong.
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxstyleemphasis{Abstraction} is therefore the process of moving from the concrete toward the abstract, or from the specific to the general.  Therefore it’s also called \sphinxstyleemphasis{generalization.}  Humans are pretty good at learning general principles from specific examples, so this is a natural thing for us to do.

It’s very useful in all kinds of programming, including data\sphinxhyphen{}related work, so it’s our focus in this chapter.


\section{Abstraction in mathematics}
\label{\detokenize{chapter-7-abstraction:abstraction-in-mathematics}}

\subsection{Example 1: Algebra class}
\label{\detokenize{chapter-7-abstraction:example-1-algebra-class}}
My kids are teenagers and have recently taken algebra classes where they learned to “complete the square.”  This procedure takes a quadratic equation like \(16x^2-9x+5=0\) and manipulates it into a form that’s easy to solve.
\begin{itemize}
\item {} 
Each homework problem was a \sphinxstyleemphasis{specific} example of this technique.

\item {} 
If you apply the technique to the equation \(ax^2+bx+c=0\), the result is the quadratic formula, \(x=\frac{-b\pm\sqrt{b^2-4ac}}{2a}\), a \sphinxstyleemphasis{general} solution to all quadratic equations.

\end{itemize}

Abstraction from the specific to the general tends to create more powerful tools, because they can be applied to any specific instance of the problem.


\subsection{Example 2: Excel formulas}
\label{\detokenize{chapter-7-abstraction:example-2-excel-formulas}}
If you took the grading policy out of the syllabus for this class, you could compute your grade in the course based on your scores on each assignment.  You could do this by hand with pencil and paper, or with a calculator.  Doing so would give you one specific course grade, for the specific assignment grades you started with.

Alternately, you could fire up a spreadsheet like Excel, and create cells for each assignment’s score, then create formulas that would do the appropriate computation and give you the corresponding course grade.  This general solution works for any specific assignment grades you might type into the spreadsheet’s input cells.

Again, the general version is more useful.


\subsection{Observations}
\label{\detokenize{chapter-7-abstraction:observations}}
Both of these mathematical examples involved replacing numbers with variables.  In Example 1, the coefficients in the specific example \(16x^2-9x+5=0\) turned into \(a\), \(b\), and \(c\) in \(ax^2+bx+c=0\).  In Example 2, you didn’t write formulas that had specific scores in them (as you would have if computing the scores by hand), but wrote formulas that contained Excel\sphinxhyphen{}style variables, which have names like A5 and B14, that come from the relevant cells.  In math (and in programming as well), abstraction typically involves \sphinxstyleemphasis{replacing specific constants with variables.}

Once we’ve rephrased our computation in terms of variables, we can do many different mathematical operations with it.
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
We can think of our computation as a function.
\begin{itemize}
\item {} 
In Example 1, the quadratic formula can be seen as a function that takes as input the values \(a,b,c\) and yields as output the two solutions of the equation.

\item {} 
In Example 2, the Excel formulas can be seen as a function that take the assignment grades as input and yield the course grade as output.

\end{itemize}

\item {} 
We can ask what happens when one of the variables changes, a question that calculus focuses on.
\begin{itemize}
\item {} 
For instance, you could ask what happens to your computation as one of the variables gets larger and larger.  (In calculus, we wrote this as \(\lim_{x\to\infty}\).)

\item {} 
Or you could ask how the result of the computation responds to changes in one input variable.  (In calculus, we wrote this as \(\frac{d}{dx}\).)

\end{itemize}

\item {} 
We can make statements about the computation in terms of the input variables.
\begin{itemize}
\item {} 
In Example 1, we might say that “Every quadratic equation has two complex number solutions.”

\item {} 
In Example 2, we might say that “It’s still possible for me to get a 4.0 in this course if my final exam score is good enough.”

\end{itemize}

\end{enumerate}

The statement above from Example 1 is a \sphinxstyleemphasis{universal} statement, also called a “for all” statement.  You could rephrase it as:  For all inputs \(a,b,c\), the outputs of the quadratic formula are two complex numbers.  The statement from Example 2 is an \sphinxstyleemphasis{existence} statement, also called a “for some” statement.  You could rephrase it as:  For some final exam scores, my final course grade is still a 4.0.  For all/for some statements are central to mathematics and we will see them show up a lot.  “For all” and “for some” are called \sphinxstyleemphasis{quantifiers} and are sometimes written \(\forall\) (for all) and \(\exists\) (for some, or “there exists”).


\section{Abstraction in programming}
\label{\detokenize{chapter-7-abstraction:abstraction-in-programming}}
\begin{sphinxadmonition}{note}{Big Picture \sphinxhyphen{} The value of abstraction in programming}

This section covers the value of abstraction for every programmer.  It is a valuable viewpoint to have and skill to be able to employ.  See the rest of this section for details on how it works and how to use it.
\end{sphinxadmonition}


\subsection{Example 3: Copying and pasting code}
\label{\detokenize{chapter-7-abstraction:example-3-copying-and-pasting-code}}
Best practices for coding include writing DRY code, where DRY stands for Don’t Repeat Yourself.  If you find yourself writing the same code (or extremely similar code) more than once, especially if you’re copying and pasting, this is a sure sign that you are not writing DRY code and should try to correct this style error.  The way to correct it is with abstraction, as shown below.  (The opposite of DRY code is WET code–Write Everything Twice.  Don’t do that.)

Here is an example of some code a student once wrote for me in a past data science course.  They had three pandas DataFrames of data about COVID\sphinxhyphen{}19, one containing numbers of cases, one containing numbers of deaths, and one containing numbers of recoveries.  They wanted to change the column names in each DataFrame.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df\PYGZus{}cases} \PYG{o}{=} \PYG{n}{df\PYGZus{}cases}\PYG{o}{.}\PYG{n}{add\PYGZus{}suffix}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZus{}cases}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{df\PYGZus{}cases}\PYG{o}{.}\PYG{n}{columns} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Province/State}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Country/Region}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Lat}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Long}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{+} \PYG{n+nb}{list}\PYG{p}{(}\PYG{n}{df\PYGZus{}cases}\PYG{o}{.}\PYG{n}{columns}\PYG{p}{[}\PYG{l+m+mi}{4}\PYG{p}{:}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{df\PYGZus{}deaths} \PYG{o}{=} \PYG{n}{df\PYGZus{}deaths}\PYG{o}{.}\PYG{n}{add\PYGZus{}suffix}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZus{}deaths}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{df\PYGZus{}deaths}\PYG{o}{.}\PYG{n}{columns} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Province/State}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Country/Region}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Lat}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Long}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{+} \PYG{n+nb}{list}\PYG{p}{(}\PYG{n}{df\PYGZus{}deaths}\PYG{o}{.}\PYG{n}{columns}\PYG{p}{[}\PYG{l+m+mi}{4}\PYG{p}{:}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{df\PYGZus{}recoveries} \PYG{o}{=} \PYG{n}{df\PYGZus{}recoveries}\PYG{o}{.}\PYG{n}{add\PYGZus{}suffix}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZus{}recoveries}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{df\PYGZus{}recoveries}\PYG{o}{.}\PYG{n}{columns} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Province/State}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Country/Region}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Lat}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Long}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{+} \PYG{n+nb}{list}\PYG{p}{(}\PYG{n}{df\PYGZus{}recoveries}\PYG{o}{.}\PYG{n}{columns}\PYG{p}{[}\PYG{l+m+mi}{4}\PYG{p}{:}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

I suspect the student wrote the first two lines, ran them, verified that they worked, and then copied and pasted them twice and changed the variable names to apply the same code to the other two DataFrames.  But this code can be made much cleaner through abstraction.  Rather than copy and paste the code, then change key parts of it, replace those key parts with a \sphinxstyleemphasis{general} (that is, abstract) variable name, and then turn the code into a function.  Since this code renames columns, the student could have made that the name of the function.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{rename\PYGZus{}columns} \PYG{p}{(} \PYG{n}{df} \PYG{p}{)}\PYG{p}{:}
    \PYG{n}{df} \PYG{o}{=} \PYG{n}{df}\PYG{o}{.}\PYG{n}{add\PYGZus{}suffix}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZus{}cases}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
    \PYG{n}{df}\PYG{o}{.}\PYG{n}{columns} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Province/State}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Country/Region}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Lat}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Long}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{+} \PYG{n+nb}{list}\PYG{p}{(}\PYG{n}{df}\PYG{o}{.}\PYG{n}{columns}\PYG{p}{[}\PYG{l+m+mi}{4}\PYG{p}{:}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

Once this general version is complete, you can apply it to each specific case you need.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{rename\PYGZus{}columns}\PYG{p}{(} \PYG{n}{df\PYGZus{}cases} \PYG{p}{)}
\PYG{n}{rename\PYGZus{}columns}\PYG{p}{(} \PYG{n}{df\PYGZus{}deaths} \PYG{p}{)}
\PYG{n}{rename\PYGZus{}columns}\PYG{p}{(} \PYG{n}{df\PYGZus{}recoveries} \PYG{p}{)}
\end{sphinxVerbatim}

There are several advantages to this new version.
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
While it is still six lines of code, half of them are much shorter, so there’s less to read and understand.

\item {} 
What the code is doing is more obvious, because we’ve given it a name; we’re obviously renaming columns.

\item {} 
It wasn’t immediately obvious in the first version of the code that we were repeating the same procedure three times.  Now it is.

\item {} 
If you later need to change how you rename columns, you have to make that change in only one place (inside the function).  Before, you would have had to make the same change three times.

\item {} 
Also, if you tried to make a change to the code later, but accidentally missed changing one of the three, you’d have broken code and not realize it.

\item {} 
You could share this same function to other notebooks or with other coders if needed.

\end{enumerate}

So the moment you find yourself copying and pasting code, remember to stay DRY instead–create a function and call it multiple times, so that you get all these benefits.


\subsection{Alternatives}
\label{\detokenize{chapter-7-abstraction:alternatives}}
Another method of abstraction would have been a loop instead of a function.  Since the original code does the same thing three times, we could have rewritten it as follows instead.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{for} \PYG{n}{df} \PYG{o+ow}{in} \PYG{p}{[} \PYG{n}{df\PYGZus{}cases}\PYG{p}{,} \PYG{n}{df\PYGZus{}deaths}\PYG{p}{,} \PYG{n}{df\PYGZus{}recoveries} \PYG{p}{]}\PYG{p}{:}
    \PYG{n}{df} \PYG{o}{=} \PYG{n}{df}\PYG{o}{.}\PYG{n}{add\PYGZus{}suffix}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZus{}cases}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
    \PYG{n}{df}\PYG{o}{.}\PYG{n}{columns} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Province/State}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Country/Region}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Lat}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Long}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{+} \PYG{n+nb}{list}\PYG{p}{(}\PYG{n}{df}\PYG{o}{.}\PYG{n}{columns}\PYG{p}{[}\PYG{l+m+mi}{4}\PYG{p}{:}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

This has all the same benefits as the previous method, except for \#6.

One could even combine the two methods together, as follows.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{rename\PYGZus{}columns} \PYG{p}{(} \PYG{n}{df} \PYG{p}{)}\PYG{p}{:}
    \PYG{n}{df} \PYG{o}{=} \PYG{n}{df}\PYG{o}{.}\PYG{n}{add\PYGZus{}suffix}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZus{}cases}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
    \PYG{n}{df}\PYG{o}{.}\PYG{n}{columns} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Province/State}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Country/Region}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Lat}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Long}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{+} \PYG{n+nb}{list}\PYG{p}{(}\PYG{n}{df}\PYG{o}{.}\PYG{n}{columns}\PYG{p}{[}\PYG{l+m+mi}{4}\PYG{p}{:}\PYG{p}{]}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{df} \PYG{o+ow}{in} \PYG{p}{[} \PYG{n}{df\PYGZus{}cases}\PYG{p}{,} \PYG{n}{df\PYGZus{}deaths}\PYG{p}{,} \PYG{n}{df\PYGZus{}recoveries} \PYG{p}{]}\PYG{p}{:}
    \PYG{n}{rename\PYGZus{}columns}\PYG{p}{(} \PYG{n}{df} \PYG{p}{)}
\end{sphinxVerbatim}


\subsection{Example 4: Testing a computation}
\label{\detokenize{chapter-7-abstraction:example-4-testing-a-computation}}
Let’s imagine that the same student as above, who has COVID\sphinxhyphen{}19 data, wants to investigate its connection to the polarized political climate in the U.S., since COVID\sphinxhyphen{}19 response has become very politicized.  They want to ask whether there’s any correlation between the spread of the virus in a state and that state’s prevailing political leaning.  So the student gets another dataset, this one listing the percentage of registered Republicans and Democrats in each U.S. state.  They will want to look up each state in the COVID\sphinxhyphen{}19 dataset in this new dataset, to connect them.  They try this:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} load political data}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{n}{df\PYGZus{}pol\PYGZus{}reg} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}excel}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZus{}static/political\PYGZhy{}registrations.xlsx}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                            \PYG{n}{sheet\PYGZus{}name}\PYG{o}{=}\PYG{l+m+mi}{0} \PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} make dictionaries for easy lookup:}
\PYG{n}{state\PYGZus{}rep\PYGZus{}pct} \PYG{o}{=} \PYG{n+nb}{dict}\PYG{p}{(} \PYG{n+nb}{zip}\PYG{p}{(} \PYG{n}{df\PYGZus{}pol\PYGZus{}reg}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{State}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{df\PYGZus{}pol\PYGZus{}reg}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{R}\PYG{l+s+s1}{\PYGZpc{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{p}{)} \PYG{p}{)}
\PYG{n}{state\PYGZus{}dem\PYGZus{}pct} \PYG{o}{=} \PYG{n+nb}{dict}\PYG{p}{(} \PYG{n+nb}{zip}\PYG{p}{(} \PYG{n}{df\PYGZus{}pol\PYGZus{}reg}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{State}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{df\PYGZus{}pol\PYGZus{}reg}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{D}\PYG{l+s+s1}{\PYGZpc{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{p}{)} \PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} see if it works on Alaska:}
\PYG{n}{state\PYGZus{}rep\PYGZus{}pct}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{AK}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
0.26
\end{sphinxVerbatim}

Great, progress!  Let’s just try one or two more random examples to be sure that wasn’t a fluke.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} see if it works on Alabama:}
\PYG{n}{state\PYGZus{}rep\PYGZus{}pct}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{AL}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gt}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}
\PYG{n+ne}{KeyError}\PYG{g+gWhitespace}{                                  }Traceback (most recent call last)
\PYG{o}{\PYGZlt{}}\PYG{n}{ipython}\PYG{o}{\PYGZhy{}}\PYG{n+nb}{input}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{012}\PYG{n}{bacaf2696}\PYG{o}{\PYGZgt{}} \PYG{o+ow}{in} \PYG{o}{\PYGZlt{}}\PYG{n}{module}\PYG{o}{\PYGZgt{}}
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{1} \PYG{c+c1}{\PYGZsh{} see if it works on Alabama:}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{2} \PYG{n}{state\PYGZus{}rep\PYGZus{}pct}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{AL}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}

\PYG{n+ne}{KeyError}: \PYGZsq{}AL\PYGZsq{}
\end{sphinxVerbatim}

Uh\sphinxhyphen{}oh.  Checking \sphinxhref{http://centerforpolitics.org/crystalball/articles/registering-by-party-where-the-democrats-and-republicans-are-ahead/}{the website where they got the data}, the student finds that Alabama doesn’t register voters by party, so Alabama isn’t in the data.  They need some code that won’t cause errors for any state input, so they update it:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{n}{state\PYGZus{}rep\PYGZus{}pct}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{AL}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{k}{if} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{AL}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o+ow}{in} \PYG{n}{state\PYGZus{}rep\PYGZus{}pct} \PYG{k}{else} \PYG{n}{np}\PYG{o}{.}\PYG{n}{nan}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
nan
\end{sphinxVerbatim}

Great, this looks like it will work for any input.  Let’s turn it into a function and test that function.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{get\PYGZus{}rep\PYGZus{}pct} \PYG{p}{(} \PYG{n}{state\PYGZus{}code} \PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{n}{state\PYGZus{}rep\PYGZus{}pct}\PYG{p}{[}\PYG{n}{state\PYGZus{}code}\PYG{p}{]} \PYG{k}{if} \PYG{n}{state\PYGZus{}code} \PYG{o+ow}{in} \PYG{n}{state\PYGZus{}rep\PYGZus{}pct} \PYG{k}{else} \PYG{n}{np}\PYG{o}{.}\PYG{n}{nan}
\PYG{k}{def} \PYG{n+nf}{get\PYGZus{}dem\PYGZus{}pct} \PYG{p}{(} \PYG{n}{state\PYGZus{}code} \PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{n}{state\PYGZus{}dem\PYGZus{}pct}\PYG{p}{[}\PYG{n}{state\PYGZus{}code}\PYG{p}{]} \PYG{k}{if} \PYG{n}{state\PYGZus{}code} \PYG{o+ow}{in} \PYG{n}{state\PYGZus{}dem\PYGZus{}pct} \PYG{k}{else} \PYG{n}{np}\PYG{o}{.}\PYG{n}{nan}
\PYG{n}{get\PYGZus{}rep\PYGZus{}pct}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{AK}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}\PYG{p}{,} \PYG{n}{get\PYGZus{}rep\PYGZus{}pct}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{AL}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}\PYG{p}{,} \PYG{n}{get\PYGZus{}dem\PYGZus{}pct}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{AK}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}\PYG{p}{,} \PYG{n}{get\PYGZus{}dem\PYGZus{}pct}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{AL}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(0.26, nan, 0.14, nan)
\end{sphinxVerbatim}

So Example 4 has shown us that abstracting a computation into a function can be done as part of an ordinary coding workflow:  Start easy, by doing the computation on just one input and get that working.  Once it does, test it on some other inputs.  Then create a function that works in general.

The benefits of this include all the benefits discussed after Example 3, plus this one:  The student wanted to run this computation for every row in a DataFrame.  That’s easy to do now, with code like the following.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df\PYGZus{}cases}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{state\PYGZus{}rep\PYGZus{}pct}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{df\PYGZus{}cases}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Province/State}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{apply}\PYG{p}{(} \PYG{n}{get\PYGZus{}rep\PYGZus{}pct} \PYG{p}{)}
\PYG{n}{df\PYGZus{}cases}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{state\PYGZus{}dem\PYGZus{}pct}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{df\PYGZus{}cases}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Province/State}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{apply}\PYG{p}{(} \PYG{n}{get\PYGZus{}dem\PYGZus{}pct} \PYG{p}{)}
\end{sphinxVerbatim}


\subsection{Little abstractions (\sphinxstyleliteralintitle{\sphinxupquote{lambda}})}
\label{\detokenize{chapter-7-abstraction:little-abstractions-lambda}}
When it would be handy to create a function, but the function is so small that it seems like giving it a name with \sphinxcode{\sphinxupquote{def}} is overkill, you can use Python’s \sphinxcode{\sphinxupquote{lambda}} syntax to create the function.

(The name comes from the fact that some branches of computer science use notation like \(\lambda x.3x+1\) to mean “the function that takes \(x\) as input and gives \(3x+1\) as output.”  So they could write \(f=\lambda x.3x+1\) instead of \(f(x)=3x+1\).)

For example, let’s say you have a dataset in which each row represents an hour of trading on an exchange, and the volume is classified using the codes 0, 1, 2, and 3, which stand (respectively) for low volume, medium volume, high volume, and unknown (missing data).  We’d like the dataset to be more readable, so we’d like to replace those numbers with the actual words low, medium, high, and unknown.  We could do it as follows.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{explain\PYGZus{}code} \PYG{p}{(} \PYG{n}{code} \PYG{p}{)}\PYG{p}{:}
    \PYG{n}{words} \PYG{o}{=} \PYG{p}{[} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{low}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{medium}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{high}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{unknown}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{]}
    \PYG{k}{return} \PYG{n}{words}\PYG{p}{[}\PYG{n}{code}\PYG{p}{]}

\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{volume}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{volume}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{apply}\PYG{p}{(} \PYG{n}{explain\PYGZus{}code} \PYG{p}{)}
\end{sphinxVerbatim}

But this requires several lines of code to do this simple task.  We could compress it into a one\sphinxhyphen{}liner as follows.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{volume}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{volume}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{apply}\PYG{p}{(} \PYG{k}{lambda} \PYG{n}{code}\PYG{p}{:} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{low}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{medium}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{high}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{unknown}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{n}{code}\PYG{p}{]} \PYG{p}{)}
\end{sphinxVerbatim}

The limitation to Python’s \sphinxcode{\sphinxupquote{lambda}} syntax is that you can put inside only a single expression, which the function will return.  A function that needs to do several preparatory computations before returning an answer cannot be converted into \sphinxcode{\sphinxupquote{lambda}} form.


\section{How to do abstraction}
\label{\detokenize{chapter-7-abstraction:how-to-do-abstraction}}
If you aren’t sure how to take specific code and turn it into a general function, I suggest following the steps given here.  Once you’ve done this a few times, it will come naturally, without thinking through the steps.

Let’s use the following example code to illustrate the steps.  It’s useful in DataFrames imported from a file where dollar amounts were written in a form like \$4,320,000.00, which pandas won’t recognize as a number, because of the commas and the dollar sign.  This code converts such a column to numeric.  Since it’s so useful, we may want to use it on multiple columns.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Tuition}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Tuition}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{str}\PYG{o}{.}\PYG{n}{replace}\PYG{p}{(} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdl{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}} \PYG{p}{)} \PYG{c+c1}{\PYGZsh{} remove dollar signs}
\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Tuition}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Tuition}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{str}\PYG{o}{.}\PYG{n}{replace}\PYG{p}{(} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}} \PYG{p}{)} \PYG{c+c1}{\PYGZsh{} remove commas}
\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Tuition}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Tuition}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(} \PYG{n+nb}{float} \PYG{p}{)}        \PYG{c+c1}{\PYGZsh{} convert to float type}
\end{sphinxVerbatim}


\subsection{Step 1:  Decide which parts of the code are customizable.  That is, which parts of the code might change the next time you want to use it?}
\label{\detokenize{chapter-7-abstraction:step-1-decide-which-parts-of-the-code-are-customizable-that-is-which-parts-of-the-code-might-change-the-next-time-you-want-to-use-it}}
In this code, we certainly want to be able to specify a different column, so \sphinxcode{\sphinxupquote{'Tuition'}} needs to be customizable.  Also, we’ve converted this column to type \sphinxcode{\sphinxupquote{float}}, but perhaps some other column of money might better be represented as \sphinxcode{\sphinxupquote{int}}, so we’ll let the type be customizable also.


\subsection{Step 2:  Move each of the customizable pieces of code out into a variable with a helpful name, declared before the code is run.}
\label{\detokenize{chapter-7-abstraction:step-2-move-each-of-the-customizable-pieces-of-code-out-into-a-variable-with-a-helpful-name-declared-before-the-code-is-run}}
This is probably clearest if it’s illustrated:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{column} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Tuition}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{new\PYGZus{}type} \PYG{o}{=} \PYG{n+nb}{float}
\PYG{n}{df}\PYG{p}{[}\PYG{n}{column}\PYG{p}{]} \PYG{o}{=} \PYG{n}{df}\PYG{p}{[}\PYG{n}{column}\PYG{p}{]}\PYG{o}{.}\PYG{n}{str}\PYG{o}{.}\PYG{n}{replace}\PYG{p}{(} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdl{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}} \PYG{p}{)} \PYG{c+c1}{\PYGZsh{} remove dollar signs}
\PYG{n}{df}\PYG{p}{[}\PYG{n}{column}\PYG{p}{]} \PYG{o}{=} \PYG{n}{df}\PYG{p}{[}\PYG{n}{column}\PYG{p}{]}\PYG{o}{.}\PYG{n}{str}\PYG{o}{.}\PYG{n}{replace}\PYG{p}{(} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}} \PYG{p}{)} \PYG{c+c1}{\PYGZsh{} remove commas}
\PYG{n}{df}\PYG{p}{[}\PYG{n}{column}\PYG{p}{]} \PYG{o}{=} \PYG{n}{df}\PYG{p}{[}\PYG{n}{column}\PYG{p}{]}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(} \PYG{n}{new\PYGZus{}type} \PYG{p}{)}         \PYG{c+c1}{\PYGZsh{} convert to new type}
\end{sphinxVerbatim}

You can then re\sphinxhyphen{}run this code to be sure it still does what it’s supposed to do.


\subsection{Step 3:  Decide on a succinct description for what your code does, to use as the name of a new function.}
\label{\detokenize{chapter-7-abstraction:step-3-decide-on-a-succinct-description-for-what-your-code-does-to-use-as-the-name-of-a-new-function}}
In this case, we’re converting a column of currency to a new type, but I don’t want to call it “convert currency” because that sound like we’re using exchange rates between two currencies.  Let’s call it “simplify currency.”


\subsection{Step 4:  Indent your original code and introduce a \sphinxstyleliteralintitle{\sphinxupquote{def}} line to define a new function with your chosen name.  Its inputs should be the names of the variables you created.}
\label{\detokenize{chapter-7-abstraction:step-4-indent-your-original-code-and-introduce-a-def-line-to-define-a-new-function-with-your-chosen-name-its-inputs-should-be-the-names-of-the-variables-you-created}}
In our example:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{simplify\PYGZus{}currency} \PYG{p}{(} \PYG{n}{column}\PYG{p}{,} \PYG{n}{new\PYGZus{}type} \PYG{p}{)}\PYG{p}{:}
    \PYG{n}{df}\PYG{p}{[}\PYG{n}{column}\PYG{p}{]} \PYG{o}{=} \PYG{n}{df}\PYG{p}{[}\PYG{n}{column}\PYG{p}{]}\PYG{o}{.}\PYG{n}{str}\PYG{o}{.}\PYG{n}{replace}\PYG{p}{(} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdl{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}} \PYG{p}{)} \PYG{c+c1}{\PYGZsh{} remove dollar signs}
    \PYG{n}{df}\PYG{p}{[}\PYG{n}{column}\PYG{p}{]} \PYG{o}{=} \PYG{n}{df}\PYG{p}{[}\PYG{n}{column}\PYG{p}{]}\PYG{o}{.}\PYG{n}{str}\PYG{o}{.}\PYG{n}{replace}\PYG{p}{(} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}} \PYG{p}{)} \PYG{c+c1}{\PYGZsh{} remove commas}
    \PYG{n}{df}\PYG{p}{[}\PYG{n}{column}\PYG{p}{]} \PYG{o}{=} \PYG{n}{df}\PYG{p}{[}\PYG{n}{column}\PYG{p}{]}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(} \PYG{n}{new\PYGZus{}type} \PYG{p}{)}
\end{sphinxVerbatim}

If you run it at this point, it doesn’t actually do anything to your DataFrame, because this just defines a function.  So we need one more step.


\subsection{Step 5:  Call your new function to accomplish what your original code used to accomplish.}
\label{\detokenize{chapter-7-abstraction:step-5-call-your-new-function-to-accomplish-what-your-original-code-used-to-accomplish}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{simplify\PYGZus{}currency} \PYG{p}{(} \PYG{n}{column}\PYG{p}{,} \PYG{n}{new\PYGZus{}type} \PYG{p}{)}\PYG{p}{:}
    \PYG{n}{df}\PYG{p}{[}\PYG{n}{column}\PYG{p}{]} \PYG{o}{=} \PYG{n}{df}\PYG{p}{[}\PYG{n}{column}\PYG{p}{]}\PYG{o}{.}\PYG{n}{str}\PYG{o}{.}\PYG{n}{replace}\PYG{p}{(} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdl{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}} \PYG{p}{)} \PYG{c+c1}{\PYGZsh{} remove dollar signs}
    \PYG{n}{df}\PYG{p}{[}\PYG{n}{column}\PYG{p}{]} \PYG{o}{=} \PYG{n}{df}\PYG{p}{[}\PYG{n}{column}\PYG{p}{]}\PYG{o}{.}\PYG{n}{str}\PYG{o}{.}\PYG{n}{replace}\PYG{p}{(} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}} \PYG{p}{)} \PYG{c+c1}{\PYGZsh{} remove commas}
    \PYG{n}{df}\PYG{p}{[}\PYG{n}{column}\PYG{p}{]} \PYG{o}{=} \PYG{n}{df}\PYG{p}{[}\PYG{n}{column}\PYG{p}{]}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(} \PYG{n}{new\PYGZus{}type} \PYG{p}{)}

\PYG{n}{simplify\PYGZus{}currency}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Tuition}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n+nb}{float} \PYG{p}{)}
\end{sphinxVerbatim}

This should have the same effect as the original code.  Except now you can re\sphinxhyphen{}use it on as many inputs as you like.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{simplify\PYGZus{}currency}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Fees}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n+nb}{float} \PYG{p}{)}
\PYG{n}{simplify\PYGZus{}currency}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Books}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n+nb}{float} \PYG{p}{)}
\PYG{n}{simplify\PYGZus{}currency}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Room and board}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n+nb}{float} \PYG{p}{)}
\end{sphinxVerbatim}

Sorry, that’s a depressing example.  Let’s move on…


\section{How do I know when to use abstraction?}
\label{\detokenize{chapter-7-abstraction:how-do-i-know-when-to-use-abstraction}}
Whenever you find yourself copying and pasting code with minor changes, this is a sure sign that you should write a function instead.  The reasons why are essentially the six benefits listed at the end of {\hyperref[\detokenize{chapter-7-abstraction:example-3-copying-and-pasting-code}]{\emph{Example 3}}}, above.

Also, if you have several lines of code in a row with only one thing changing, you can use abstraction to create a loop instead of a function.  We saw an example of this in the {\hyperref[\detokenize{chapter-7-abstraction:alternatives}]{\emph{Alternatives}}} section, above.  This is especially important if there’s a numeric progression involved.

In class, we will practice using abstraction to improve code written in a redundant style.

\sphinxstylestrong{Later, the skill of abstracting code will be a crucial part of our work on creating interactive dashboards.}

\begin{sphinxadmonition}{note}{Learning on Your Own \sphinxhyphen{} Code refactoring}

Some IDEs can help automate the process of abstraction.  This is part of a larger set of features that such apps often call “refactoring” or “code refactoring.”  Consider researching features in VS Code, PyCharm, or Eclipse that support code refactoring in Python and creating a report or video showing the class how to use those features to accomplish the content of this chapter.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Learning on Your Own \sphinxhyphen{} Writing Python modules}

Once you’ve created a useful function, such as the \sphinxcode{\sphinxupquote{simplify\_currency}} function above, that you might want to reuse in many Python scripts or Jupyter notebooks, where should you store it?  Copying and pasting it across many notebooks creates the same problems that copying and pasting any code causes.  The best strategy is to create a Python module.
\end{sphinxadmonition}

A tutorial on writing Python modules could answer the following questions.
\begin{itemize}
\item {} 
How do I start creating a Python module?

\item {} 
How do I move a function I’ve written into my new Python module?

\item {} 
Where do I store a Python module I’ve created?

\item {} 
How do I import my new module into scripts or notebooks I write?

\item {} 
How do I use the functions in my module after I’ve imported it?

\item {} 
Can I publish my module online in an official way?

\end{itemize}


\chapter{Version Control}
\label{\detokenize{chapter-8-version-control:version-control}}\label{\detokenize{chapter-8-version-control::doc}}
See also the slides that summarize a portion of this content.


\section{What is version control and why should I care?}
\label{\detokenize{chapter-8-version-control:what-is-version-control-and-why-should-i-care}}
\begin{sphinxadmonition}{note}{Big Picture \sphinxhyphen{} Why people use tools like git}

The most common version control system is called \sphinxcode{\sphinxupquote{git}}.  It helps you with:
\begin{itemize}
\item {} 
keeping old snapshots of your work in case you need to undo a mistake

\item {} 
collaborating with others on your team by sharing a project

\item {} 
publishing your project online, for sharing or as a backup

\end{itemize}
\end{sphinxadmonition}

It’s called “version control” software because of the first of those bullet points.  The other two are also important, but aren’t the main purpose of \sphinxcode{\sphinxupquote{git}}.  Let’s dive a little deeper into each of those three points and learn some terminology.


\section{Details and terminology}
\label{\detokenize{chapter-8-version-control:details-and-terminology}}

\subsection{Repositories}
\label{\detokenize{chapter-8-version-control:repositories}}
When you start a new project, you should make a folder to contain just the stuff for that project.  By default, a folder on your computer is \sphinxstyleemphasis{not} tracked by \sphinxcode{\sphinxupquote{git}}.  If you want \sphinxcode{\sphinxupquote{git}} to start tracking a folder and keeping snapshots, to enable the features listed above, you have to turn the folder into what is called a \sphinxstylestrong{\sphinxcode{\sphinxupquote{git}} repository}, or for short, a \sphinxstylestrong{repo.}  (You might also hear “source code repository” or “source repo” or similar terms.)

Once you do so, \sphinxcode{\sphinxupquote{git}} is ready to track the changes in that folder.  But it need some direction from you.  Let’s see why.


\subsection{Tracking changes}
\label{\detokenize{chapter-8-version-control:tracking-changes}}
As you work on the project, inevitably you have ups and downs.  Maybe it goes like this:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
You start by downloading a dataset from the instructor and starting a new blank Python script or Jupyter notebook in your repo folder.  Everything’s fine so far.  😀

\item {} 
You try to load the dataset but keep getting errors.  You don’t manage to solve it before you have to go to dinner.  😡

\item {} 
A friend at dinner reminded you about setting the text encoding, and that fixed the problem.  You get the dataset loading before bed.  Yes!  😀

\item {} 
The next day before MA346 you get the data cleaned without a problem.  😀

\item {} 
During class, the instructor asks your team to make progress on a hypothesis test, but you run out of time in class before you can figure out all the details.  The last few lines of code still give errors.  😡

\end{enumerate}

And so on.  You could make the story up yourself.

If you were keeping snapshots of your work for the project, you typically wouldn’t want to have any broken ones.  That is, you might want to have stored your work in steps 1, 3, and 4, because if you ever had to undo some mistake you made later, you’d want to go back to a situation where you know everything was working fine. 😀    Nobody wants to rewind to a broken repository; that’s not helpful. 😡

So you wouldn’t want your version control system to automatically make snapshots for you; it would probably save a snapshot after 1, 2, 3, 4, and 5, some broken and some not.  Therefore \sphinxcode{\sphinxupquote{git}} doesn’t do this.  If you want to save a snapshot, you have to tell \sphinxcode{\sphinxupquote{git}} to do so; this is called \sphinxstylestrong{committing} your changes.  (Or sometimes you’ll hear people call it \sphinxstylestrong{making a commit.})  When you do so, you attach a brief note (one phrase or half a sentence) describing it, called a \sphinxstylestrong{commit message.}

If you did so after each of steps 1, 3, and 4, above, you might have a list of commit messages that look like this:
\begin{itemize}
\item {} 
Downloaded dataset and started new Python script

\item {} 
Wrote code to load data

\item {} 
Added code to clean data

\end{itemize}

Later, if you wanted to go back to some old snapshot, \sphinxcode{\sphinxupquote{git}} can show you this list of commit messages so you know exactly which one you’d like to rewind to.  (At this point, I’ll stop calling them “snapshots” and start using the official term, “commits.”)

In fact, these course notes are stored in a \sphinxcode{\sphinxupquote{git}} repository, and you can \sphinxhref{https://github.com/nathancarter/MA346-course-notes/commits/master}{see its list of commits online, here}.


\subsection{Sharing online}
\label{\detokenize{chapter-8-version-control:sharing-online}}
When you want to back your work up on another computer (in case yours gets broken, or if you want to publish it for others to see) there are websites that specialize in \sphinxcode{\sphinxupquote{git}}.  The most popular is \sphinxhref{https://github.com/}{GitHub}, acquired by Microsoft in 2018.  In these notes, we’ll teach you how to use GitHub and assume that’s where you’re publishing your work.

The \sphinxcode{\sphinxupquote{git}} term for a site on which you back up or publish a repository is called a \sphinxstylestrong{remote.}  This is in contrast to the repo folder on your computer, which is called your \sphinxstylestrong{local} copy.

There are three important terms to know regarding dealing with remotes in \sphinxcode{\sphinxupquote{git}}; I’ll phrase each of them in terms of using GitHub, but the same terms apply to any remote:
\begin{itemize}
\item {} 
For repositories you created:
\begin{itemize}
\item {} 
Sending my most recent commits to GitHub is called \sphinxstylestrong{pushing} my changes (that is, my commits).

\end{itemize}

\item {} 
For repositories someone else created:
\begin{itemize}
\item {} 
Getting a copy of a repository is called \sphinxstylestrong{cloning} the repository.  It’s not the same as downloading.  A download contains just the latest version; a clone contains all past snapshots, too.

\item {} 
If the original author updates the repository with new content and I want to update my clone, that’s called \sphinxstylestrong{pulling} the changes (opposite of push, obviously).

\end{itemize}

\end{itemize}

Although technically it’s possible to pull and push to the same repository, we’ll come to that later.  Let’s start simple.

So how do we do all the things just described?  The next section gives the specifics.


\section{How to use \sphinxstyleliteralintitle{\sphinxupquote{git}} and GitHub}
\label{\detokenize{chapter-8-version-control:how-to-use-git-and-github}}
\begin{sphinxadmonition}{warning}{Warning:}
When you’re reading this chapter to prepare for Week 4’s class, you do not need to follow all these instructions.  We will do them together in class.  Feel free to just skim this section for now, and begin reading again in {\hyperref[\detokenize{chapter-8-version-control:what-if-i-want-to-collaborate}]{\emph{the next section}}}.
\end{sphinxadmonition}


\subsection{Get a GitHub account}
\label{\detokenize{chapter-8-version-control:get-a-github-account}}
\sphinxhref{https://github.com/join}{Do so on this page of the GitHub website.}  Easy!

\begin{sphinxadmonition}{warning}{Warning:}
Please choose a GitHub username that lets me know who you are.  Grading will be a confusing challenge if everyone has names like \sphinxcode{\sphinxupquote{DarkKitten75XD}}.
\end{sphinxadmonition}

Just be sure to remember the username and password, because you’ll need them in the next step.


\subsection{Download the GitHub app}
\label{\detokenize{chapter-8-version-control:download-the-github-app}}
If you ever hear horror stories of people dealing with \sphinxcode{\sphinxupquote{git}}, there are two main reasons for this.  First, they may have had a repo get screwed up because multiple people were trying to edit it in conflicting ways.  We will avoid such problems by focusing first on using \sphinxcode{\sphinxupquote{git}} by yourself before we consider how to use it on a team project.  Second, they may have been using \sphinxcode{\sphinxupquote{git}}’s command\sphinxhyphen{}line interface, meaning they interact with it through typing commands, rather than using an app.  We will avoid this hassle by getting the GitHub app.

\sphinxhref{https://desktop.github.com/}{Download and install the GitHub app from here.}

When you set the app up, it will ask for the username and password of your GitHub account, so it can connect to the GitHub site.


\subsection{Create a repository}
\label{\detokenize{chapter-8-version-control:create-a-repository}}
Let’s create a repository for you to use when submitting Project 1 later in a little over two weeks.
\begin{itemize}
\item {} 
If you haven’t already done so, create a folder on your computer for storing your work on Project 1.
\begin{itemize}
\item {} 
You don’t have to put anything in the repository at all—the folder can stay empty for now.

\end{itemize}

\item {} 
Using the GitHub app, turn that folder into a repository.
\begin{itemize}
\item {} 
From the File menu, choose “Add local repository…” and pick the folder you just created.

\end{itemize}

\end{itemize}


\subsection{Publish the repository}
\label{\detokenize{chapter-8-version-control:publish-the-repository}}
It’s okay that your repository is still empty; you can add files later.
\begin{itemize}
\item {} 
In the center of the GitHub app window there should be a button called “Publish repository.”

\item {} 
If not, go to the Repository menu and choose “Push.”

\end{itemize}

\begin{sphinxadmonition}{warning}{Warning:}
Ensure that you check the box to \sphinxstylestrong{keep the code private.}  This is so that when you actually begin work on Project 1, you are not tempting anyone else to violate Bentley’s academic integrity policy by looking at your work.
\end{sphinxadmonition}


\subsection{View it online}
\label{\detokenize{chapter-8-version-control:view-it-online}}
From the GitHub app, click the Repository menu, and “View on GitHub.”  Easy!  You’ve successfully found where the repository lives online.

Because you marked the repository private, anyone other than you who visits that page won’t be allowed to see the repository.  You can see it only because you’ve already logged in to GitHub with your username and password.

Later you’ll share this repository with your instructor so that he can visit it to grade your Project 1, once it’s complete.


\subsection{Make a commit}
\label{\detokenize{chapter-8-version-control:make-a-commit}}
In order to commit some changes to our new Project 1 repo, we have to actually do something in that folder, so there \sphinxstyleemphasis{are} some changes to commit.  Let’s do some simple setup.
\begin{itemize}
\item {} 
The Project 1 assignment on Blackboard lists three datasets you should download for use in the project.  If you haven’t already downloaded them, do so now.  Once you’ve downloaded them, move them into the folder for your new repo.

\item {} 
Return to the GitHub app and you should notice the three new files listed in the left column, showing you what’s new in the repo since it was created.

\item {} 
On the bottom left of the page, type an appropriate commit message, such as “Adding data files,” and click “Commit to master.”
\begin{itemize}
\item {} 
You can have multiple different flavors of a project all in one repo.  They’re called \sphinxstylestrong{branches} and the main one is called the \sphinxstylestrong{master branch} by default.

\item {} 
In an effort to remove any potential reference to slavery, however indirect, \sphinxhref{https://www.zdnet.com/article/github-to-replace-master-with-alternative-term-to-avoid-slavery-references/}{GitHub is in the process of changing the term “master” to “main,”} but that process is not yet complete as of this writing.

\item {} 
In the meantime, think of the term “master” as just a reference to the primary copy of your work.

\item {} 
You probably won’t need to create any other branches in any repo in MA346.

\end{itemize}

\end{itemize}

You should see your changes disappear from the left column.  This doesn’t mean that they’ve been removed!  It just means that the snapshot has been saved, so those changes aren’t “new” any more.  They’ve been committed (saved) to the repo’s history.


\subsection{Publish your commit}
\label{\detokenize{chapter-8-version-control:publish-your-commit}}
Push your changes to the repo with the Push button in the center of the app, then reload the webpage that views the repo online.  You should see your new data files in the web interface.  That’s how easy it is to publish your work to GitHub!


\subsection{Repeat as needed}
\label{\detokenize{chapter-8-version-control:repeat-as-needed}}
Whenever you make changes to your work and want to save a snapshot, feel free to repeat the “commit” instructions you see above.  The best practice is to do this as often as possible, but to try to never commit a project that’s got errors or broken code.  So try to make small, successful changes and commit after each one.

\begin{sphinxadmonition}{warning}{Warning:}
The GitHub app and \sphinxcode{\sphinxupquote{git}} in general can see \sphinxstyleemphasis{only changes that you have saved to disk!}  So if you’ve edited a Python script but \sphinxstyleemphasis{have not saved,} then \sphinxcode{\sphinxupquote{git}}/GitHub will not be able to see those changes.  The GitHub app looks only at the files on your hard drive.  It does not spy on what you’re doing in Jupyter or VS Code or any other app you have open.

\sphinxstylestrong{The takeaway:}  Be sure to save your files to disk before you try to commit.
\end{sphinxadmonition}

Whenever you want to publish your most recent commits to the GitHub site, repeat the “publish” instructions you see above.


\section{What if I want to collaborate?}
\label{\detokenize{chapter-8-version-control:what-if-i-want-to-collaborate}}
Collaborating with \sphinxcode{\sphinxupquote{git}} is a very specific type of collaboration.

On the one hand, it’s much less snappy and convenient than Google\sphinxhyphen{}docs\sphinxhyphen{}style collaboration, which happens instantaneously.  You can see one another’s cursors moving about the document and making edits in real time, live.  (You can do this on Deepnote and CoCalc, too, in Jupyter notebooks.)

On the other hand, that’s actually a good thing.  If you and someone else are editing code at the same time, one of you might make changes to a variable name at the top of the file that breaks code you’re writing using that variable at the bottom of the file.  With \sphinxcode{\sphinxupquote{git}}, you have to take intentional steps to combine two people’s work, and this helps you make sure that the changes are consistent and don’t lead to broken code.

Here’s how you do it.


\subsection{How to let someone view your private repository}
\label{\detokenize{chapter-8-version-control:how-to-let-someone-view-your-private-repository}}
You will want to do this with your Project 1 repository in two different ways.
\begin{itemize}
\item {} 
Recall that you’re permitted to have a collaborator on Project 1 in MA346 if you want one.  If so, you would add them as a collaborator using the steps below.

\item {} 
Every team will share their Project 1 repository with the instructor, so that I can grade it later.

\end{itemize}

The steps for sharing a private repository with selected individuals are very straightforward:
\begin{itemize}
\item {} 
Visit your repository on GitHub.

\item {} 
Click Settings (rightmost tab near the top of the page), then Manage access (near the top left), and Invite teams or people (bottom center).

\item {} 
You’ll need the GitHub username of your intended collaborator.  My username on GitHub is (unsurprisingly) \sphinxcode{\sphinxupquote{nathancarter}}.

\end{itemize}

To share a public repository, you can just email the link.  Also, people doing a web search or viewing your GitHub profile can see all your public repositories (but not your private ones, of course).


\subsection{How to have two contributors in a repository}
\label{\detokenize{chapter-8-version-control:how-to-have-two-contributors-in-a-repository}}
Let’s say Teammate A creates the repository and shares it with Teammate B, using the procedure described above.  Then Teammate B needs to get their own local copy, like so:
\begin{itemize}
\item {} 
Visit the repository on the GitHub website.

\item {} 
Click the green Code button, and on the menu that appears, choose Open with GitHub Desktop.

\item {} 
This will launch the GitHub app and ask Teammate B to choose where on their computer they’d like to store a clone of the repository.
\begin{itemize}
\item {} 
When you choose a folder, the repository will be placed as a new \sphinxstyleemphasis{folder} inside the one you choose.

\item {} 
For example, if you pick \sphinxcode{\sphinxupquote{My Documents\textbackslash{}MA346\textbackslash{}}}, then the repository will be cloned into \sphinxcode{\sphinxupquote{My Documents\textbackslash{}MA346\textbackslash{}the\sphinxhyphen{}repo\sphinxhyphen{}name\textbackslash{}}}, with all the files inside that inner folder.

\end{itemize}

\end{itemize}

Then Teammate A can go off and do some work on the project and \sphinxstyleemphasis{Teammate B can do work at the same time.}  They should coordinate, however, so that they don’t do conflicting work.  We’ll come back to this in detail later.

Let’s say Teammate A accomplishes some stuff and wants to commit it and share it with Teammate B.  They can do this:
\begin{itemize}
\item {} 
Do a commit just as they ordinarily would.  (See instructions up above.)

\item {} 
Push that commit to GitHub just as before.  (See instructions up above.)

\item {} 
Tell Teammate B they have pushed, so that Teammate B knows there’s new work they’ll want to get.

\end{itemize}

Then Teammate B uses the GitHub app to \sphinxstylestrong{pull} the latest changes from the repo.  This will download Teammate A’s work and automatically merge it in with Teammate B’s latest copy of things.

But wait…that sounds like it could go horribly wrong!  What if Teammates A and B were editing the same file?  Yes, it is important to coordinate, like so:

\sphinxstylestrong{Good ways to collaborate:}
\begin{itemize}
\item {} 
Teammate A can work on data cleaning in one Python script while Teammate B works on data analysis in a Jupyter notebook (a totally different file).

\item {} 
Teammate A works on data analysis code (in a Python file) while Teammate B starts writing a report (in a Word doc).

\item {} 
Teammate A edits code at the top of a file while Teammate B edits different code at the bottom of the same file.

\end{itemize}

If you follow one of these workflows, then you will not run into any headaches.  But it is possible to create headaches in two different ways.

The first headache comes if you both edit the same part of the same file.  Then when Teammate B tries to pull the changes from the repository, \sphinxcode{\sphinxupquote{git}} will tell them there’s a conflict and they need to resolve it.  Resolving the conflict can be done, but it’s a huge pain, and would probably require a trip to office hours for help.  Try to avoid it.  (Not that I don’t want to see you in office hours—I do!  But I’d love to save you the headache of the problem in the first place.)

The second headache comes if Teammate B doesn’t check to be sure that Teammate A’s changes integrate smoothly.  Here’s an example of how this might happen:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Becky edits the last few cells of a Jupyter notebook, sees that they work well, and commits the changes to her local repo.

\item {} 
She now wants to pass these edits to Carlo, so she uses the GitHub app to push.  The app tells her she can’t push yet, because Carlo pushed some changes that Becky needs to download first.  This is great, because it’s ensuring that the team makes sure that their work combines sensibly before publishing it online—nice!

\item {} 
So Becky clicks the Pull button in the app.  Because the team was careful not to edit the same code, it works smoothly and brings Carlo’s changes down to Becky’s local repo on her laptop.  Great!

\item {} 
At this point comes the danger:  Becky can push her latest changes to the web, \sphinxstyleemphasis{but she hasn’t yet checked to be sure they still work.}  She knows they worked \sphinxstyleemphasis{before} she pulled Carlo’s work in.  But what if Carlo changed something that makes Becky’s code no longer run?

\end{enumerate}

It’s always important, before pushing your code to the GitHub site, to check once more that it still runs correctly.  If it doesn’t, fix the problems and commit the fixes first, before you push to the web.


\section{Complications we’re skipping}
\label{\detokenize{chapter-8-version-control:complications-we-re-skipping}}
Everything you need to know for using \sphinxcode{\sphinxupquote{git}} in MA346 is described up above.  But there is much more to \sphinxcode{\sphinxupquote{git}} than this simple chapter has covered.  In particular:
\begin{itemize}
\item {} 
We will not need to introduce the concept of “branches,” which are very important for software development teams.  Branches are less important in data science than they are in software development, so we won’t cover them.

\item {} 
The instructions above help you avoid the concept of a “merge conflict” (when two people edit the same part of the same file).  Learning how to resolve merge conflicts is an important part of \sphinxcode{\sphinxupquote{git}} usage, but the instructions above should help you avoid the problem in the first place.

\item {} 
There are many ways to use \sphinxcode{\sphinxupquote{git}} on the command line, without the GitHub app user interface.  We will not cover those in our course.

\end{itemize}

If you’re a CIS major or minor and want to dive into the details we’re not covering, \sphinxhref{https://learn.datacamp.com/courses/introduction-to-git}{DataCamp has a git course} that covers many low\sphinxhyphen{}level details.  Feel free to take that course if you like while you have free DataCamp access in MA346, but we won’t use all those details in our work.

\begin{sphinxadmonition}{note}{Learning on Your Own \sphinxhyphen{} VS Code’s git features}

If you use VS Code for your Python coding, you may find it convenient to use VS Code’s git features, rather than having to switch back and forth to the GitHub app.  Feel free to investigate those features on your own, and if you do so, prepare a tutorial video for the class covering:
\begin{itemize}
\item {} 
how to do each of the activities covered in these notes using VS Code’s \sphinxcode{\sphinxupquote{git}} support rather than the GitHub app

\item {} 
the advantages and disadvatages to each of those two options

\end{itemize}
\end{sphinxadmonition}


\chapter{Mathematics and Statistics in Python}
\label{\detokenize{chapter-9-math-and-stats:mathematics-and-statistics-in-python}}\label{\detokenize{chapter-9-math-and-stats::doc}}
See also the slides that summarize a portion of this content.


\section{Math in Python}
\label{\detokenize{chapter-9-math-and-stats:math-in-python}}
Having had CS230, you are surely familiar with Python’s built\sphinxhyphen{}in math operators \sphinxcode{\sphinxupquote{+}}, \sphinxcode{\sphinxupquote{\sphinxhyphen{}}}, \sphinxcode{\sphinxupquote{*}}, \sphinxcode{\sphinxupquote{/}}, and \sphinxcode{\sphinxupquote{**}}.  You’re probably also familiar with the fact that Python has a \sphinxcode{\sphinxupquote{math}} module that you can use for things like trigonometry.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{math}
\PYG{n}{math}\PYG{o}{.}\PYG{n}{cos}\PYG{p}{(} \PYG{l+m+mi}{0} \PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
1.0
\end{sphinxVerbatim}

I list here just a few highlights from that module that are relevant for statistical computations.

\sphinxcode{\sphinxupquote{math.exp(x)}} is \(e^x\), so the following computes \(e\).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{math}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(} \PYG{l+m+mi}{1} \PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
2.718281828459045
\end{sphinxVerbatim}

Natural logarithms are written \(\ln x\) in mathematics, but just \sphinxcode{\sphinxupquote{log}} in Python.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{math}\PYG{o}{.}\PYG{n}{log}\PYG{p}{(} \PYG{l+m+mi}{10} \PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} natural log of 10}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
2.302585092994046
\end{sphinxVerbatim}

There are some other functions useful for data work (like \sphinxcode{\sphinxupquote{math.dist()}}, \sphinxcode{\sphinxupquote{math.comb()}}, and \sphinxcode{\sphinxupquote{math.perm()}}) coming in Python 3.8, but most Python tools (like pandas, NumPy, and SciPy) haven’t yet been udpated to work with Python 3.8.  So I do not cover those functions here, and I recommend that you stick with Python 3.7 for now.


\section{Naming mathematical variables}
\label{\detokenize{chapter-9-math-and-stats:naming-mathematical-variables}}
In programming, we almost never name variables with unhelpful names like \sphinxcode{\sphinxupquote{k}} and \sphinxcode{\sphinxupquote{x}}, because later readers of the code (or even ourselves reading it in two months) won’t know what \sphinxcode{\sphinxupquote{k}} and \sphinxcode{\sphinxupquote{x}} actually do.  The one exception to this is in mathematics, where it is normal to use single\sphinxhyphen{}letter variables, and indeed sometimes the letters matter.

\sphinxstylestrong{Example 1:}  The quadratic formula is almost always written using the letters \(a\), \(b\), and \(c\).  Yes, names like \sphinxcode{\sphinxupquote{x\_squared\_coefficient}}, \sphinxcode{\sphinxupquote{x\_coefficient}}, and \sphinxcode{\sphinxupquote{constant}} are more descriptive, but they would lead to much uglier code that’s not what anyone expects.  Compare:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} not super easy to read, but not bad:}
\PYG{k}{def} \PYG{n+nf}{quadratic\PYGZus{}nice} \PYG{p}{(} \PYG{n}{a}\PYG{p}{,} \PYG{n}{b}\PYG{p}{,} \PYG{n}{c} \PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{p}{(} \PYG{p}{(} \PYG{o}{\PYGZhy{}}\PYG{n}{b} \PYG{o}{+} \PYG{p}{(} \PYG{n}{b}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{4}\PYG{o}{*}\PYG{n}{a}\PYG{o}{*}\PYG{n}{c} \PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mf}{0.5} \PYG{p}{)} \PYG{o}{/} \PYG{p}{(} \PYG{l+m+mi}{2}\PYG{o}{*}\PYG{n}{a} \PYG{p}{)}\PYG{p}{,}
             \PYG{p}{(} \PYG{o}{\PYGZhy{}}\PYG{n}{b} \PYG{o}{\PYGZhy{}} \PYG{p}{(} \PYG{n}{b}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{4}\PYG{o}{*}\PYG{n}{a}\PYG{o}{*}\PYG{n}{c} \PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mf}{0.5} \PYG{p}{)} \PYG{o}{/} \PYG{p}{(} \PYG{l+m+mi}{2}\PYG{o}{*}\PYG{n}{a} \PYG{p}{)} \PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} oh my make it stop:}
\PYG{k}{def} \PYG{n+nf}{quadratic\PYGZus{}bad} \PYG{p}{(} \PYG{n}{x\PYGZus{}squared\PYGZus{}coefficient}\PYG{p}{,} \PYG{n}{x\PYGZus{}coefficient}\PYG{p}{,} \PYG{n}{constant} \PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{p}{(}
        \PYG{p}{(} \PYG{o}{\PYGZhy{}}\PYG{n}{x\PYGZus{}coefficient} \PYG{o}{+} \PYGZbs{}
             \PYG{p}{(} \PYG{n}{x\PYGZus{}coefficient}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{4}\PYG{o}{*}\PYG{n}{x\PYGZus{}squared\PYGZus{}coefficient}\PYG{o}{*}\PYG{n}{constant} \PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mf}{0.5} \PYG{p}{)} \PYGZbs{}
           \PYG{o}{/} \PYG{p}{(} \PYG{l+m+mi}{2}\PYG{o}{*}\PYG{n}{x\PYGZus{}squared\PYGZus{}coefficient} \PYG{p}{)}\PYG{p}{,}
        \PYG{p}{(} \PYG{o}{\PYGZhy{}}\PYG{n}{x\PYGZus{}coefficient} \PYG{o}{\PYGZhy{}} \PYGZbs{}
             \PYG{p}{(} \PYG{n}{x\PYGZus{}coefficient}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{4}\PYG{o}{*}\PYG{n}{x\PYGZus{}squared\PYGZus{}coefficient}\PYG{o}{*}\PYG{n}{constant} \PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mf}{0.5} \PYG{p}{)} \PYGZbs{}
           \PYG{o}{/} \PYG{p}{(} \PYG{l+m+mi}{2}\PYG{o}{*}\PYG{n}{x\PYGZus{}squared\PYGZus{}coefficient} \PYG{p}{)}
    \PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} of course both work fine:}
\PYG{n}{quadratic\PYGZus{}nice}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{9}\PYG{p}{,}\PYG{l+m+mi}{6}\PYG{p}{)}\PYG{p}{,} \PYG{n}{quadratic\PYGZus{}bad}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{9}\PYG{p}{,}\PYG{l+m+mi}{6}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
((2.0, 1.0), (2.0, 1.0))
\end{sphinxVerbatim}

But the first one is so much easier to read.

\sphinxstylestrong{Example 2:}  Statistics always uses \(\mu\) for the mean of a population and \(\sigma\) for its standard deviation.  If we wrote code where we used \sphinxcode{\sphinxupquote{mean}} and \sphinxcode{\sphinxupquote{standard\_deviation}} for those, that wouldn’t be hard to read, but it wouldn’t be as clear, either.

Interestingly, you can actually type Greek letters into Python code and use them as variable names!  In Jupyter, just type a backslash (\sphinxcode{\sphinxupquote{\textbackslash{}}}) followed by the name of the letter (such as \sphinxcode{\sphinxupquote{mu}}) and then press the Tab key.  It will replace the code \sphinxcode{\sphinxupquote{\textbackslash{}mu}} with the actual letter \(\mu\).  I’ve done so in the example code below.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{normal\PYGZus{}pdf} \PYG{p}{(} \PYG{n}{μ}\PYG{p}{,} \PYG{n}{σ}\PYG{p}{,} \PYG{n}{x} \PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}The value of the probability density function for}
\PYG{l+s+sd}{    the normal distribution N(μ,σ\PYGZca{}2), with mean μ and}
\PYG{l+s+sd}{    variance σ\PYGZca{}2.\PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{shifted} \PYG{o}{=} \PYG{p}{(} \PYG{n}{x} \PYG{o}{\PYGZhy{}} \PYG{n}{μ} \PYG{p}{)} \PYG{o}{/} \PYG{n}{σ}
    \PYG{k}{return} \PYG{n}{math}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(} \PYG{o}{\PYGZhy{}}\PYG{n}{shifted}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2} \PYG{o}{/} \PYG{l+m+mf}{2.0} \PYG{p}{)} \PYGZbs{}
         \PYG{o}{/} \PYG{n}{math}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(} \PYG{l+m+mi}{2}\PYG{o}{*}\PYG{n}{math}\PYG{o}{.}\PYG{n}{pi} \PYG{p}{)} \PYG{o}{/} \PYG{n}{σ}

\PYG{n}{normal\PYGZus{}pdf}\PYG{p}{(} \PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{15} \PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
0.00876415024678427
\end{sphinxVerbatim}


\section{But what about NumPy?}
\label{\detokenize{chapter-9-math-and-stats:but-what-about-numpy}}
Most data science projects in Python import both pandas and NumPy.  Since NumPy implements tons of mathematical tools, why bother using the ones in Python’s built\sphinxhyphen{}in \sphinxcode{\sphinxupquote{math}} module?  Well, on the one hand, NumPy doesn’t have \sphinxstyleemphasis{everything}; for instance, the \sphinxcode{\sphinxupquote{math.comb()}} and \sphinxcode{\sphinxupquote{math.perm()}} functions mentioned above don’t exist in NumPy.  But when you \sphinxstyleemphasis{can} use NumPy, you \sphinxstyleemphasis{should,} for the following important reason.

\begin{sphinxadmonition}{note}{Big Picture \sphinxhyphen{} Vectorization and its benefits}

All the functions in NumPy are \sphinxstyleemphasis{vectorized,} meaning that they will automatically apply themselves to every element of a NumPy array.  For instance, you can just as easily compute \sphinxcode{\sphinxupquote{square(5)}} (and get 25) as you can compute \sphinxcode{\sphinxupquote{square(x)}} if \sphinxcode{\sphinxupquote{x}} is a list of 1000 entries.  NumPy notices that you provided a list of things to square, and it squares them all.  What are the benefits to vectorization?
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Using vectorization saves you \sphinxstyleemphasis{the work of writing loops.}

\item {} 
Using vectorization saves the readers of your code \sphinxstyleemphasis{the work of reading and understanding loops.}

\item {} 
If you had to write a loop to apply a Python function (like \sphinxcode{\sphinxupquote{lambda x: x**2}}) to a list of 1000 entries, then the loop would (obviously) run in Python.  Although Python is a very convenient language to code in, it does not produce very fast\sphinxhyphen{}running code.  Tools like NumPy are written in languages like C++, which are less convenient to code in, but produce faster\sphinxhyphen{}running results.  So if you can have NumPy automatically loop over your data, rather than writing a loop in Python, \sphinxstyleemphasis{the code will execute faster.}

\end{enumerate}
\end{sphinxadmonition}

We will return to vectorization and loops in Chapter 11 of these notes.  For now, let’s just run a few NumPy functions.  In each case, notice that we give it an array as input, and it automatically knows that it should take action on each entry in the array.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Create an array of 30 random numbers to work with.}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{n}{values} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(} \PYG{l+m+mi}{30} \PYG{p}{)}
\PYG{n}{values}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
array([0.80724218, 0.0349007 , 0.33182566, 0.19286228, 0.95152264,
       0.74163011, 0.44681219, 0.89176513, 0.98331512, 0.14908147,
       0.3770531 , 0.05929311, 0.82069252, 0.31441441, 0.83632037,
       0.51864917, 0.00441048, 0.33075984, 0.47909396, 0.74597849,
       0.52155408, 0.97789045, 0.35827873, 0.60716754, 0.30250828,
       0.27467315, 0.89969365, 0.15954295, 0.37129444, 0.44652997])
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{np}\PYG{o}{.}\PYG{n}{around}\PYG{p}{(} \PYG{n}{values}\PYG{p}{,} \PYG{l+m+mi}{2} \PYG{p}{)} \PYG{c+c1}{\PYGZsh{} round to 2 decimal digits}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
array([0.81, 0.03, 0.33, 0.19, 0.95, 0.74, 0.45, 0.89, 0.98, 0.15, 0.38,
       0.06, 0.82, 0.31, 0.84, 0.52, 0.  , 0.33, 0.48, 0.75, 0.52, 0.98,
       0.36, 0.61, 0.3 , 0.27, 0.9 , 0.16, 0.37, 0.45])
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{np}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(} \PYG{n}{values} \PYG{p}{)} \PYG{c+c1}{\PYGZsh{} compute e\PYGZca{}x for each x in the array}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
array([2.2417172 , 1.03551688, 1.39350989, 1.21271576, 2.58964978,
       2.09935491, 1.56332067, 2.43943176, 2.67330388, 1.16076755,
       1.45798173, 1.06108621, 2.27207274, 1.36945713, 2.30785927,
       1.67975705, 1.00442022, 1.39202544, 1.61461083, 2.10850357,
       1.68464368, 2.65884135, 1.43086439, 1.83522582, 1.35324889,
       1.31610044, 2.45884972, 1.17297464, 1.44960983, 1.56287952])
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{np}\PYG{o}{.}\PYG{n}{square}\PYG{p}{(} \PYG{n}{values} \PYG{p}{)} \PYG{c+c1}{\PYGZsh{} square each value}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
array([6.51639934e\PYGZhy{}01, 1.21805910e\PYGZhy{}03, 1.10108270e\PYGZhy{}01, 3.71958583e\PYGZhy{}02,
       9.05395343e\PYGZhy{}01, 5.50015223e\PYGZhy{}01, 1.99641136e\PYGZhy{}01, 7.95245041e\PYGZhy{}01,
       9.66908619e\PYGZhy{}01, 2.22252837e\PYGZhy{}02, 1.42169040e\PYGZhy{}01, 3.51567261e\PYGZhy{}03,
       6.73536206e\PYGZhy{}01, 9.88564194e\PYGZhy{}02, 6.99431763e\PYGZhy{}01, 2.68996962e\PYGZhy{}01,
       1.94523650e\PYGZhy{}05, 1.09402069e\PYGZhy{}01, 2.29531019e\PYGZhy{}01, 5.56483901e\PYGZhy{}01,
       2.72018655e\PYGZhy{}01, 9.56269725e\PYGZhy{}01, 1.28363649e\PYGZhy{}01, 3.68652416e\PYGZhy{}01,
       9.15112624e\PYGZhy{}02, 7.54453415e\PYGZhy{}02, 8.09448662e\PYGZhy{}01, 2.54539518e\PYGZhy{}02,
       1.37859560e\PYGZhy{}01, 1.99389011e\PYGZhy{}01])
\end{sphinxVerbatim}

Notice that this makes it very easy to compute certain mathematical formulas.  For example, when we want to measure the quality of a model, we might compute the RSSE, or Root Sum of Squared Errors, that is, the square root of the sum of the squared differences between each actual data value \(y_i\) and its predicted value \(\hat y_i\).  In math, we write it like this:
\begin{equation*}
\begin{split} \text{RSSE} = \sqrt{\sum_{i=1}^n (y_i-\hat y_i)^2} \end{split}
\end{equation*}
The summation symbol lets you know that a loop will take place.  But in NumPy, we can do it without writing any loops.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ys}    \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(} \PYG{p}{[} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{5} \PYG{p}{]} \PYG{p}{)}   \PYG{c+c1}{\PYGZsh{} made up data}
\PYG{n}{yhats} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(} \PYG{p}{[} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{4} \PYG{p}{]} \PYG{p}{)}   \PYG{c+c1}{\PYGZsh{} also made up}
\PYG{n}{RSSE}  \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(} \PYG{n}{np}\PYG{o}{.}\PYG{n}{square}\PYG{p}{(} \PYG{n}{ys} \PYG{o}{\PYGZhy{}} \PYG{n}{yhats} \PYG{p}{)} \PYG{p}{)} \PYG{p}{)}
\PYG{n}{RSSE}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
3.605551275463989
\end{sphinxVerbatim}

Notice how the NumPy code also reads just like the English:  It’s the square root of the sume of the squared differences; the code literally says that in the formula itself!  If we had had to write it in pure Python, we would have used either a loop or a list comprehension, like in the example below.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{RSSE} \PYG{o}{=} \PYG{n}{math}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(} \PYG{n+nb}{sum}\PYG{p}{(} \PYG{p}{[} \PYG{p}{(} \PYG{n}{ys}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{\PYGZhy{}} \PYG{n}{yhats}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{ys}\PYG{p}{)}\PYG{p}{)} \PYG{p}{]} \PYG{p}{)} \PYG{p}{)} \PYG{c+c1}{\PYGZsh{} not as readable}
\PYG{n}{RSSE}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
3.605551275463989
\end{sphinxVerbatim}

A comprehensive list of NumPy’s math routines appear \sphinxhref{https://numpy.org/doc/stable/reference/routines.math.html}{in the NumPy documentation}.


\section{Binding function arguments}
\label{\detokenize{chapter-9-math-and-stats:binding-function-arguments}}
Many functions in statistics have two types of parameters.  Some of the parameters you change very rarely, and others you change all the time.

\sphinxstylestrong{Example 1:}  Consider the \sphinxcode{\sphinxupquote{normal\_pdf}} function whose code appears earlier in this chapter.  It has three parameters, \(\mu\), \(\sigma\), and \(x\).  You’ll probably have a particular normal distribution you want to work with, so you’ll choose \(\mu\) and \(\sigma\), and then you’ll want to use the function on many different values of \(x\).  So the first two parameters we choose just once, and the third parameter changes all the time.

\sphinxstylestrong{Example 2:}  Consider fitting a linear model \(\beta_0+\beta_1x\) to some data \(x_1,x_2,\ldots,x_n\).  That linear model is technically a function of three variables; we might write it as \(f(\beta_0,\beta_1,x)\).  But when we fit the model to the data, then \(\beta_0\) and \(\beta_1\) get chosen, and we don’t change them after that.  But we might plug in hundreds or even thousands of different \(x\) values to \(f\), using the same \(\beta_0\) and \(\beta_1\) values each time.

Programmers have a word for this; they call it \sphinxstyleemphasis{binding} the arguments of a function.  Binding allows us to tell Python that we’ve chosen values for some parameters and won’t be changing them; Python can thus give us a function with fewer parameters, to make things simpler.  Python does this with a tool called \sphinxcode{\sphinxupquote{partial}} in its \sphinxcode{\sphinxupquote{functools}} module.  Here’s how we would apply it to the \sphinxcode{\sphinxupquote{normal\_pdf}} function.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{functools} \PYG{k+kn}{import} \PYG{n}{partial}

\PYG{c+c1}{\PYGZsh{} Let\PYGZsq{}s say I want the standard normal distribution, that is,}
\PYG{c+c1}{\PYGZsh{} I want to fill in the values μ=0 and σ=1 once for all.}
\PYG{n}{my\PYGZus{}pdf} \PYG{o}{=} \PYG{n}{partial}\PYG{p}{(} \PYG{n}{normal\PYGZus{}pdf}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1} \PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} now I can use that on as many x inputs as I like, such as:}
\PYG{n}{my\PYGZus{}pdf}\PYG{p}{(} \PYG{l+m+mi}{0} \PYG{p}{)}\PYG{p}{,} \PYG{n}{my\PYGZus{}pdf}\PYG{p}{(} \PYG{l+m+mi}{1} \PYG{p}{)}\PYG{p}{,} \PYG{n}{my\PYGZus{}pdf}\PYG{p}{(} \PYG{l+m+mi}{2} \PYG{p}{)}\PYG{p}{,} \PYG{n}{my\PYGZus{}pdf}\PYG{p}{(} \PYG{l+m+mi}{3} \PYG{p}{)}\PYG{p}{,} \PYG{n}{my\PYGZus{}pdf}\PYG{p}{(} \PYG{l+m+mi}{4} \PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(0.3989422804014327,
 0.24197072451914337,
 0.05399096651318806,
 0.0044318484119380075,
 0.00013383022576488537)
\end{sphinxVerbatim}

In fact, SciPy’s built\sphinxhyphen{}in random number generating procedures let you use them either by binding arguments or not, at your preference.  For instance, to generate 10 random floating point values between 0 and 100, we can do the following.  (The \sphinxcode{\sphinxupquote{rvs}} function stands for “random values.”)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{stats} \PYG{k}{as} \PYG{n+nn}{stats}
\PYG{n}{stats}\PYG{o}{.}\PYG{n}{uniform}\PYG{o}{.}\PYG{n}{rvs}\PYG{p}{(} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{100}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{l+m+mi}{10} \PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
array([72.45814241, 48.37636268, 37.64218462, 59.19184957, 70.29433467,
       64.18287033, 99.34424078, 94.95203695, 36.45039975, 31.37376436])
\end{sphinxVerbatim}

Or we can use built\sphinxhyphen{}in SciPy functionality to bind the first two arguments and create a specific random variable, then call \sphinxcode{\sphinxupquote{rvs}} on that.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{X} \PYG{o}{=} \PYG{n}{stats}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{100} \PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} make a random variable}
\PYG{n}{X}\PYG{o}{.}\PYG{n}{rvs}\PYG{p}{(} \PYG{n}{size}\PYG{o}{=}\PYG{l+m+mi}{10} \PYG{p}{)}             \PYG{c+c1}{\PYGZsh{} generate 10 values from it}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
array([80.33953909, 44.72173619, 61.46458148, 19.7498066 , 53.50710756,
        9.18715567, 30.60251347, 59.96554507, 49.19081409, 24.76156349])
\end{sphinxVerbatim}

The same random variable can, of course, be used to create more values later.

The \sphinxcode{\sphinxupquote{partial}} tool built into Python only works if you want to bind the \sphinxstyleemphasis{first} arguments of the function.  If you need to bind later ones, then you can do it yourself using a \sphinxcode{\sphinxupquote{lambda}}, as in the following example.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{subtract} \PYG{p}{(} \PYG{n}{a}\PYG{p}{,} \PYG{n}{b} \PYG{p}{)}\PYG{p}{:}   \PYG{c+c1}{\PYGZsh{} silly little example function}
    \PYG{k}{return} \PYG{n}{a} \PYG{o}{\PYGZhy{}} \PYG{n}{b}

\PYG{n}{subtract\PYGZus{}1} \PYG{o}{=} \PYG{k}{lambda} \PYG{n}{a}\PYG{p}{:} \PYG{n}{subtract}\PYG{p}{(} \PYG{n}{a}\PYG{p}{,} \PYG{l+m+mi}{1} \PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} bind second argument to 1}

\PYG{n}{subtract\PYGZus{}1}\PYG{p}{(} \PYG{l+m+mi}{5} \PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
4
\end{sphinxVerbatim}

We will also use the concept of binding function parameters when we come to curve fitting at the end of this chapter.


\section{GB213 in Python}
\label{\detokenize{chapter-9-math-and-stats:gb213-in-python}}
You can refer at any time to one of the appendices in these course notes, a {\hyperref[\detokenize{GB213-review-in-Python::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{review of GB213, but in Python}}}}.

Topics covered there:
\begin{itemize}
\item {} 
Discrete and continuous random variables
\begin{itemize}
\item {} 
creating

\item {} 
plotting

\item {} 
generating random values

\item {} 
computing probabilities

\item {} 
computing statistics

\end{itemize}

\item {} 
Hypothesis testing for a population mean
\begin{itemize}
\item {} 
one\sphinxhyphen{}sided

\item {} 
two\sphinxhyphen{}sided

\end{itemize}

\item {} 
Simple linear regression (one predictor variable)
\begin{itemize}
\item {} 
creating the model from data

\item {} 
computing \(R\) and \(R^2\)

\item {} 
visualizing the model

\end{itemize}

\end{itemize}

Topics not covered in that chapter, but that you may have seen in GB213:
\begin{itemize}
\item {} 
Basic probability (covered in every GB213 section)

\item {} 
ANOVA (covered in some GB213 sections)

\item {} 
\(\chi^2\) tests (covered in some GB213 sections)

\end{itemize}

\begin{sphinxadmonition}{note}{Learning on Your Own \sphinxhyphen{} Pingouin}

The GB213 review appendix that I linked to above uses the very popular Python statistics tools \sphinxcode{\sphinxupquote{statsmodels}} and \sphinxcode{\sphinxupquote{scipy.stats}}.  But there is a relatively new toolkit called Pingouin; it’s not as popular (yet?) but it has some advantages over the other two.  See \sphinxhref{https://towardsdatascience.com/the-new-kid-on-the-statistics-in-python-block-pingouin-6b353a1db57c}{this blog post} for an introduction and consider a tutorial, video, presentation, or notebook for the class that showcases when you might prefer Pingouin to the others, and how to use it in such cases.  Be sure to include the installation procedure.
\end{sphinxadmonition}


\section{Curve fitting in general}
\label{\detokenize{chapter-9-math-and-stats:curve-fitting-in-general}}
The final topic covered in the GB213 review mentioned above is simple linear regression, which fits a line to a set of (two\sphinxhyphen{}dimensional) data points.  But Python’s scientific tools permit you to handle much more complex models.  We cannot cover mathematical modeling in detail in MA346, because it can take several courses on its own, but you can learn more about regression modeling in particular in \sphinxhref{https://catalog.bentley.edu/search/?P=MA\%20252}{MA252 at Bentley}.  But we will cover how to fit an arbitrary curve to data in Python.


\subsection{1. Say we have some data}
\label{\detokenize{chapter-9-math-and-stats:say-we-have-some-data}}
We will assume you have data stored in a pandas DataFrame, and we will lift out just two columns of the DataFrame, one that will be used as our \(x\) values (independent variable), and the other as our \(y\) values (dependent variable).  I’ll make up some data here just for use in this example.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} example data only, totally made up:}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(} \PYG{p}{\PYGZob{}}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{salt used (x)}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{:}     \PYG{p}{[} \PYG{l+m+mf}{2.1}\PYG{p}{,} \PYG{l+m+mf}{2.9}\PYG{p}{,} \PYG{l+m+mf}{3.1}\PYG{p}{,} \PYG{l+m+mf}{3.5}\PYG{p}{,} \PYG{l+m+mf}{3.7}\PYG{p}{,} \PYG{l+m+mf}{4.6} \PYG{p}{]}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ice remaining (y)}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{:} \PYG{p}{[} \PYG{l+m+mf}{7.9}\PYG{p}{,} \PYG{l+m+mf}{6.5}\PYG{p}{,} \PYG{l+m+mf}{6.5}\PYG{p}{,} \PYG{l+m+mf}{6.0}\PYG{p}{,} \PYG{l+m+mf}{6.2}\PYG{p}{,} \PYG{l+m+mf}{6.0} \PYG{p}{]}
\PYG{p}{\PYGZcb{}} \PYG{p}{)}
\PYG{n}{df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
   salt used (x)  ice remaining (y)
0            2.1                7.9
1            2.9                6.5
2            3.1                6.5
3            3.5                6.0
4            3.7                6.2
5            4.6                6.0
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{n}{xs} \PYG{o}{=} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{salt used (x)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{ys} \PYG{o}{=} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ice remaining (y)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(} \PYG{n}{xs}\PYG{p}{,} \PYG{n}{ys} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{chapter-9-math-and-stats_36_0}.png}


\subsection{2. Choose a model}
\label{\detokenize{chapter-9-math-and-stats:choose-a-model}}
Curve\sphinxhyphen{}fitting is a powerful tool, and it’s easy to misuse it by fitting to your data a model that doesn’t make sense for that data.  A mathematical modeling course can help you learn how to assess the appropriateness of a given type of line, curve, or more complex model for a given situation.  But for this small example, let’s pretend that we know that the following model makes sense, perhaps because some earlier work with salt and ice had success with it.  (Again, keep in mind that this example is really, truly, totally made up.)
\begin{equation*}
\begin{split} y=\frac{\beta_0}{\beta_1+x}+\beta_2 \end{split}
\end{equation*}
We will use this model.  Obviously, it’s not the equation of a line, so linear regression tools like those covered in the GB213 review notebook won’t be sufficient.  To begin, we code the model as a Python function taking inputs in this order: first, \(x\), then after it, all the model parameters \(\beta_0,\beta_1\), and so on, however many model parameters there happen to be (in this case three).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{my\PYGZus{}model} \PYG{p}{(} \PYG{n}{x}\PYG{p}{,} \PYG{n}{β0}\PYG{p}{,} \PYG{n}{β1}\PYG{p}{,} \PYG{n}{β2} \PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{n}{β0} \PYG{o}{/} \PYG{p}{(} \PYG{n}{β1} \PYG{o}{+} \PYG{n}{x} \PYG{p}{)} \PYG{o}{+} \PYG{n}{β2}
\end{sphinxVerbatim}


\subsection{3. Have SciPy find the \protect\(\beta\protect\)s}
\label{\detokenize{chapter-9-math-and-stats:have-scipy-find-the-betas}}
This step is called “fitting the model to your data.”  It finds the values of \(\beta_0,\beta_1,\beta_2\) that make the most sense for the particular \(x\) and \(y\) adata values that you have.  Using the language from earlier in this chapter, SciPy will tell us how to \sphinxstyleemphasis{bind values to the parameters} \(\beta_0,\beta_1,\beta_2\) of \sphinxcode{\sphinxupquote{my\_model}} so that the resulting function, which just takes \sphinxcode{\sphinxupquote{x}} as input, is the one best fit to our data.

For example, if we picked our own values for the model parameters, we would probably guess poorly.  Let’s try guessing \(\beta_0=1,\beta_1=2,\beta_2=3\).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{guess\PYGZus{}model} \PYG{o}{=} \PYG{k}{lambda} \PYG{n}{x}\PYG{p}{:} \PYG{n}{my\PYGZus{}model}\PYG{p}{(} \PYG{n}{x}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{5} \PYG{p}{)}

\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{n}{many\PYGZus{}xs} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{100} \PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(} \PYG{n}{xs}\PYG{p}{,} \PYG{n}{ys} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(} \PYG{n}{many\PYGZus{}xs}\PYG{p}{,} \PYG{n}{guess\PYGZus{}model}\PYG{p}{(} \PYG{n}{many\PYGZus{}xs} \PYG{p}{)} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{chapter-9-math-and-stats_40_0}.png}

Yyyyyyeah…  Our model is nowhere near the data.  That’s why we need SciPy to find the \(\beta\)s.  Here’s how we ask it to do so.  You start with your own guess for the parameters, and SciPy will improve it.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{optimize} \PYG{k+kn}{import} \PYG{n}{curve\PYGZus{}fit}
\PYG{n}{my\PYGZus{}guessed\PYGZus{}betas} \PYG{o}{=} \PYG{p}{[} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{5} \PYG{p}{]}
\PYG{n}{found\PYGZus{}betas}\PYG{p}{,} \PYG{n}{covariance} \PYG{o}{=} \PYG{n}{curve\PYGZus{}fit}\PYG{p}{(} \PYG{n}{my\PYGZus{}model}\PYG{p}{,} \PYG{n}{xs}\PYG{p}{,} \PYG{n}{ys}\PYG{p}{,} \PYG{n}{p0}\PYG{o}{=}\PYG{n}{my\PYGZus{}guessed\PYGZus{}betas} \PYG{p}{)}
\PYG{n}{β0}\PYG{p}{,} \PYG{n}{β1}\PYG{p}{,} \PYG{n}{β2} \PYG{o}{=} \PYG{n}{found\PYGZus{}betas}
\PYG{n}{β0}\PYG{p}{,} \PYG{n}{β1}\PYG{p}{,} \PYG{n}{β2}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(1.3739384272240622, \PYGZhy{}1.5255461192343747, 5.510233385761209)
\end{sphinxVerbatim}

So how does SciPy’s found model look?


\subsection{4. Describe and show the fit model}
\label{\detokenize{chapter-9-math-and-stats:describe-and-show-the-fit-model}}
Rounding to a few decimal places, our model is therefore the following:
\begin{equation*}
\begin{split} y=\frac{1.37}{-1.53+x}+5.51 \end{split}
\end{equation*}
It fits the data very well, as you can see below.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{fit\PYGZus{}model} \PYG{o}{=} \PYG{k}{lambda} \PYG{n}{x}\PYG{p}{:} \PYG{n}{my\PYGZus{}model}\PYG{p}{(} \PYG{n}{x}\PYG{p}{,} \PYG{n}{β0}\PYG{p}{,} \PYG{n}{β1}\PYG{p}{,} \PYG{n}{β2} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(} \PYG{n}{xs}\PYG{p}{,} \PYG{n}{ys} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(} \PYG{n}{many\PYGZus{}xs}\PYG{p}{,} \PYG{n}{fit\PYGZus{}model}\PYG{p}{(} \PYG{n}{many\PYGZus{}xs} \PYG{p}{)} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{chapter-9-math-and-stats_44_0}.png}

\begin{sphinxadmonition}{note}{Big Picture \sphinxhyphen{} Models vs. fit models}

In mathematical modeling and machine learning, we sometimes distinguish between a \sphinxstyleemphasis{model} and a \sphinxstyleemphasis{fit model.}
\begin{itemize}
\item {} 
A \sphinxstyleemphasis{model} is a general purpose technique that you decide might suit the data.  Examples:
\begin{itemize}
\item {} 
A linear model, \(y=\beta_0+\beta_1x\)

\item {} 
A quadratic model, \(y=\beta_0+\beta_1x+\beta_2x^2\)

\item {} 
A logistic curve, \(y=\frac{\beta_0}{1+e^{\beta_1(-x+\beta_2)}}\)

\item {} 
A neural network

\end{itemize}

\item {} 
A \sphinxstyleemphasis{fit model} is the specific version of the general model that’s been tailored to suit your data.  We create it from the general model by \sphinxstyleemphasis{binding} the values of the \(\beta\)s to specific numbers.

\end{itemize}

For example, if your model were \(y=\beta_0+\beta_1x\), then your fit model might be \(y=-0.95+1.13x\).  In the general model, \(y\) depends on three variables (\(x,\beta_0,\beta_1\)).  In the fit model, it depends on only one variable (\(x\)).  So model fitting is an example of binding the variables of a function.
\end{sphinxadmonition}

In class, we will use this technique to fit a logistic growth model to COVID\sphinxhyphen{}19 data.  Be sure to have completed the preparatory work on writing a function that extracts the series of COVID\sphinxhyphen{}19 cases over time for a given state!  Recall that it appears on the final slide of the Chapter 8 slides.


\chapter{Visualization}
\label{\detokenize{chapter-10-visualization:visualization}}\label{\detokenize{chapter-10-visualization::doc}}
See also the slides that summarize a portion of this content.

In preparation for today, you learned many \sphinxhref{big-cheat-sheet.html\#before-week-5}{data visualization tools from DataCamp}.  In fact, if you’re doing this reading before you do the DataCamp homework, I strongly suggest that you stop here, do the DataCamp first, and then come back here.

Rather than review those tools here, I will categorize them instead.  This page is therefore a reference in which you can look up the kind of data you \sphinxstyleemphasis{have} and see which visualizations make the most sense for it, and what each one accomplishes.

We will use two datasets throughout the examples below.  The first is a set of sales data for the employees of an imaginary company (Dunder Mifflin, perhaps?).  The data has the following format, organized by employee ID numbers, and including year, quarter, sales quantity, and bonus earned for each ID in each relevant time frame.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{n}{sales\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{./\PYGZus{}static/fictitious\PYGZhy{}sales\PYGZhy{}data.csv}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{sales\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
    emp\PYGZus{}id  year  quarter       sales  bonus
0  1275342  2010        2    8.000000      0
1  1275342  2010        3  333.000000      0
2  1275342  2010        4  594.000000   2000
3  1275342  2011        1  276.066177      0
4  1275342  2011        2  340.000000      0
\end{sphinxVerbatim}

The second dataset is the basic NASDAQ data for Renewable Energy Group, Inc. (symbol REGI) for the first half of 2020.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{regi\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{./\PYGZus{}static/regi\PYGZhy{}prices\PYGZhy{}2020.csv}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{regi\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
       Date   Open   High    Low  Close  Adj Close   Volume
0  2\PYGZhy{}Jan\PYGZhy{}20  27.21  27.95  26.62  27.89      27.89   781100
1  3\PYGZhy{}Jan\PYGZhy{}20  28.16  28.95  27.73  28.82      28.82  1405100
2  6\PYGZhy{}Jan\PYGZhy{}20  28.53  28.81  28.00  28.39      28.39   716800
3  7\PYGZhy{}Jan\PYGZhy{}20  28.17  28.28  26.08  26.44      26.44  1378900
4  8\PYGZhy{}Jan\PYGZhy{}20  26.37  26.40  24.86  25.19      25.19  1195900
\end{sphinxVerbatim}


\section{What if I have two columns of numeric data?}
\label{\detokenize{chapter-10-visualization:what-if-i-have-two-columns-of-numeric-data}}
This situation is \sphinxstyleemphasis{extremely common,} and that’s why we address it first.  If we consider the two datasets described above, we can find many ways to create two columns of numeric data, including the following examples.
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
The year and sales columns from \sphinxcode{\sphinxupquote{sales\_df}}

\item {} 
The year and sales columns we would get by grouping \sphinxcode{\sphinxupquote{sales\_df}} by year

\item {} 
The Volume and High columns from \sphinxcode{\sphinxupquote{regi\_df}}

\item {} 
The index and the Close column from \sphinxcode{\sphinxupquote{regi\_df}}

\end{enumerate}

\begin{sphinxadmonition}{note}{Big Picture \sphinxhyphen{} Visualizing relations vs. functions}

Recall that two columns of data always form a binary relation, but may or may not be a function.  Noticing whether the data are a function is very important when deciding how to visualize them.
\begin{itemize}
\item {} 
A \sphinxstylestrong{function} can be shown with a \sphinxstylestrong{line plot}, as in algebra classes.

\item {} 
A \sphinxstylestrong{relation} that is not a function must be shown as a \sphinxstylestrong{scatterplot.}

\end{itemize}
\end{sphinxadmonition}

Both scatterplots and line plots are drawn with \sphinxcode{\sphinxupquote{plt.plot()}} in Matplotlib.  There are many ways to specify the plot type, as you’ve seen in DataCamp.  Let’s look at the same four examples mentioned above.

\sphinxstylestrong{Example 1:} The year and sales columns from \sphinxcode{\sphinxupquote{sales\_df}} do not form a function, because each year has multiple sales figures.  We can see this if we visualize them with a scatterplot.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(} \PYG{n}{sales\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{year}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{sales\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sales}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{bo}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)} \PYG{c+c1}{\PYGZsh{} blue circles}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{This is a relation,}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{so we use a scatterplot.}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontdict}\PYG{o}{=}\PYG{p}{\PYGZob{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{fontsize}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{25} \PYG{p}{\PYGZcb{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Year}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sales}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{chapter-10-visualization_6_0}.png}

The same example would have gone quite wrong if we had attempted to use a line plot instead, as you can see below.  Matplotlib tries to connect the dots in sequence to show a line, but it doesn’t make any sense, because the data is not a function.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(} \PYG{n}{sales\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{year}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{sales\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sales}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZhy{}o}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)} \PYG{c+c1}{\PYGZsh{} dots and lines}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{This is a relation, so we}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{should have used a scatterplot!}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontdict}\PYG{o}{=}\PYG{p}{\PYGZob{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{fontsize}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{25} \PYG{p}{\PYGZcb{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{chapter-10-visualization_8_0}.png}

It would have been even more hideous if the data hadn’t been sorted by year.  Let’s see what it would have been like if it had been sorted by employee instead, for instance.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{temp\PYGZus{}df} \PYG{o}{=} \PYG{n}{sales\PYGZus{}df}\PYG{o}{.}\PYG{n}{sort\PYGZus{}values}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{emp\PYGZus{}id}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(} \PYG{n}{temp\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{year}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{temp\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sales}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZhy{}o}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)} \PYG{c+c1}{\PYGZsh{} dots and lines}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{This is a relation, so we}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{should have used a scatterplot!}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontdict}\PYG{o}{=}\PYG{p}{\PYGZob{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{fontsize}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{25} \PYG{p}{\PYGZcb{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{chapter-10-visualization_10_0}.png}

The bad graphs just shown illustrate the importance of knowing whether your data is a function or relation, and choosing the appropriate plotting technique.  Let’s see how line plots can look nice when the data \sphinxstyleemphasis{is} a function.

\sphinxstylestrong{Example 2:} If we group the sales data by year, then each year appears only once, and the relationship between year and sales becomes a function.  Let’s use \sphinxcode{\sphinxupquote{sum()}} to do the grouping, so that we can see total sales by year.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{grouped\PYGZus{}df} \PYG{o}{=} \PYG{n}{sales\PYGZus{}df}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{year}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(} \PYG{n}{grouped\PYGZus{}df}\PYG{o}{.}\PYG{n}{index}\PYG{p}{,} \PYG{n}{grouped\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sales}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZhy{}o}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)} \PYG{c+c1}{\PYGZsh{} dots and lines}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{This is a function,}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{so we use a line plot.}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontdict}\PYG{o}{=}\PYG{p}{\PYGZob{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{fontsize}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{25} \PYG{p}{\PYGZcb{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Year}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Total Sales}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{chapter-10-visualization_12_0}.png}

That plot looks the way we expect.  It is especially sensible because the independent variable (\(x\) axis) is sequential, so it makes sense for us to think of the data as connected and flowing from left to right.

Note that if your data aren’t already sorted by the independent variable, connecting the dots with lines will jump all over your plot as it plots points in the wrong order.  Use \sphinxcode{\sphinxupquote{sort\_values()}} to get the data in the right order, in such a case.

Let’s consider one more example of a function and a non\sphinxhyphen{}function, but we’ll do them quickly.

\sphinxstylestrong{Example 3:} The Volume and High columns from \sphinxcode{\sphinxupquote{regi\_df}} may or may not be a function; it depends on the data we happened to get.  The \sphinxstyleemphasis{meanings} of the columns indicate that they probably are not a function, if given enough historical data.  So we’ll use a scatterplot.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(} \PYG{n}{regi\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Volume}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{regi\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{High}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{bo}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)} \PYG{c+c1}{\PYGZsh{} blue circles}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{This is a relation,}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{so we use a scatterplot.}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontdict}\PYG{o}{=}\PYG{p}{\PYGZob{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{fontsize}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{25} \PYG{p}{\PYGZcb{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Volume Traded}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{High Price}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{chapter-10-visualization_14_0}.png}

We can clearly see that there \sphinxstyleemphasis{might} be a collision in there of two \(x\) values having the same \(y\) value.  Even if they don’t, we certainly wouldn’t want to try connecting those dots with lines; it would be a meaningless mess.

\sphinxstylestrong{Example 4:} The index and the Close column from \sphinxcode{\sphinxupquote{regi\_df}} are a function, because each index represents a separate day, and thus only appears once in the data.  Let’s see.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(} \PYG{n}{regi\PYGZus{}df}\PYG{o}{.}\PYG{n}{index}\PYG{p}{,} \PYG{n}{regi\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Close}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZhy{}o}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)} \PYG{c+c1}{\PYGZsh{} dots and lines}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{This is a function,}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{so we use a line plot.}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontdict}\PYG{o}{=}\PYG{p}{\PYGZob{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{fontsize}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{25} \PYG{p}{\PYGZcb{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Date}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Closing Price}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{chapter-10-visualization_16_0}.png}


\section{But can my two columns of data look more awesome?}
\label{\detokenize{chapter-10-visualization:but-can-my-two-columns-of-data-look-more-awesome}}
Recall that the Seaborn library makes it easy to add histograms to both the horizontal and vertical axes of a standard ploot to get a better sense of the distribution.  This is possible with both line and scatter plots, but it is more commonly useful with scatterplots.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{jointplot}\PYG{p}{(} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Volume}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{High}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{regi\PYGZus{}df} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{chapter-10-visualization_18_0}.png}

If there were thousands of datapoints (or more), I suggest trying any of the following options.  I’ll illustrate some of them using the same data we just saw, even thought it doesn’t have thousands of points.
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Use \sphinxcode{\sphinxupquote{kind='kde'}} in a joint plot to smooth the histograms, as shown in the first plot below.

\item {} 
Use \sphinxcode{\sphinxupquote{alpha=0.5}} or an even smaller number, so that points in your scatterplot that stack up on top of one another show different levels of density throughout the graph.

\item {} 
Use \sphinxcode{\sphinxupquote{kind='hex'}} to bin values within the scatterplot as well, again showing the varying density throughout the plot, as in the second plot below.

\end{enumerate}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{jointplot}\PYG{p}{(} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Volume}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{High}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{regi\PYGZus{}df}\PYG{p}{,} \PYG{n}{kind}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{kde}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{chapter-10-visualization_20_0}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{jointplot}\PYG{p}{(} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Volume}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{High}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{regi\PYGZus{}df}\PYG{p}{,} \PYG{n}{kind}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{hex}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{chapter-10-visualization_21_0}.png}


\section{What if my two columns are very related?}
\label{\detokenize{chapter-10-visualization:what-if-my-two-columns-are-very-related}}
Seaborn provides a few tools for showing how one variable depends on another.

First, you can plot a line of best fit over a scatterplot, together with confidence bars for the predictions made by that linear model.  Recall from GB213 that it is not always sensible to fit a linear model to data.  But in cases where it makes sense, Seaborn makes it easy to visualize.

Keep in mind that Seaborn is quite happy to show you a linear model even when it does not make any sense to do so!  Just because Python will plot it for you does not mean that you should ask it to!  Here’s an example of just such a situation.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{lmplot}\PYG{p}{(} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Volume}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{High}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{regi\PYGZus{}df} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{This is a truly terrible idea!}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{\PYGZsq{}}
         \PYG{o}{+} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{This data is not remotely linear!}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{\PYGZsq{}}
         \PYG{o}{+} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{A linear model does not belong here!}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
           \PYG{n}{fontdict}\PYG{o}{=}\PYG{p}{\PYGZob{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{fontsize}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{15} \PYG{p}{\PYGZcb{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{chapter-10-visualization_23_0}.png}

Seaborn won’t show you the coefficients of the model, nor measure its goodness of fit; see {\hyperref[\detokenize{GB213-review-in-Python::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{the GB213 review}}}} for how to do those things in Python.

Of course, there are some situations where a linear model is reasonable, like the total sales over time plot from earlier.  Seaborn is fussy about using column names only in \sphinxcode{\sphinxupquote{lmplot}}, so we have to move the index in as an actual column here.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{grouped\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{year}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{grouped\PYGZus{}df}\PYG{o}{.}\PYG{n}{index}
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{lmplot}\PYG{p}{(} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{year}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sales}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{grouped\PYGZus{}df} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{A more reasonable time for a linear model}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
           \PYG{n}{fontdict}\PYG{o}{=}\PYG{p}{\PYGZob{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{fontsize}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{15} \PYG{p}{\PYGZcb{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{chapter-10-visualization_25_0}.png}

As you know from GB213, part of assessing whether linear regression is appropriate involves inspecting the residuals (the difference between each data point and the linear model).  Seaborn makes this easy, too.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{residplot}\PYG{p}{(} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{year}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sales}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{grouped\PYGZus{}df} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Residuals}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{chapter-10-visualization_27_0}.png}


\section{What if I have only one column of data?}
\label{\detokenize{chapter-10-visualization:what-if-i-have-only-one-column-of-data}}
The primary visualization tools appropriate for such a situation are variations on the idea of a histogram.  These include a standard histogram plus swarm plots, strip plots, and violin plots.  A secondary visualization in this situation is an ECDF, which we will return to below.

We can plot a standard histogram with \sphinxcode{\sphinxupquote{plt.hist()}}, but this doesn’t work very well for very small data sets.  It can also suffer from “binning bias,” which distorts the actual distribution through the approximation inherent in clustering points into bars.  But with many data points distributed smoothly along the horizontal axis, it often works well.

When labeling a histogram, the \(y\) axis is almost always “frequency” and the title should typically mention the idea of a “distribution.”

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{hist}\PYG{p}{(} \PYG{n}{sales\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sales}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sales}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Frequency}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Distribution of Quarterly Sales}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{chapter-10-visualization_29_0}.png}

Matplotlib’s built\sphinxhyphen{}in \sphinxcode{\sphinxupquote{plt.hist()}} works fine, but to upgrade your histogram game, consider checking out \sphinxhref{https://seaborn.pydata.org/generated/seaborn.distplot.html}{Seaborn’s \sphinxcode{\sphinxupquote{sns.distplot()}}}, which also shows histograms, but with handy options for commonly\sphinxhyphen{}desired additional features.

To remove the problem of binning bias, you can try a swarm plot.  This works well with a small\sphinxhyphen{}to\sphinxhyphen{}medium number of data points, but becomes unmanageable for large datasets, because it attempts to give each data point its own visual space.  Also, data points are just plotted \sphinxstyleemphasis{close} to where they actually belong, so the distortion of a histogram’s binning bias has been reduced, but not fully removed.  The picture is still an approximation of the actual data, but still much more accurate than a histogram.

Note that in a one\sphinxhyphen{}column swarm plot, there is no horizontal variable, and thus we do not label that axis.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{swarmplot}\PYG{p}{(} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sales}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{sales\PYGZus{}df} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Distribution of Quarterly Sales}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{chapter-10-visualization_31_0}.png}

A swarm plot can get quite wide if there are many data points clustered in a small area.  If your data has this problem, try using a strip plot, which keeps a constant width everywhere.

This comes at a price, however.  Some data points are stacked on top of one another, so you won’t really be able to see as much variation in density.  You can combat this problem by choosing \sphinxcode{\sphinxupquote{alpha=0.5}} or some smaller number, so that overlapping data points show variations in color.

Finally, a strip plot uses random jittering to place the points, so it won’t always look the same each time you render it!

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{stripplot}\PYG{p}{(} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sales}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{sales\PYGZus{}df} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Distribution of Quarterly Sales}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{chapter-10-visualization_33_0}.png}

Lastly, if you have enough data, you may want to simply smooth it out into curves instead.  This is not a faithful representation of sparse data, but it can be a faithful representation of a very large dataset.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{violinplot}\PYG{p}{(} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sales}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{sales\PYGZus{}df} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Distribution of Quarterly Sales}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{chapter-10-visualization_35_0}.png}

Finally, if you care only about the quartiles of the distribution (25\%, 50\%, 75\%) and the outliers, you can use a box plot.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{boxplot}\PYG{p}{(} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sales}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{sales\PYGZus{}df} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Distribution of Quarterly Sales}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{chapter-10-visualization_37_0}.png}

Every one of the options above can also be shown horizontally instead.  Just use \sphinxcode{\sphinxupquote{orient='h'}} in the plotting command.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{swarmplot}\PYG{p}{(} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sales}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{sales\PYGZus{}df}\PYG{p}{,} \PYG{n}{orient}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{h}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Distribution of Quarterly Sales}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{chapter-10-visualization_39_0}.png}


\section{Can’t I test a single column for normality?}
\label{\detokenize{chapter-10-visualization:can-t-i-test-a-single-column-for-normality}}
I’m so glad you asked!  One of the most common assumptions in statistics is that a dataset comes from an approximately normally distributed population.  We can get a sense of whether that holds true for some dataset we have by plotting the cumulative distribution function (CDF) of the data against that of a normal distribution, as you saw in DataCamp.  (A CDF from data is called an empirical CDF, or ECDF.)

While DataCamp did it manually, there are libraries that can handle it for you.  {\hyperref[\detokenize{chapter-9-math-and-stats::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{The notes for Chapter 9}}}} suggested a Learning On Your Own activity about Pingouin, a new Python statistics module, which implements QQ plots (quartile\sphinxhyphen{}quartile plots), for comparing two cumulative distribution functions.

Here, we’ll use what you saw in DataCamp.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}

\PYG{c+c1}{\PYGZsh{} create an ECDF from the data}
\PYG{n}{ecdf\PYGZus{}xs} \PYG{o}{=} \PYG{n}{sales\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sales}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{sort\PYGZus{}values}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{ecdf\PYGZus{}ys} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{ecdf\PYGZus{}xs}\PYG{p}{)}\PYG{o}{+}\PYG{l+m+mi}{1} \PYG{p}{)} \PYG{o}{/} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{ecdf\PYGZus{}xs}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} simulate a normal CDF with the same mean and std}
\PYG{n}{sample\PYGZus{}mean} \PYG{o}{=} \PYG{n}{ecdf\PYGZus{}xs}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{sample\PYGZus{}std} \PYG{o}{=} \PYG{n}{ecdf\PYGZus{}xs}\PYG{o}{.}\PYG{n}{std}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{samples} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(} \PYG{n}{sample\PYGZus{}mean}\PYG{p}{,} \PYG{n}{sample\PYGZus{}std}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{l+m+mi}{10000} \PYG{p}{)}
\PYG{n}{normal\PYGZus{}xs} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sort}\PYG{p}{(} \PYG{n}{samples} \PYG{p}{)}
\PYG{n}{normal\PYGZus{}ys} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{normal\PYGZus{}xs}\PYG{p}{)}\PYG{o}{+}\PYG{l+m+mi}{1} \PYG{p}{)} \PYG{o}{/} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{normal\PYGZus{}xs}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} plot them on the same graph}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(} \PYG{n}{normal\PYGZus{}xs}\PYG{p}{,} \PYG{n}{normal\PYGZus{}ys}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b\PYGZhy{}}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(} \PYG{n}{ecdf\PYGZus{}xs}\PYG{p}{,} \PYG{n}{ecdf\PYGZus{}ys}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{r\PYGZhy{}}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{chapter-10-visualization_41_0}.png}

This case is hard to judge visually.  The graphs are quite different for the leftmost 30\% of the graph, and somewhat different for the middle, only converging at the end.  If the project you’re working on is something quick and dirty that just needs to be approximate, you might call this distribution close enough to normal.  But if your project demands high accuracy, such as something in health care, you should resort to official statistical tests for normality of an empirical distribution.  We do not cover those in MA346.


\section{What if I have lots of columns of data?}
\label{\detokenize{chapter-10-visualization:what-if-i-have-lots-of-columns-of-data}}
If you want to compare them as distributions, then all of the Seaborn plotting commands from the previous section still apply.  They will show multiple distributions side\sphinxhyphen{}by\sphinxhyphen{}side, horizontally or vertically.  Here are two examples.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{swarmplot}\PYG{p}{(} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{emp\PYGZus{}id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sales}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{sales\PYGZus{}df} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Distribution of Quarterly Sales by Employee}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xticks}\PYG{p}{(} \PYG{n}{rotation}\PYG{o}{=}\PYG{l+m+mi}{90} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{chapter-10-visualization_44_0}.png}

When showing only one variable (earlier), a box plot was quite boring.  But when showing many variables, the simplicity of a box plot helps reduce visual clutter and make the variables much easier to compare.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{boxplot}\PYG{p}{(} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{emp\PYGZus{}id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sales}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{sales\PYGZus{}df} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Distribution of Quarterly Sales by Employee}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xticks}\PYG{p}{(} \PYG{n}{rotation}\PYG{o}{=}\PYG{l+m+mi}{90} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{chapter-10-visualization_46_0}.png}

What if we wanted to plot the four price distributions in the REGI dataset, the open, close, low, and high prices, side\sphinxhyphen{}by\sphinxhyphen{}side?  Right now, these are stored in three separate columns in the data.  But as you can see from the code above, Seaborn expects the data to be in a single column, and it will use a separate column to split the values into categories.

Of course, we know how to combine four columns of related data into one based on our work in a previous week–it’s melting!

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{melted\PYGZus{}df} \PYG{o}{=} \PYG{n}{regi\PYGZus{}df}\PYG{o}{.}\PYG{n}{melt}\PYG{p}{(} \PYG{n}{id\PYGZus{}vars}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Date}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{value\PYGZus{}vars}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Open}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Close}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Low}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{High}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}
                          \PYG{n}{var\PYGZus{}name}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Type of price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{value\PYGZus{}name}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Price}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{melted\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
       Date Type of price  Price
0  2\PYGZhy{}Jan\PYGZhy{}20          Open  27.21
1  3\PYGZhy{}Jan\PYGZhy{}20          Open  28.16
2  6\PYGZhy{}Jan\PYGZhy{}20          Open  28.53
3  7\PYGZhy{}Jan\PYGZhy{}20          Open  28.17
4  8\PYGZhy{}Jan\PYGZhy{}20          Open  26.37
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{swarmplot}\PYG{p}{(} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Type of price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{melted\PYGZus{}df} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Distribution of REGI Prices}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{chapter-10-visualization_49_0}.png}

And you can use the old, trusty histogram to compare distributions as well.  Simply pass an array of Series instead of just one Series when calling \sphinxcode{\sphinxupquote{plt.hist()}}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{hist}\PYG{p}{(} \PYG{p}{[} \PYG{n}{regi\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Open}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{regi\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Close}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{p}{]}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{p}{[} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Open}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Close}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{]} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{chapter-10-visualization_51_0}.png}

The REGI dataset is already set up for us to do this, because each distribution is in its own column.  If it had not been so (but had been like the sales data, for instance), recall that the opposite of melting is pivoting, and that would get the data in the needed form.

It’s also possible to do overlapping histograms with transparent bars, but to get it to look good, you need to create the bin boundaries in advance and tell each histogram to use the same boundaries.  Otherwise, \sphinxcode{\sphinxupquote{plt.hist()}} will choose different bins for each Series of data.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{bins} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(} \PYG{l+m+mi}{15}\PYG{p}{,} \PYG{l+m+mi}{35}\PYG{p}{,} \PYG{l+m+mi}{21} \PYG{p}{)} \PYG{c+c1}{\PYGZsh{} 20 bins from x=15 to x=35}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{hist}\PYG{p}{(} \PYG{n}{regi\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Open}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{bins}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Open}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.5}\PYG{p}{,} \PYG{n}{edgecolor}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{black}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{hist}\PYG{p}{(} \PYG{n}{regi\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Close}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{bins}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Close}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.5}\PYG{p}{,} \PYG{n}{edgecolor}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{black}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{chapter-10-visualization_53_0}.png}

There’s a lot more that could be said about plotting distributions; for instance, \sphinxhref{https://towardsdatascience.com/sorry-but-sns-distplot-just-isnt-good-enough-this-is-though-ef2ddbf28078}{here’s a cool blog post} about how to make an even more beautiful plot that compares several distributions.


\section{What if I need to know if the colums are related?}
\label{\detokenize{chapter-10-visualization:what-if-i-need-to-know-if-the-colums-are-related}}
DataCamp showed you two visualizations for this.  One focuses on giving you some visual intuition for whether the variables are related, by showing you the shape of all possible scatterplots of your data.  It’s called a pair plot because it pairs up the variables in every possible way.  Let’s try it on the REGI dataset; the explanation follows the picture.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{pairplot}\PYG{p}{(} \PYG{n}{regi\PYGZus{}df} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{chapter-10-visualization_56_0}.png}

The histograms shown along the diagonal of this graph are histograms of each variable, which are not the interesting part of the visualization.

Next, take a look at the scatterplots that are \sphinxstyleemphasis{not} in the last row or last column.  Almost all of them show a very tight linear relationship, but this is unsurprising because of the meaning of the data.  For instance, the leftmost scatterplot in the second row relates the High price of a stock with the Open price of the stock on the same day.  Because the stock opens and closes at approximately the same price on most days (no enormous fluctuations in any one day), these numbers are always close together, and thus highly correlated.  The same goes for all the histograms except the final row and final column.

The final row and final column include the Volume variable.  One might naturally wonder whether the volume of the stock traded on a day correlates to anything about the value of the stock on that day.  In the case of Renewable Energy Group, Inc., for the first half of 2020, the answer seems to be no.  There does not seem to be any discernable relationship in those histograms; they’re just fuzzy blobs of data points.

Earlier I mentioned that \sphinxcode{\sphinxupquote{sns.pairplot()}} was the technique that would give us some visual intuition for relationships, and it did.  But there is another visualiation technique that doesn’t show us as much visually, but gives us more easy\sphinxhyphen{}to\sphinxhyphen{}read measurements of the relationships among the variables.  It’s a heat map of the covariance matrix.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{numeric\PYGZus{}columns\PYGZus{}only} \PYG{o}{=} \PYG{n}{regi\PYGZus{}df}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Date}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1} \PYG{p}{)}
\PYG{n}{correlation\PYGZus{}coefficients} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{corrcoef}\PYG{p}{(} \PYG{n}{numeric\PYGZus{}columns\PYGZus{}only}\PYG{p}{,} \PYG{n}{rowvar}\PYG{o}{=}\PYG{k+kc}{False} \PYG{p}{)}
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{heatmap}\PYG{p}{(} \PYG{n}{correlation\PYGZus{}coefficients}\PYG{p}{,} \PYG{n}{annot}\PYG{o}{=}\PYG{k+kc}{True} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xticks}\PYG{p}{(} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{6}\PYG{p}{)}\PYG{o}{+}\PYG{l+m+mf}{0.5}\PYG{p}{,} \PYG{n}{numeric\PYGZus{}columns\PYGZus{}only}\PYG{o}{.}\PYG{n}{columns} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{yticks}\PYG{p}{(} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{6}\PYG{p}{)}\PYG{o}{+}\PYG{l+m+mf}{0.5}\PYG{p}{,} \PYG{n}{numeric\PYGZus{}columns\PYGZus{}only}\PYG{o}{.}\PYG{n}{columns}\PYG{p}{,} \PYG{n}{rotation}\PYG{o}{=}\PYG{l+m+mi}{0} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{chapter-10-visualization_58_0}.png}

Of course, because we used the same data, we still find out that all the prices are highly correlated (because they’re organized by day) and the volume isn’t really correlated much with anything.  But it’s much easier to tell both the correlations and the lacks of correlation when we have hard numbers to look at, rather than having to estimate it ourselves from shapes.


\section{Summary of plotting tools}
\label{\detokenize{chapter-10-visualization:summary-of-plotting-tools}}
I know that was a huge amount to take in!  So let’s make it simpler:


\subsection{With one numeric column of data:}
\label{\detokenize{chapter-10-visualization:with-one-numeric-column-of-data}}

\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\sphinxstyletheadfamily 
If you want to see this
&\sphinxstyletheadfamily 
Then use this
\\
\hline
Just the distribution’s quartiles and outliers
&
Box plot
\\
\hline
Simple approximation of the distribution
&
Histogram
\\
\hline
Very good approximation of the distribution, maybe very wide
&
Swarm plot
\\
\hline
Good approximation of the distribution, not too wide
&
Strip plot
\\
\hline
Good approximation of a large distribution, smoothed
&
Violin plot
\\
\hline
How similar is the distribution to normal?
&
Overlapping ECDFs
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}


\subsection{With two numeric columns of data:}
\label{\detokenize{chapter-10-visualization:with-two-numeric-columns-of-data}}

\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\sphinxstyletheadfamily 
If you want to see this
&\sphinxstyletheadfamily 
Then use this
\\
\hline
A graph of my data, which is a function
&
Line plot
\\
\hline
The shape of my data, which is a relation
&
Scatter plot
\\
\hline
The shape of my data, which is a relation, plus each variable’s distribution
&
Joint plot
\\
\hline
The line of best fit through my data
&
\sphinxcode{\sphinxupquote{sns.lmplot}}
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}


\subsection{With many numeric columns of data:}
\label{\detokenize{chapter-10-visualization:with-many-numeric-columns-of-data}}

\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\sphinxstyletheadfamily 
If you want to see this
&\sphinxstyletheadfamily 
Then use this
\\
\hline
The quartiles and outliers of each
&
Side\sphinxhyphen{}by\sphinxhyphen{}side box plots
\\
\hline
Simple approximation of the distributions
&
Histograms with side\sphinxhyphen{}by\sphinxhyphen{}side bars
\\
\hline
Very good approximation of each distribution (can’t fit too many)
&
Side\sphinxhyphen{}by\sphinxhyphen{}side swarm plots
\\
\hline
Good approximation of each distribution (can fit more)
&
Side\sphinxhyphen{}by\sphinxhyphen{}side strip plots
\\
\hline
Good approximation if the distributions are large (will be smoothed)
&
Side\sphinxhyphen{}by\sphinxhyphen{}side violin plots
\\
\hline
The shape of all possible two\sphinxhyphen{}column relationships
&
Pair plot
\\
\hline
A measurement of all possible correlations
&
Heat map of correlation coefficients
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}


\section{Techniques \sphinxstyleemphasis{not} to use (and why)}
\label{\detokenize{chapter-10-visualization:techniques-not-to-use-and-why}}
You may notice that we did not cover \sphinxstylestrong{pie charts} anywhere in this tutorial.  Matplotlib can certainly produce pie charts for you, but visualization experts recommend against them, because viewers tend to have trouble assessing the exact meanings of the shapes.  It’s much harder to compare how much bigger one pie slice is to another than it is to compare, say, two bars on a histogram, or to points on a graph.  So I suggest you avoid pie charts.

We also did not cover \sphinxstylestrong{bubble charts} anywhere in this tutorial.  (A bubble chart is one in which each data point is plotted by a large circle, proportional to one of the variables in the data.)  These are very popular in modern data visualization because they are eye\sphinxhyphen{}catching and attractive.  But visualization experts recommend against these as well, because each person perceives the bubble sizes differently.  For example, some people perceive the magnitude of a bubble on a graph in proportion to its radius, some perceive it in proportion to its area, and others are somewhere in between.

Visualization is a type of communication, and doing it well means focusing on the message you want to convey.  Using a visualization that gives each viewer a different message is a bad idea.  Unpredictability of viewer response is undesirable.  So I suggest you avoid bubble charts as well.

We did not cover charts with \sphinxstylestrong{3D elements,} as Microsoft Excel often creates.  This is because those elements also tend to distort the viewer’s perception of the data and make it unclear exactly how extreme (or not) they’re perceiving what you’re showing.  Thus we avoid any 3D elements in charts for the same reason we avoid bubble charts.

Finally, DataCamp showed you how to fit polynomial models to data using \sphinxcode{\sphinxupquote{sns.regplot()}}.  But I did not cover it hear, because it is dangerous to dive into polynomial models without a solid grounding in mathematical modeling, which this course does not cover.  Before using a polynomial model, you would need a solid, domain\sphinxhyphen{}specific reason to believe that such a model is applicable, or \sphinxcode{\sphinxupquote{sns.regplot()}} will (obediently) produce result that are unreliable if used for prediction.  Consequently, I won’t cover \sphinxcode{\sphinxupquote{sns.regplot()}} in MA346.


\section{What about plot styles?}
\label{\detokenize{chapter-10-visualization:what-about-plot-styles}}
I didn’t cover plot styles here, but there’s nothing wrong with them.  I simply left them out because most of them are only cosmetic; \sphinxhref{big-cheat-sheet\#before-week-5}{see this week’s section in the DataCamp cheat sheet} for details on items like \sphinxcode{\sphinxupquote{sns.set()}}, \sphinxcode{\sphinxupquote{plt.subplot()}}, and \sphinxcode{\sphinxupquote{plt.style}}.

There are also some good blog posts on Matplotlib styles you might want to check out, such as \sphinxhref{https://towardsdatascience.com/the-last-matplotlib-tweaking-guide-youll-ever-need-dbd4374a1c1e}{this} or \sphinxhref{https://towardsdatascience.com/cyberpunk-style-with-matplotlib-f47404c9d4c5}{this}.

But there is one stylistic element I want to highlight:  DataCamp showed that \sphinxcode{\sphinxupquote{plt.annotate()}} can be used to place text on a plot, which can be very useful for drawing a viewer’s attention to the part of the graph that you want them to focus on.  Consider the following graph, which we produced earlier, but now with a prominent annotation to explain why sales were so high one year.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(} \PYG{n}{grouped\PYGZus{}df}\PYG{o}{.}\PYG{n}{index}\PYG{p}{,} \PYG{n}{grouped\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sales}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZhy{}o}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)} \PYG{c+c1}{\PYGZsh{} dots and lines}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Yearly Sales}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontdict}\PYG{o}{=}\PYG{p}{\PYGZob{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{fontsize}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{25} \PYG{p}{\PYGZcb{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Year}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Total Sales}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylim}\PYG{p}{(} \PYG{p}{[} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{10000} \PYG{p}{]} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{annotate}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Competitor}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{flooded}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{xy}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{2017.5}\PYG{p}{,}\PYG{l+m+mi}{8000}\PYG{p}{)}\PYG{p}{,}
              \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{l+m+mi}{15}\PYG{p}{,} \PYG{n}{ha}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{right}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{chapter-10-visualization_63_0}.png}


\section{There’s so much more!}
\label{\detokenize{chapter-10-visualization:there-s-so-much-more}}
Because visualization is a huge topic, I list several Learning On Your Own opportunities for extending your visualization knowledge and sharing it with the rest of the class.

\begin{sphinxadmonition}{note}{Learning on Your Own \sphinxhyphen{} Plot with Less Code}

In some cases, you can plot data directly from pandas without needing to use Matplotlib.  \sphinxhref{https://towardsdatascience.com/the-simplest-way-to-create-complex-visualizations-in-python-isnt-with-matplotlib-a5802f2dba92}{Investigate this blog post for details} and decide on the best format by which to report that information to the class.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Learning on Your Own \sphinxhyphen{} Geographical Plots}

Drawing data on a map is extremely common and useful, but we don’t have time to cover it in today’s notes.  \sphinxhref{https://www.earthdatascience.org/tutorials/introduction-to-leaflet-animated-maps/}{Here’s a blog post about an easy way to do so in Python,} but you don’t need to feel bound to that one.  There are many map toolkits for use in Python\sphinxhyphen{}based visualizations.  Feel free to choose the one you like best and decide on the best format by which to report on it to the class.  As an example, try showing how housing costs vary across the U.S. by plotting the property values in the mortgage dataset from Week 3 on a map.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Learning on Your Own \sphinxhyphen{} Tableau}

One of the most famous tools for data visualization in industry is Tableau.  Although coding in Python, R, etc., is always the most flexible option, tools like Tableau are far easier and faster when you don’t need maximal flexibility.  Take a Tableau tutorial and report to the class on its key features.  Ensure you cover how to get a copy of Tableau, how to get data into it, and what it’s best at.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Learning on Your Own \sphinxhyphen{} Visualization Design Principles}

I’ve suggested a few concepts {\hyperref[\detokenize{chapter-10-visualization:techniques-not-to-use-and-why}]{\emph{up above}}} that can guide you towards effective visualizations and away from ineffective ones.  But there is a lot to learn about visualization design principles that we can’t cover here.  Consider checking out \sphinxhref{https://flowingdata.com/2017/01/24/one-dataset-visualized-25-ways/}{this blog post} or \sphinxhref{https://serialmentor.com/dataviz/}{this free online book} and chooosing about five important concepts you learn that are relevant to our work in MA346.  Find a good way to report them to the rest of the class, and be sure to include plenty of visual examples in your work of what to do and what not to do.
\end{sphinxadmonition}


\chapter{Processing the Rows of a DataFrame}
\label{\detokenize{chapter-11-processing-rows:processing-the-rows-of-a-dataframe}}\label{\detokenize{chapter-11-processing-rows::doc}}
See also the slides that summarize a portion of this content.


\section{Goal}
\label{\detokenize{chapter-11-processing-rows:goal}}
Back in the early days of programming, when I was a kid, we wrote code with stone tools.

\sphinxincludegraphics{{cave-man-public-domain}.jpg}

And when we wanted to work with all the elements of an array, we had no choice but to write a loop.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{shipments\PYGZus{}received} \PYG{o}{=} \PYG{p}{[} \PYG{l+m+mi}{6}\PYG{p}{,} \PYG{l+m+mi}{9}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{7}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{13} \PYG{p}{]}

\PYG{n}{total} \PYG{o}{=} \PYG{l+m+mi}{0}
\PYG{k}{for} \PYG{n}{num\PYGZus{}received} \PYG{o+ow}{in} \PYG{n}{shipments\PYGZus{}received}\PYG{p}{:}
    \PYG{n}{total} \PYG{o}{+}\PYG{o}{=} \PYG{n}{num\PYGZus{}received}

\PYG{n}{total}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
68
\end{sphinxVerbatim}

Most introductory programming courses teach loops, and for good reason; they show up a lot in programming!  But there are a few reasons we’ll try to avoid loops in data work whenever we can.

The lesser reason is \sphinxstylestrong{readability.}  Loops are always at least two lines of code in Python; the one above is three because it has to initialize the \sphinxcode{\sphinxupquote{total}} variable to zero.  Many alternatives to loops can be done in just one line of code, which is more readable.

The more important reason is \sphinxstylestrong{speed.}  Loops in Python are not very efficient, and this can be a serious problem.  In the final project for MA346 in Spring 2020, many students came to my office hours with a loop that had been running for hours, and they didn’t know if or when it would finish.  There are \sphinxstyleemphasis{many} ways to speed loops up, sometimes by just altering the loop, but usually by replacing the loop with something else entirely.

\sphinxstylestrong{In fact, that’s the purpose of this chapter:  \sphinxstyleemphasis{What can I do to improve a slow loop?}}

The title of the chapter mentions DataFrames specifically, because in data work we’re almost always processing a DataFrame row\sphinxhyphen{}by\sphinxhyphen{}row.  But many of the techniques we’ll cover apply to many different kinds of loops, with or without DataFrames.

An added benefit is that improving (or replacing) loops with something faster often means writing shorter or clearer code as well, achieving improvements in readability at the same time.


\section{The \sphinxstyleliteralintitle{\sphinxupquote{apply()}} function}
\label{\detokenize{chapter-11-processing-rows:the-apply-function}}
The most common use of a loop is when we need to do the same thing to each element of a sequence of values.  Let’s see an example.


\subsection{Baseball example}
\label{\detokenize{chapter-11-processing-rows:baseball-example}}
In an earlier homework assignment, I provided a cleaned dataset of baseball players’ salaries.  Let’s take a look at the original version of the dataset when I downloaded it \sphinxhref{https://data.world/natereed/baseball-salaries}{from the web}, before it was cleaned.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZus{}static/baseball\PYGZhy{}salaries.csv}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
        salary               name  total\PYGZus{}value pos     years   avg\PYGZus{}annual team
0  \PYGZdl{} 3,800,000  Darryl Strawberry  \PYGZdl{} 3,800,000  OF  1 (1991)  \PYGZdl{} 3,800,000  LAD
1  \PYGZdl{} 3,750,000     Kevin Mitchell  \PYGZdl{} 3,750,000  OF  1 (1991)  \PYGZdl{} 3,750,000   SF
2  \PYGZdl{} 3,750,000         Will Clark  \PYGZdl{} 3,750,000  1B  1 (1991)  \PYGZdl{} 3,750,000   SF
3  \PYGZdl{} 3,625,000         Mark Davis  \PYGZdl{} 3,625,000   P  1 (1991)  \PYGZdl{} 3,625,000   KC
4  \PYGZdl{} 3,600,000         Eric Davis  \PYGZdl{} 3,600,000  OF  1 (1991)  \PYGZdl{} 3,600,000  CIN
\end{sphinxVerbatim}

The “years” column looks particularly annoying.  Why does it say “1 (1991)” instead of just 1991?  Let’s take a look at some other rows…

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{l+m+mi}{14440}\PYG{p}{:}\PYG{l+m+mi}{14445}\PYG{p}{,}\PYG{p}{:}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
             salary             name    total\PYGZus{}value pos         years  \PYGZbs{}
14440     \PYGZdl{} 100,000     Steve Monson      \PYGZdl{} 100,000   P      1 (1990)   
14441  \PYGZdl{} 28,000,000   Alex Rodriguez  \PYGZdl{} 275,000,000  DH  10 (2008\PYGZhy{}17)   
14442     \PYGZdl{} 200,000   Mike Colangelo      \PYGZdl{} 200,000  OF      1 (1999)   
14443     \PYGZdl{} 200,000  Mike Jerzembeck      \PYGZdl{} 200,000   P      1 (1999)   
14444  \PYGZdl{} 21,680,727   Alex Rodriguez   \PYGZdl{} 21,680,727  3B      1 (2006)   

         avg\PYGZus{}annual team  
14440     \PYGZdl{} 100,000  MIL  
14441  \PYGZdl{} 27,500,000  NYY  
14442     \PYGZdl{} 200,000  LAA  
14443     \PYGZdl{} 200,000  NYY  
14444  \PYGZdl{} 21,680,727  NYY  
\end{sphinxVerbatim}

Aha, some entries in the “years” column represent multiple years.  We might naturally want to split that column up into three columns: number of years, first year, and last year.  Each is a little project all on its own, but we just want to look at one example, so let’s consider just the task of extracting the first year from the text.  If we wrote a loop, it might go something like this.


\subsection{Using a loop}
\label{\detokenize{chapter-11-processing-rows:using-a-loop}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{first\PYGZus{}years} \PYG{o}{=} \PYG{p}{[} \PYG{p}{]}
\PYG{k}{for} \PYG{n}{text} \PYG{o+ow}{in} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{years}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{:}
    \PYG{k}{if} \PYG{n}{text}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} one\PYGZhy{}digit number of years}
        \PYG{n}{first\PYGZus{}years}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(} \PYG{n+nb}{int}\PYG{p}{(} \PYG{n}{text}\PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{:}\PYG{l+m+mi}{7}\PYG{p}{]} \PYG{p}{)} \PYG{p}{)}
    \PYG{k}{else}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} two\PYGZhy{}digit number of years}
        \PYG{n}{first\PYGZus{}years}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(} \PYG{n+nb}{int}\PYG{p}{(} \PYG{n}{text}\PYG{p}{[}\PYG{l+m+mi}{4}\PYG{p}{:}\PYG{l+m+mi}{8}\PYG{p}{]} \PYG{p}{)} \PYG{p}{)}
\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{first\PYGZus{}year}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{first\PYGZus{}years}
        
\PYG{n}{df}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{14441}\PYG{p}{]}\PYG{p}{,}\PYG{p}{:}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} quick spot check of our work}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
             salary               name    total\PYGZus{}value pos         years  \PYGZbs{}
0       \PYGZdl{} 3,800,000  Darryl Strawberry    \PYGZdl{} 3,800,000  OF      1 (1991)   
14441  \PYGZdl{} 28,000,000     Alex Rodriguez  \PYGZdl{} 275,000,000  DH  10 (2008\PYGZhy{}17)   

         avg\PYGZus{}annual team  first\PYGZus{}year  
0       \PYGZdl{} 3,800,000  LAD        1991  
14441  \PYGZdl{} 27,500,000  NYY        2008  
\end{sphinxVerbatim}

A loop over a list of values is what pandas’ \sphinxcode{\sphinxupquote{apply()}} function was made for.  You write \sphinxcode{\sphinxupquote{df{[}'column'{]}.apply(f)}} to apply the function \sphinxcode{\sphinxupquote{f}} to every entry in the chosen column.  For example, we could simplify our work above as follows.  The differences are noted in the comments.


\subsection{Using \sphinxstyleliteralintitle{\sphinxupquote{apply()}}}
\label{\detokenize{chapter-11-processing-rows:using-apply}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
                                 \PYG{c+c1}{\PYGZsh{} No need to start with an empty list.}
\PYG{k}{def} \PYG{n+nf}{get\PYGZus{}first\PYGZus{}year} \PYG{p}{(} \PYG{n}{text} \PYG{p}{)}\PYG{p}{:}     \PYG{c+c1}{\PYGZsh{} Function name helps explain the code.}
    \PYG{k}{if} \PYG{n}{text}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}
        \PYG{k}{return} \PYG{n+nb}{int}\PYG{p}{(} \PYG{n}{text}\PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{:}\PYG{l+m+mi}{7}\PYG{p}{]} \PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Clearer and shorter than append().}
    \PYG{k}{else}\PYG{p}{:}
        \PYG{k}{return} \PYG{n+nb}{int}\PYG{p}{(} \PYG{n}{text}\PYG{p}{[}\PYG{l+m+mi}{4}\PYG{p}{:}\PYG{l+m+mi}{8}\PYG{p}{]} \PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Clearer and shorter than append().}
\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{first\PYGZus{}year}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{years}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{apply}\PYG{p}{(} \PYG{n}{get\PYGZus{}first\PYGZus{}year} \PYG{p}{)}

\PYG{n}{df}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{14441}\PYG{p}{]}\PYG{p}{,}\PYG{p}{:}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} same check as before}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
             salary               name    total\PYGZus{}value pos         years  \PYGZbs{}
0       \PYGZdl{} 3,800,000  Darryl Strawberry    \PYGZdl{} 3,800,000  OF      1 (1991)   
14441  \PYGZdl{} 28,000,000     Alex Rodriguez  \PYGZdl{} 275,000,000  DH  10 (2008\PYGZhy{}17)   

         avg\PYGZus{}annual team  first\PYGZus{}year  
0       \PYGZdl{} 3,800,000  LAD        1991  
14441  \PYGZdl{} 27,500,000  NYY        2008  
\end{sphinxVerbatim}

If we’re honest, the code didn’t get \sphinxstyleemphasis{that} much simpler.  But \sphinxcode{\sphinxupquote{apply()}} is especially nice if the function we want to write is a function that already exists.  Here’s a silly example, but it illustrates the point.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{name\PYGZus{}length}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{apply}\PYG{p}{(} \PYG{n+nb}{len} \PYG{p}{)}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
        salary               name  total\PYGZus{}value pos     years   avg\PYGZus{}annual  \PYGZbs{}
0  \PYGZdl{} 3,800,000  Darryl Strawberry  \PYGZdl{} 3,800,000  OF  1 (1991)  \PYGZdl{} 3,800,000   
1  \PYGZdl{} 3,750,000     Kevin Mitchell  \PYGZdl{} 3,750,000  OF  1 (1991)  \PYGZdl{} 3,750,000   
2  \PYGZdl{} 3,750,000         Will Clark  \PYGZdl{} 3,750,000  1B  1 (1991)  \PYGZdl{} 3,750,000   
3  \PYGZdl{} 3,625,000         Mark Davis  \PYGZdl{} 3,625,000   P  1 (1991)  \PYGZdl{} 3,625,000   
4  \PYGZdl{} 3,600,000         Eric Davis  \PYGZdl{} 3,600,000  OF  1 (1991)  \PYGZdl{} 3,600,000   

  team  first\PYGZus{}year  name\PYGZus{}length  
0  LAD        1991           17  
1   SF        1991           14  
2   SF        1991           10  
3   KC        1991           10  
4  CIN        1991           10  
\end{sphinxVerbatim}

Using \sphinxcode{\sphinxupquote{apply()}} will run a little faster than writing your own loop, but unless the DataFrame is really huge, you probably won’t notice the difference, so speed is not a significant concern here.  But switching to the \sphinxcode{\sphinxupquote{apply()}} form sets us up nicely for a later speed improvement {\hyperref[\detokenize{chapter-11-processing-rows:parallel-apply}]{\emph{we’ll discuss further below}}}.

Although it’s less often useful, you can use \sphinxcode{\sphinxupquote{df.apply(f)}} to run \sphinxcode{\sphinxupquote{f}} on each column of the DataFrame, or \sphinxcode{\sphinxupquote{df.apply(f,axis=1)}} to run \sphinxcode{\sphinxupquote{f}} on each row of the DataFrame.

There is, unfortunately, a related function \sphinxcode{\sphinxupquote{map()}}.  It behaves very similarly to \sphinxcode{\sphinxupquote{apply()}}, with a few subtle differences.  This is unfortunate because in computer programming more broadly, the concepts of “map” and “apply” are often used synonymously/interchangeably.  So to have them behave almost the same (but slightly differently!) in pandas is unfortunate.  Oh well.  Here are the differences:


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|}
\hline
\sphinxstyletheadfamily 
Feature
&\sphinxstyletheadfamily 
\sphinxcode{\sphinxupquote{apply()}}
&\sphinxstyletheadfamily 
\sphinxcode{\sphinxupquote{map()}}
\\
\hline
You can use it on DataFrames, as in \sphinxcode{\sphinxupquote{df.apply(f)}}
&
Yes
&
No
\\
\hline
You can provide extra \sphinxcode{\sphinxupquote{args}} or \sphinxcode{\sphinxupquote{kwargs}}
&
Yes
&
No
\\
\hline
You can use a dictionary instead of \sphinxcode{\sphinxupquote{f}}
&
No
&
Yes
\\
\hline
You can ask it to skip NaNs
&
No
&
Yes
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\begin{sphinxadmonition}{note}{Big Picture \sphinxhyphen{} Informally, map is the same as apply}

In most programming contexts, including data work, if someone speaks of “mapping” or “applying” a function, they mean the same thing:  Automatically running the function on each element of a list or series.
\begin{itemize}
\item {} 
The function for this is often called \sphinxcode{\sphinxupquote{map()}} or \sphinxcode{\sphinxupquote{apply()}}, as in pandas, but not always.

\item {} 
In mathematics, it’s called using a function “elementwise,” meaning on each element of a structure separately.

\item {} 
In the popular language Julia, it’s called “broadcasting” a function over an array or table.

\end{itemize}
\end{sphinxadmonition}

The function that you give to \sphinxcode{\sphinxupquote{apply()}} can’t be just any function.  Its input type needs to match the data type of the individual elements in the Series or DataFrame you’re applying it to.  Its output type will determine what kind of output you get.  For example, the \sphinxcode{\sphinxupquote{get\_first\_year()}} function defined above takes strings as input and gives integers as output.  So using \sphinxcode{\sphinxupquote{apply(get\_first\_year)}} will need to be done on a Series containing strings, and will produce a Series containing integers.

If you have a function that takes multiple inputs, you might want to bind some of the arguments so that it becomes a unary function and can be used in \sphinxcode{\sphinxupquote{apply()}}.  Or you can use the \sphinxcode{\sphinxupquote{args}} or \sphinxcode{\sphinxupquote{kwargs}} feature of \sphinxcode{\sphinxupquote{apply()}}, but we won’t cover that in these course notes.  You can see a small example in \sphinxhref{https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.apply.html}{the pandas documentation}.  We will, however, take a look at the possibility of using a dictionary with \sphinxcode{\sphinxupquote{map()}}, because it is extremely useful.  We will consider a simple example application, but do a more sophisticated one in class.


\subsection{Using \sphinxstyleliteralintitle{\sphinxupquote{map()}}}
\label{\detokenize{chapter-11-processing-rows:using-map}}
Let’s assume that the analysis we wanted to do cared only about whether the baseball player had an infield position (IF), outfield position (OF), was a pitcher (P), or a designated hitter (DH), and we didn’t care about any other details of the position (such as first base vs. second base, or starting pitcher vs. relief pitcher).  We’d therefore like to simplify the “pos” column and convert all infield positions to IF, and so on.  First, let’s see what all the positions are.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{pos}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{unique}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
array([\PYGZsq{}OF\PYGZsq{}, \PYGZsq{}1B\PYGZsq{}, \PYGZsq{}P\PYGZsq{}, \PYGZsq{}DH\PYGZsq{}, \PYGZsq{}3B\PYGZsq{}, \PYGZsq{}2B\PYGZsq{}, \PYGZsq{}C\PYGZsq{}, \PYGZsq{}SS\PYGZsq{}, \PYGZsq{}RF\PYGZsq{}, \PYGZsq{}SP\PYGZsq{}, \PYGZsq{}LF\PYGZsq{},
       \PYGZsq{}CF\PYGZsq{}, \PYGZsq{}RP\PYGZsq{}], dtype=object)
\end{sphinxVerbatim}

We could convert them with a big \sphinxcode{\sphinxupquote{if}} statement, like you see here, but this is tedious and repetitive code.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{simpler\PYGZus{}position} \PYG{p}{(} \PYG{n}{pos} \PYG{p}{)}\PYG{p}{:}   \PYG{c+c1}{\PYGZsh{} BAD STYLE.  See better version below.}
    \PYG{k}{if} \PYG{n}{pos} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{P}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}  \PYG{k}{return} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{P}\PYG{l+s+s1}{\PYGZsq{}}
    \PYG{k}{if} \PYG{n}{pos} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{SP}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{k}{return} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{P}\PYG{l+s+s1}{\PYGZsq{}}
    \PYG{k}{if} \PYG{n}{pos} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{RP}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{k}{return} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{P}\PYG{l+s+s1}{\PYGZsq{}}
    \PYG{k}{if} \PYG{n}{pos} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{C}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}  \PYG{k}{return} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{IF}\PYG{l+s+s1}{\PYGZsq{}}
    \PYG{k}{if} \PYG{n}{pos} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{1B}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{k}{return} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{IF}\PYG{l+s+s1}{\PYGZsq{}}
    \PYG{k}{if} \PYG{n}{pos} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{2B}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{k}{return} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{IF}\PYG{l+s+s1}{\PYGZsq{}}
    \PYG{k}{if} \PYG{n}{pos} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{3B}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{k}{return} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{IF}\PYG{l+s+s1}{\PYGZsq{}}
    \PYG{k}{if} \PYG{n}{pos} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{SS}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{k}{return} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{IF}\PYG{l+s+s1}{\PYGZsq{}}
    \PYG{k}{if} \PYG{n}{pos} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{OF}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{k}{return} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{OF}\PYG{l+s+s1}{\PYGZsq{}}
    \PYG{k}{if} \PYG{n}{pos} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{LF}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{k}{return} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{OF}\PYG{l+s+s1}{\PYGZsq{}}
    \PYG{k}{if} \PYG{n}{pos} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{CF}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{k}{return} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{OF}\PYG{l+s+s1}{\PYGZsq{}}
    \PYG{k}{if} \PYG{n}{pos} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{RF}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{k}{return} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{OF}\PYG{l+s+s1}{\PYGZsq{}}
    \PYG{k}{if} \PYG{n}{pos} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{DH}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{k}{return} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{DH}\PYG{l+s+s1}{\PYGZsq{}}

\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{simple\PYGZus{}pos}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{pos}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{apply}\PYG{p}{(} \PYG{n}{simpler\PYGZus{}position} \PYG{p}{)}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
        salary               name  total\PYGZus{}value pos     years   avg\PYGZus{}annual  \PYGZbs{}
0  \PYGZdl{} 3,800,000  Darryl Strawberry  \PYGZdl{} 3,800,000  OF  1 (1991)  \PYGZdl{} 3,800,000   
1  \PYGZdl{} 3,750,000     Kevin Mitchell  \PYGZdl{} 3,750,000  OF  1 (1991)  \PYGZdl{} 3,750,000   
2  \PYGZdl{} 3,750,000         Will Clark  \PYGZdl{} 3,750,000  1B  1 (1991)  \PYGZdl{} 3,750,000   
3  \PYGZdl{} 3,625,000         Mark Davis  \PYGZdl{} 3,625,000   P  1 (1991)  \PYGZdl{} 3,625,000   
4  \PYGZdl{} 3,600,000         Eric Davis  \PYGZdl{} 3,600,000  OF  1 (1991)  \PYGZdl{} 3,600,000   

  team  first\PYGZus{}year  name\PYGZus{}length simple\PYGZus{}pos  
0  LAD        1991           17         OF  
1   SF        1991           14         OF  
2   SF        1991           10         IF  
3   KC        1991           10          P  
4  CIN        1991           10         OF  
\end{sphinxVerbatim}

All the repetitive code is just establishing a simple relationship among some very short strings.  We could store that same relationship in a dictionary with many fewer lines of code.  Note that we must use \sphinxcode{\sphinxupquote{map()}}, because \sphinxcode{\sphinxupquote{apply()}} doesn’t accept dictionaries.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{simple\PYGZus{}pos}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{pos}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{map}\PYG{p}{(} \PYG{p}{\PYGZob{}}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{P}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}  \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{P}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{SP}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{P}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{RP}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{P}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{C}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}  \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{IF}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{1B}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{IF}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}   \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{2B}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{IF}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}   \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{3B}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{IF}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}   \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{SS}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{IF}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{OF}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{OF}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}   \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{LF}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{OF}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}   \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{CF}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{OF}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}   \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{RF}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{OF}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}   \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{DH}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{DH}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{p}{\PYGZcb{}} \PYG{p}{)}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
        salary               name  total\PYGZus{}value pos     years   avg\PYGZus{}annual  \PYGZbs{}
0  \PYGZdl{} 3,800,000  Darryl Strawberry  \PYGZdl{} 3,800,000  OF  1 (1991)  \PYGZdl{} 3,800,000   
1  \PYGZdl{} 3,750,000     Kevin Mitchell  \PYGZdl{} 3,750,000  OF  1 (1991)  \PYGZdl{} 3,750,000   
2  \PYGZdl{} 3,750,000         Will Clark  \PYGZdl{} 3,750,000  1B  1 (1991)  \PYGZdl{} 3,750,000   
3  \PYGZdl{} 3,625,000         Mark Davis  \PYGZdl{} 3,625,000   P  1 (1991)  \PYGZdl{} 3,625,000   
4  \PYGZdl{} 3,600,000         Eric Davis  \PYGZdl{} 3,600,000  OF  1 (1991)  \PYGZdl{} 3,600,000   

  team  first\PYGZus{}year  name\PYGZus{}length simple\PYGZus{}pos  
0  LAD        1991           17         OF  
1   SF        1991           14         OF  
2   SF        1991           10         IF  
3   KC        1991           10          P  
4  CIN        1991           10         OF  
\end{sphinxVerbatim}

In class, we will do a more complex example of applying a dictionary using \sphinxcode{\sphinxupquote{map()}}.  Before class, you may want to glance back at Exercise 3 from {\hyperref[\detokenize{chapter-2-mathematical-foundations::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{the Chapter 2 notes}}}}, which shows you how to take two columns of a DataFrame representing a mathematical function and convert them into a dictionary for use in situations just like this one.  And be sure to complete the homework about the NPR dataset before class as well, because we will use that in our example!


\subsection{Parallel \sphinxstyleliteralintitle{\sphinxupquote{apply()}}}
\label{\detokenize{chapter-11-processing-rows:parallel-apply}}
I mentioned earlier that converting a loop into an \sphinxcode{\sphinxupquote{apply()}} or \sphinxcode{\sphinxupquote{map()}} call doesn’t gain us much speed.  But it does make it easy for us to add a nice speed improvement.  There’s a Python package called \sphinxhref{https://github.com/jmcarpenter2/swifter}{swifter} that you can install using the instructions on that page.  Once it’s installed, you can convert any code like \sphinxcode{\sphinxupquote{df{[}'column'{]}.apply(f)}} easily into a faster version by replacing it with \sphinxcode{\sphinxupquote{df{[}'column'{]}.swifter.apply(f)}}.  That’s all!

Under the hood, swifter is trying a variety of speedup mechanisms (many of which we discuss in this chapter) and deciding which of them works best for your situation.  The most common one for large dataset is probably parallel processing.  This means that if your computer has more than one processor core (which most modern laptops do), then it can process more than one entry of the data at once, each on a separate core.

Without swifter, you could accomplish the same thing with code like the following.  (In fact, if you have trouble installing swifter, you can use this code instead.)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Use Python\PYGZsq{}s built\PYGZhy{}in multiprocessing module to find your number of cores.}
\PYG{k+kn}{import} \PYG{n+nn}{multiprocessing} \PYG{k}{as} \PYG{n+nn}{mp}
\PYG{n}{n\PYGZus{}cores} \PYG{o}{=} \PYG{n}{mp}\PYG{o}{.}\PYG{n}{cpu\PYGZus{}count}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Create a \PYGZdq{}pool\PYGZdq{} of functions that can work at the same time and run them.}
\PYG{n}{pool} \PYG{o}{=} \PYG{n}{mp}\PYG{o}{.}\PYG{n}{Pool}\PYG{p}{(} \PYG{n}{n\PYGZus{}cores} \PYG{p}{)}
\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{simple\PYGZus{}pos}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{pool}\PYG{o}{.}\PYG{n}{map}\PYG{p}{(} \PYG{n}{simpler\PYGZus{}position}\PYG{p}{,} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{pos}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{n\PYGZus{}cores} \PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Clean up afterwards.}
\PYG{n}{pool}\PYG{o}{.}\PYG{n}{close}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{pool}\PYG{o}{.}\PYG{n}{join}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} See result.}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
        salary               name  total\PYGZus{}value pos     years   avg\PYGZus{}annual  \PYGZbs{}
0  \PYGZdl{} 3,800,000  Darryl Strawberry  \PYGZdl{} 3,800,000  OF  1 (1991)  \PYGZdl{} 3,800,000   
1  \PYGZdl{} 3,750,000     Kevin Mitchell  \PYGZdl{} 3,750,000  OF  1 (1991)  \PYGZdl{} 3,750,000   
2  \PYGZdl{} 3,750,000         Will Clark  \PYGZdl{} 3,750,000  1B  1 (1991)  \PYGZdl{} 3,750,000   
3  \PYGZdl{} 3,625,000         Mark Davis  \PYGZdl{} 3,625,000   P  1 (1991)  \PYGZdl{} 3,625,000   
4  \PYGZdl{} 3,600,000         Eric Davis  \PYGZdl{} 3,600,000  OF  1 (1991)  \PYGZdl{} 3,600,000   

  team  first\PYGZus{}year  name\PYGZus{}length simple\PYGZus{}pos  
0  LAD        1991           17         OF  
1   SF        1991           14         OF  
2   SF        1991           10         IF  
3   KC        1991           10          P  
4  CIN        1991           10         OF  
\end{sphinxVerbatim}


\section{Map\sphinxhyphen{}Reduce}
\label{\detokenize{chapter-11-processing-rows:map-reduce}}
\begin{sphinxadmonition}{note}{Big Picture \sphinxhyphen{} Important phrases: map\sphinxhyphen{}reduce and split\sphinxhyphen{}apply\sphinxhyphen{}combine}

Both \sphinxstyleemphasis{map\sphinxhyphen{}reduce} and \sphinxstyleemphasis{split\sphinxhyphen{}apply\sphinxhyphen{}combine} are data manipulation buzzwords that you’ll want to be familiar with, for
\begin{itemize}
\item {} 
thinking about your own data manipulation work,

\item {} 
discussing that work with coworkers, and

\item {} 
knowing what people are saying in, e.g., interviews.

\end{itemize}

This section covers map\sphinxhyphen{}reduce and the next section covers split\sphinxhyphen{}apply\sphinxhyphen{}combine.
\end{sphinxadmonition}

A map\sphinxhyphen{}reduce process is one that takes any list, \sphinxstyleemphasis{maps} a specific function across all entries of the list, then \sphinxstyleemphasis{reduces} those outputs down to a single, smaller result.  Consider the following picture, which shows a very simple map\sphinxhyphen{}reduce operation that takes a DataFrame about historic revenue numbers and computes the lowest revenue across all quarters.

\sphinxincludegraphics{{map-reduce}.png}

Let’s actually do the above computation on some small sample (fictional) data:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} setup \PYGZhy{} example tiny dataset}
\PYG{n}{rev\PYGZus{}quarters} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(} \PYG{p}{\PYGZob{}}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Year}\PYG{l+s+s1}{\PYGZsq{}}    \PYG{p}{:} \PYG{p}{[} \PYG{l+m+mi}{2010}\PYG{p}{,} \PYG{l+m+mi}{2010}\PYG{p}{,} \PYG{l+m+mi}{2010}\PYG{p}{,} \PYG{l+m+mi}{2010}\PYG{p}{,} \PYG{l+m+mi}{2011}\PYG{p}{,} \PYG{l+m+mi}{2011}\PYG{p}{,} \PYG{l+m+mi}{2011}\PYG{p}{,} \PYG{l+m+mi}{2011}\PYG{p}{,} \PYG{l+m+mi}{2012}\PYG{p}{,} \PYG{l+m+mi}{2012} \PYG{p}{]}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Quarter}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{:} \PYG{p}{[}    \PYG{l+m+mi}{1}\PYG{p}{,}    \PYG{l+m+mi}{2}\PYG{p}{,}    \PYG{l+m+mi}{3}\PYG{p}{,}    \PYG{l+m+mi}{4}\PYG{p}{,}    \PYG{l+m+mi}{1}\PYG{p}{,}    \PYG{l+m+mi}{2}\PYG{p}{,}    \PYG{l+m+mi}{3}\PYG{p}{,}    \PYG{l+m+mi}{4}\PYG{p}{,}    \PYG{l+m+mi}{1}\PYG{p}{,}    \PYG{l+m+mi}{2} \PYG{p}{]}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Revenue}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{:} \PYG{p}{[}  \PYG{l+m+mi}{177}\PYG{p}{,}  \PYG{l+m+mi}{186}\PYG{p}{,}  \PYG{l+m+mi}{167}\PYG{p}{,}  \PYG{l+m+mi}{263}\PYG{p}{,}  \PYG{l+m+mi}{180}\PYG{p}{,}  \PYG{l+m+mi}{193}\PYG{p}{,}  \PYG{l+m+mi}{189}\PYG{p}{,}  \PYG{l+m+mi}{281}\PYG{p}{,}  \PYG{l+m+mi}{201}\PYG{p}{,}  \PYG{l+m+mi}{210} \PYG{p}{]}
\PYG{p}{\PYGZcb{}} \PYG{p}{)}
\PYG{n}{rev\PYGZus{}quarters}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
   Year  Quarter  Revenue
0  2010        1      177
1  2010        2      186
2  2010        3      167
3  2010        4      263
4  2011        1      180
5  2011        2      193
6  2011        3      189
7  2011        4      281
8  2012        1      201
9  2012        2      210
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} map\PYGZhy{}reduce work, one line:}
\PYG{n}{rev\PYGZus{}quarters}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Revenue}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{min}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
167
\end{sphinxVerbatim}

As mentioned earlier, “map” is a synonym for “apply,” so the first step of the process applies the same operation to all rows of the DataFrame; in this case, that operation extracts the revenue from the row.  The “reduce” operation in this case is a simple \sphinxcode{\sphinxupquote{min()}} operation, but it can be something more complex.

So a map\sphinxhyphen{}reduce operation involves two functions, the first performing a \sphinxcode{\sphinxupquote{map()}} operation (as discussed earlier), and the second doing something new.  The function used for the reducing step must be something that takes an entire list or series as input and produces a single value as output.  The \sphinxcode{\sphinxupquote{min()}} operation was used in the example above, but other operations are common, such as \sphinxcode{\sphinxupquote{max()}}, \sphinxcode{\sphinxupquote{sum()}}, \sphinxcode{\sphinxupquote{len()}}, \sphinxcode{\sphinxupquote{mean()}}, \sphinxcode{\sphinxupquote{median()}}, and more.


\subsection{Argmin and argmax}
\label{\detokenize{chapter-11-processing-rows:argmin-and-argmax}}
A very common function that shows up in statistics is called \sphinxcode{\sphinxupquote{argmin}} (and its companion \sphinxcode{\sphinxupquote{argmax}}).  These are also implemented in pandas and are very useful in map\sphinxhyphen{}reduce situations.  In the example above, let’s say we didn’t want to know the minimum revenue, but we wanted to know in which quarter the minimum revenue happened.  We can replace \sphinxcode{\sphinxupquote{min}} in the above code with \sphinxcode{\sphinxupquote{argmin}} to ask that question.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{rev\PYGZus{}quarters}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Revenue}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{argmin}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
2
\end{sphinxVerbatim}

The \sphinxcode{\sphinxupquote{argmin}} function is short for “the argument that yields the minimum,” or in other words, what value would I need to supply as \sphinxstyleemphasis{input} to the map function to get the minimum output?  In this case, the map function takes each row and extracts its revenue, so we’re asking pandas, “When you found the minimum revenue, which row was the input?”  The answer was row 2, and we can see that it’s the correct row as follows.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{rev\PYGZus{}quarters}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Year       2010
Quarter       3
Revenue     167
Name: 2, dtype: int64
\end{sphinxVerbatim}

While the pandas documentation for \sphinxcode{\sphinxupquote{argmin}} and \sphinxcode{\sphinxupquote{argmax}} suggest that they return multiple values in the case of ties, this doesn’t seem to be true.  They seem to return the first index only.  You can therefore always rely on the result of \sphinxcode{\sphinxupquote{argmin}}/\sphinxcode{\sphinxupquote{argmax}} being a single value, never a list or series.  If you want the indices of all max/min entries, you will need to compute it another way.


\subsection{Map\sphinxhyphen{}reduce example: sample standard deviation}
\label{\detokenize{chapter-11-processing-rows:map-reduce-example-sample-standard-deviation}}
The formula for the standard deviation of a sample of data should be familiar you to from GB213.
\begin{equation*}
\begin{split} s=\sqrt{\frac{\sum_{i=1}^n (x_i-\bar x)^2}{n-1}} \end{split}
\end{equation*}
Let’s assume we’ve already computed the mean value \(\bar x\).  Then computing the standard deviation is actually a map\sphinxhyphen{}reduce operation.  The map function takes each \(x_i\) as input and computes \((x_i-\bar x)^2\) as output.  The reduce operation then does a sum, divides by \(n-1\), and takes a square root.  We could code it like so:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}

\PYG{n}{example\PYGZus{}data} \PYG{o}{=} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{first\PYGZus{}year}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{x\PYGZus{}bar} \PYG{o}{=} \PYG{n}{example\PYGZus{}data}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}

\PYG{k}{def} \PYG{n+nf}{map\PYGZus{}func} \PYG{p}{(} \PYG{n}{x} \PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{p}{(} \PYG{n}{x} \PYG{o}{\PYGZhy{}} \PYG{n}{x\PYGZus{}bar} \PYG{p}{)} \PYG{o}{*}\PYG{o}{*} \PYG{l+m+mi}{2}
\PYG{k}{def} \PYG{n+nf}{reduce\PYGZus{}func} \PYG{p}{(} \PYG{n}{data} \PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(} \PYG{n}{data}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)} \PYG{o}{/} \PYG{p}{(} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{data}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{1} \PYG{p}{)} \PYG{p}{)}

\PYG{n}{reduce\PYGZus{}func}\PYG{p}{(} \PYG{n}{example\PYGZus{}data}\PYG{o}{.}\PYG{n}{map}\PYG{p}{(} \PYG{n}{map\PYGZus{}func} \PYG{p}{)} \PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
7.926156939014573
\end{sphinxVerbatim}

Of course, we didn’t have to code that.  There’s already an existing standard deviation function built into pandas, and it gives almost exactly the same answer.  (I suspect theirs does something more careful with tiny issues of accuracy than my simple example does.)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{example\PYGZus{}data}\PYG{o}{.}\PYG{n}{std}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
7.926156939014146
\end{sphinxVerbatim}

But it is still important to notice that the pattern in computing a sample standard deviation is a map\sphinxhyphen{}reduce pattern, because we cannot always rely on pandas to do computations for us.  For instance, if the data we were dealing with were many gigabytes spread over a database, we couldn’t load it all into a pandas DataFrame in memory and then call \sphinxcode{\sphinxupquote{data.std()}} to get our answer.

There are specialized tools in the industry for applying the map\sphinxhyphen{}reduce paradigm to databases (even if the database is enormous and spread over many different servers).  One famous example is \sphinxhref{https://spark.apache.org/}{Apache Spark}, but there are many.

Many more examples of map\sphinxhyphen{}reduce from math and statistics could have been shown instead of the one above.  Any time a list of values collapses to give a single result, map\sphinxhyphen{}reduce is behind it.  This happens for summations, approximations of integrals (e.g., trapezoidal rule), expected values, matrix multiplication, computing probabilities from trees of possible outcomes, any weighted averages (chemical concentrations, portfolio values, etc.), and many more.


\section{Split\sphinxhyphen{}Apply\sphinxhyphen{}Combine}
\label{\detokenize{chapter-11-processing-rows:split-apply-combine}}
Data scientist and R developer Hadley Wickham seems to coin lots of important phrases.  Recall from {\hyperref[\detokenize{chapter-5-before-and-after::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{the Chapter 5 notes}}}} that he introduced the phrase “tidy data.”  He also introduced the phrase “split, apply, combine,” in \sphinxhref{http://www.stat.wvu.edu/~jharner/courses/stat623/docs/plyrJSS.pdf}{this paper}.

It is another extremely common operation done on DataFrames, and it is closely related to map\sphinxhyphen{}reduce, as we will see below.

Let’s say you were concerned about pay equity, and wanted to compute the median salary across your organization, by gender, to get a sense of whether there were any important discrepancies.  The computation would look something like the following.  (We assume that the gender column contains either M for male, F for female, or a missing value for those who do not wish to classify.)

\sphinxincludegraphics{{split-apply-combine}.png}

As you can see from the picture, the first phase (called “split”) breaks the data into groups by the categorical variable we care about–in this case, gender.  After that, each smaller DataFrame undergoes a map\sphinxhyphen{}reduce process, and the results of each small map\sphinxhyphen{}reduce get aggregated into a result, indexed by the original categorical variable.

Note that the output type of the split operation (which, in pandas, is a \sphinxcode{\sphinxupquote{df.groupby()}} call) is \sphinxstyleemphasis{NOT} a DataFrame, but rather a collection of DataFrames.  It is essential to follow a \sphinxcode{\sphinxupquote{df.groupby()}} call with the apply and combine steps of the process, so that the result is a familiar and usable type of object again–a pandas DataFrame.

The easiest type of split\sphinxhyphen{}apply\sphinxhyphen{}combine is shown in the picture above and can be done with a single line of code.  We’ll compute minimum revenue by year with the DataFrame from our map\sphinxhyphen{}reduce example.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{rev\PYGZus{}quarters}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Year}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Revenue}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{min}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Year
2010    167
2011    180
2012    201
Name: Revenue, dtype: int64
\end{sphinxVerbatim}

Split\sphinxhyphen{}apply\sphinxhyphen{}combine is actually a specific type of pivot table.  Thus split\sphinxhyphen{}apply\sphinxhyphen{}combine operations can be done on data in Excel as well, using its pivot table features.  We can even use \sphinxcode{\sphinxupquote{df.pivot\_table()}} to mimic the above procedure, as follows.  (Because we don’t need data separated into separate columns, we don’t provide a columns variable.)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{rev\PYGZus{}quarters}\PYG{o}{.}\PYG{n}{pivot\PYGZus{}table}\PYG{p}{(} \PYG{n}{index}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Year}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{p}{[}\PYG{p}{]}\PYG{p}{,} \PYG{n}{values}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Revenue}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{aggfunc}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{min}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
      Revenue
Year         
2010      167
2011      180
2012      201
\end{sphinxVerbatim}


\section{More on math in Python}
\label{\detokenize{chapter-11-processing-rows:more-on-math-in-python}}

\subsection{Arithmetic in formulas}
\label{\detokenize{chapter-11-processing-rows:arithmetic-in-formulas}}
Recall that pandas is built on NumPy, and in {\hyperref[\detokenize{chapter-9-math-and-stats::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Chapter 9 of the notes}}}} we talked about NumPy’s support for vectorization.  If we have a Series \sphinxcode{\sphinxupquote{height}} containing heights in inches and we need instead to have it in centimeters, we don’t need to do \sphinxcode{\sphinxupquote{height.apply()}} and give it a conversion function, because we can just do \sphinxcode{\sphinxupquote{height * 2.54}}.  NumPy automatically \sphinxstyleemphasis{vectorizes} this operation, spreading the “times 2.54” over each entry in the \sphinxcode{\sphinxupquote{height}} array.

This is quite natural, because we have mathematical notation that does the same thing (in math, not Python).  If you’ve taken a class involving vectors, you know that vector addition \(\vec x+\vec y\) means to do exactly what NumPy does–add the corresponding entries in each vector.  Similarly, scalar multiplication \(s\vec x\) means to multiply \(s\) by each entry in the vector \(\vec x\), just like \sphinxcode{\sphinxupquote{height * 2.54}} does in Python.  So NumPy is not inventing something strange here; it’s normal mathematical stuff.

All the basic mathematical operations are built into NumPy.  For example, if we have created a linear model \(\hat y=\beta_0+\beta_1 x\) with parameters stored in Python variables \sphinxcode{\sphinxupquote{β0}} and \sphinxcode{\sphinxupquote{β1}}, we can apply it to an entire series of inputs \sphinxcode{\sphinxupquote{xs}} at once with the following code, because NumPy knows how to spread both \sphinxcode{\sphinxupquote{+}} and \sphinxcode{\sphinxupquote{*}} across arrays.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{y\PYGZus{}hat} \PYG{o}{=} \PYG{n}{β0} \PYG{o}{+} \PYG{n}{β1} \PYG{o}{*} \PYG{n}{xs}
\end{sphinxVerbatim}

In fact, if we had actual \sphinxcode{\sphinxupquote{ys}} that went with the \sphinxcode{\sphinxupquote{xs}}, we could then compute a list of residuals all at once with \sphinxcode{\sphinxupquote{y\_hat \sphinxhyphen{} ys}}, or even compute the RMSE (root mean squared error) with code like this.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{np}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(} \PYG{p}{(} \PYG{n}{y\PYGZus{}hat} \PYG{o}{\PYGZhy{}} \PYG{n}{ys} \PYG{p}{)} \PYG{o}{*}\PYG{o}{*} \PYG{l+m+mi}{2} \PYG{p}{)} \PYG{o}{/} \PYG{n+nb}{len}\PYG{p}{(} \PYG{n}{ys} \PYG{p}{)} \PYG{p}{)}
\end{sphinxVerbatim}

The subtraction with \sphinxcode{\sphinxupquote{\sphinxhyphen{}}} and the squaring with \sphinxcode{\sphinxupquote{** 2}} would all be spread across arrays of inputs correctly, because NumPy comes with code to support doing so.


\subsection{Conditionals with \sphinxstyleliteralintitle{\sphinxupquote{np.where()}}}
\label{\detokenize{chapter-11-processing-rows:conditionals-with-np-where}}
This removes a lot of the need for both loops and \sphinxcode{\sphinxupquote{apply()}}/\sphinxcode{\sphinxupquote{map()}} calls, but not all.  One of the first things that makes us think we might need a loop is when a conditional computation needs to be done.  For instance, let’s say we were given a dataset like the following (made up) example.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{patients} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(} \PYG{p}{\PYGZob{}}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{id}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{:} \PYG{p}{[} \PYG{l+m+mi}{100615}\PYG{p}{,} \PYG{l+m+mi}{51}\PYG{p}{,} \PYG{l+m+mi}{100616}\PYG{p}{,} \PYG{l+m+mi}{83}\PYG{p}{,} \PYG{l+m+mi}{100607}\PYG{p}{,} \PYG{l+m+mi}{100618}\PYG{p}{,} \PYG{l+m+mi}{19}\PYG{p}{,} \PYG{l+m+mi}{65} \PYG{p}{]}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{height}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{:} \PYG{p}{[} \PYG{l+m+mi}{72}\PYG{p}{,} \PYG{l+m+mi}{158}\PYG{p}{,} \PYG{l+m+mi}{75}\PYG{p}{,} \PYG{l+m+mi}{173}\PYG{p}{,} \PYG{l+m+mi}{68}\PYG{p}{,} \PYG{l+m+mi}{67}\PYG{p}{,} \PYG{l+m+mi}{163}\PYG{p}{,} \PYG{l+m+mi}{178} \PYG{p}{]}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{dose}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{:} \PYG{p}{[} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mf}{2.5}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mf}{2.5}\PYG{p}{,} \PYG{l+m+mi}{0} \PYG{p}{]}
\PYG{p}{\PYGZcb{}} \PYG{p}{)}
\PYG{n}{patients}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
       id  height  dose
0  100615      72   2.0
1      51     158   0.0
2  100616      75   2.5
3      83     173   2.0
4  100607      68   0.0
5  100618      67   2.0
6      19     163   2.5
7      65     178   0.0
\end{sphinxVerbatim}

Let’s imagine that we then found out that it was the result of merging data from two different studies, one done in the U.S. and one done in France.  The data with IDs that begin with 100 are from the U.S. study, where heights were measured in inches.  The data with two\sphinxhyphen{}digit IDs are from the French study, where heights were measured in cm.  We need to standardize the units.

We can’t simply convert to cm with \sphinxcode{\sphinxupquote{patients{[}'height'{]} * 2.54}} because that would apply the conversion to all data rather than just the measurements in inches.  We need some conditional logic, perhaps using an \sphinxcode{\sphinxupquote{if}} statement, to be selective.  Our first inclination might be a loop.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} before changing the contents, make a backup, for use later.}
\PYG{n}{backup} \PYG{o}{=} \PYG{n}{patients}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} solving the problem with a loop:}
\PYG{k}{for} \PYG{n}{index}\PYG{p}{,}\PYG{n}{row} \PYG{o+ow}{in} \PYG{n}{patients}\PYG{o}{.}\PYG{n}{iterrows}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{if} \PYG{n}{row}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{100000}\PYG{p}{:}  \PYG{c+c1}{\PYGZsh{} US data}
        \PYG{n}{patients}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{n}{index}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{height}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{*}\PYG{o}{=} \PYG{l+m+mf}{2.54}
\PYG{n}{patients}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
       id  height  dose
0  100615  182.88   2.0
1      51  158.00   0.0
2  100616  190.50   2.5
3      83  173.00   2.0
4  100607  172.72   0.0
5  100618  170.18   2.0
6      19  163.00   2.5
7      65  178.00   0.0
\end{sphinxVerbatim}

Note that \sphinxcode{\sphinxupquote{row{[}'height'{]} *= 2.54}} actually wouldn’t alter the DataFrame, so we’re forced to use \sphinxcode{\sphinxupquote{patients.loc{[}{]}}} instead.

But if you were trying to follow the advice in this chapter of the notes, you might switch to an \sphinxcode{\sphinxupquote{apply()}} function instead.  The trouble is, it’s a bit annoying to do, because we need the \sphinxcode{\sphinxupquote{if}} to operate on the “id” column and the conversion to operate on the “height” column, so which one do we call \sphinxcode{\sphinxupquote{apply()}} on?  We can call \sphinxcode{\sphinxupquote{apply()}} on the whole DataFrame, but the loop is actually simpler in that case!

The solution here is to use NumPy’s \sphinxcode{\sphinxupquote{np.where()}} function.  It lets you select just which rows should get which type of computation, like so:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} restore the original data:}
\PYG{n}{patients} \PYG{o}{=} \PYG{n}{backup}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} solution with np.where():}
\PYG{n}{patients}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{height}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{where}\PYG{p}{(} \PYG{n}{patients}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{100000}\PYG{p}{,} \PYG{n}{patients}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{height}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{*} \PYG{l+m+mf}{2.54}\PYG{p}{,} \PYG{n}{patients}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{height}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{p}{)}
\PYG{n}{patients}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
       id  height  dose
0  100615  182.88   2.0
1      51  158.00   0.0
2  100616  190.50   2.5
3      83  173.00   2.0
4  100607  172.72   0.0
5  100618  170.18   2.0
6      19  163.00   2.5
7      65  178.00   0.0
\end{sphinxVerbatim}

The \sphinxcode{\sphinxupquote{np.where()}} function works just like \sphinxcode{\sphinxupquote{=IF()}} does in Excel, taking three inputs, a conditional, an “if” result, and an “else” result.  But the difference is that \sphinxcode{\sphinxupquote{np.where()}} is vectorized, effectively doing an Excel \sphinxcode{\sphinxupquote{=IF()}} on each entry in the Series separately.  You can read an \sphinxcode{\sphinxupquote{np.where()}} function just like a sentence:

Where patient id is over 100000, do patient height times 2.54, otherwise just keep the original height.

In summary, thanks to \sphinxcode{\sphinxupquote{np.where()}}, even many conditional computations don’t require a loop or an apply; they can be done with NumPy vectorization as well.


\subsection{Speeding up mathematics}
\label{\detokenize{chapter-11-processing-rows:speeding-up-mathematics}}
There are also some very impressive tools for speeding up mathematical operations in NumPy a \sphinxstyleemphasis{LOT.}  I will not cover them here, but will lest each of the following as an opportunity for Learning On Your Own.  Note that these are relevant only if you have a very large dataset over which you need to do complex mathematical computations, so that you notice pandas behaving slowly, and thus you need a speed boost.

\begin{sphinxadmonition}{note}{Learning on Your Own \sphinxhyphen{} CuPy (fastest option)}

Doing certain types of computations can be sped up significantly by using graphics cards (originally designed for gaming rather than data science) instead of the computer’s CPU (which does all the non\sphinxhyphen{}graphics computations).  See \sphinxhref{https://towardsdatascience.com/make-your-python-functions-10x-faster-142ab40b31a7}{this blog post} for information on CuPy, a Python library for harnessing your GPU to do fast arithmetic.

CuPy requires you to first describe to it the computation you’ll want to do quickly, and it will compile it into GPU\sphinxhyphen{}friendly code that you can then use.  This is an extra level of annoyance for the programmer, but often produces the fastest results.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Learning on Your Own \sphinxhyphen{} NumExpr (easiest option)}

If you’ve already got some code that does the arithmetic operation you want on NumPy arrays (or pandas Series, which are also NumPy arrays), then it’s pretty easy to convert that code to use NumExpr.  It doesn’t give as big a speedup as CuPy, but it’s easier to set up.  \sphinxhref{https://towardsdatascience.com/speed-up-your-numpy-and-pandas-with-numexpr-package-25bd1ab0836b}{See this blog post} for details, and note the connection to \sphinxcode{\sphinxupquote{pd.eval()}}.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Learning on Your Own \sphinxhyphen{} Cython (most flexible)}

The previous two options work only for speeding up arithmetic.  To speed up any operation (including string manipulation, working with dictionaries, sets, or any Python class), you’ll need Cython.  This is a tool for converting Python code into C code automatically, without your having to learn to program in C.  C code almost always runs significantly faster than Python code, but C is much less easy to use, especially for data work.  See \sphinxhref{https://ipython-books.github.io/55-accelerating-python-code-with-cython/}{this tutorial} on using Cython in Jupyter, plus the example below.
\end{sphinxadmonition}

Let’s say I have the following function that computes \(n!\), the product of all positive integers up to \(n\).  (This is not the best way to write this function, but it’s just an example.)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{factorial} \PYG{p}{(} \PYG{n}{n} \PYG{p}{)}\PYG{p}{:}
    \PYG{n}{result} \PYG{o}{=} \PYG{l+m+mi}{1}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{n}\PYG{o}{+}\PYG{l+m+mi}{1} \PYG{p}{)}\PYG{p}{:}
        \PYG{n}{result} \PYG{o}{*}\PYG{o}{=} \PYG{n}{i}
    \PYG{k}{return} \PYG{n}{result}

\PYG{n}{factorial}\PYG{p}{(} \PYG{l+m+mi}{5} \PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
120
\end{sphinxVerbatim}

I can ask Jupyter to compile this into C code for me, so that it runs faster, as follows.

First, use one cell of the notebook to load the Cython extension.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZpc{}}\PYG{n}{load\PYGZus{}ext} \PYG{n}{cython}
\end{sphinxVerbatim}

Then, ask Cython to convert your Python code into C.  This requires giving it some hints (highlighted in the comments below) about the data types of the variables.  In this simple case, they’re all integers.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZpc{}}\PYG{o}{\PYGZpc{}}\PYG{n}{cython} \PYG{o}{\PYGZhy{}}\PYG{n}{a}
\PYG{k}{def} \PYG{n+nf}{factorial} \PYG{p}{(} \PYG{n+nb}{int} \PYG{n}{n} \PYG{p}{)}\PYG{p}{:}   \PYG{c+c1}{\PYGZsh{} n is an integer}
    \PYG{n}{cdef} \PYG{n+nb}{int} \PYG{n}{result}\PYG{p}{,} \PYG{n}{i}     \PYG{c+c1}{\PYGZsh{} so are result and i}
    \PYG{n}{result} \PYG{o}{=} \PYG{l+m+mi}{1}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{n}\PYG{o}{+}\PYG{l+m+mi}{1} \PYG{p}{)}\PYG{p}{:}
        \PYG{n}{result} \PYG{o}{*}\PYG{o}{=} \PYG{n}{i}
    \PYG{k}{return} \PYG{n}{result}
\end{sphinxVerbatim}

If you run the above code in Jupyter, it will show you an interactive display of the code it created and how much speedup you can expect.  The function still generates the same outputs as before, but typically much faster.  How much faster?  Check out the tutorial linked to above for more information.


\section{So do we \sphinxstyleemphasis{always} avoid loops?}
\label{\detokenize{chapter-11-processing-rows:so-do-we-always-avoid-loops}}
No, there are some times when you might still want to avoid loops.


\subsection{When to opt for a loop}
\label{\detokenize{chapter-11-processing-rows:when-to-opt-for-a-loop}}
The two most prominent times to choose loops are these.
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
If the code you’re running is a search for one thing, and you want to stop once it’s found, a loop might be best.  Take the home mortgage database of 15 million records, for example.  Let’s say you were looking for an example of a Hispanic male in Nevada applying for a mortgage for a rental property.  If you ask pandas to filter the dataset, it will examine all 15M rows and give you \sphinxstyleemphasis{all} the ones fitting these criteria.  But you just needed one.  Maybe you’d find it in the first 50,000 rows and not need to search the other 14.95 million!  A loop definitely has the potential to be faster in such a case.

\item {} 
Sometimes the computation you’re doing involves comparing one row to adjacent rows.  For example, you might want to find those days when the price of a stock was significantly more or less than it was on the two adjacent days (one before and one after).  Although it’s possible to do this without a loop, the code is a harder to write and to read, as you can see in the example below.  With a loop, it’s not as fast, but it’s clearer.  So if speed isn’t an issue, use the loop.

\end{enumerate}

Let’s see how we might write the code for the stock example just given, but instead of stock data, we’ll use the (made up) quarterly revenue data from earlier.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} get just the column I care about:}
\PYG{n}{revenues} \PYG{o}{=} \PYG{n}{rev\PYGZus{}quarters}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Revenue}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}

\PYG{n}{results} \PYG{o}{=} \PYG{p}{[} \PYG{p}{]}
\PYG{c+c1}{\PYGZsh{} For each quarter execpt the first and last...}
\PYG{k}{for} \PYG{n}{index} \PYG{o+ow}{in} \PYG{n}{revenues}\PYG{o}{.}\PYG{n}{index}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} If it\PYGZsq{}s bigger than the previous and the next...}
    \PYG{k}{if} \PYG{n}{revenues}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{n}{index}\PYG{p}{]} \PYG{o}{\PYGZgt{}} \PYG{n}{revenues}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{n}{index}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{o+ow}{and} \PYGZbs{}
       \PYG{n}{revenues}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{n}{index}\PYG{p}{]} \PYG{o}{\PYGZgt{}} \PYG{n}{revenues}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{n}{index}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{:}
        \PYG{n}{results}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(} \PYG{n}{index} \PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Save it for later}

\PYG{c+c1}{\PYGZsh{} Show me just the quarters I saved.}
\PYG{n}{rev\PYGZus{}quarters}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{n}{results}\PYG{p}{,}\PYG{p}{:}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
   Year  Quarter  Revenue
1  2010        2      186
3  2010        4      263
5  2011        2      193
7  2011        4      281
\end{sphinxVerbatim}

Compare that to the same results computed using vectorization in NumPy rather than a loop.  If the data were large, this implementation would be faster, but it’s definitely not as clear to read.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Get all but first and last, for searching.}
\PYG{n}{to\PYGZus{}search} \PYG{o}{=} \PYG{n}{rev\PYGZus{}quarters}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} Compute arrays of previous/next quarters, for comparison.}
\PYG{n}{previous\PYGZus{}rev} \PYG{o}{=} \PYG{n}{rev\PYGZus{}quarters}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{p}{:}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{]}
\PYG{n}{next\PYGZus{}rev} \PYG{o}{=} \PYG{n}{rev\PYGZus{}quarters}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{:}\PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} Adjust indices so they match the to\PYGZus{}search Series.}
\PYG{n}{previous\PYGZus{}rev}\PYG{o}{.}\PYG{n}{index} \PYG{o}{=} \PYG{n}{previous\PYGZus{}rev}\PYG{o}{.}\PYG{n}{index} \PYG{o}{+} \PYG{l+m+mi}{1}
\PYG{n}{next\PYGZus{}rev}\PYG{o}{.}\PYG{n}{index} \PYG{o}{=} \PYG{n}{next\PYGZus{}rev}\PYG{o}{.}\PYG{n}{index} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{1}

\PYG{c+c1}{\PYGZsh{} Do the computation using NumPy vectorized comparisons.}
\PYG{n}{to\PYGZus{}search}\PYG{p}{[}\PYG{p}{(} \PYG{n}{to\PYGZus{}search}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Revenue}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{\PYGZgt{}} \PYG{n}{previous\PYGZus{}rev}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Revenue}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{p}{)} \PYGZbs{}
        \PYG{o}{\PYGZam{}} \PYG{p}{(} \PYG{n}{to\PYGZus{}search}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Revenue}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{\PYGZgt{}} \PYG{n}{next\PYGZus{}rev}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Revenue}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{p}{)}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
   Year  Quarter  Revenue
1  2010        2      186
3  2010        4      263
5  2011        2      193
7  2011        4      281
\end{sphinxVerbatim}

Any time when speed isn’t an issue, and you think the clearest way to write the code is a loop, then go right ahead and write clear code!  Loops aren’t always bad.


\subsection{Factoring computations out of the loop}
\label{\detokenize{chapter-11-processing-rows:factoring-computations-out-of-the-loop}}
Sometimes what’s making a loop slow is a repeated computation that doesn’t need to happen inside the loop.  The \sphinxstyleemphasis{loop variable} is the variable that immediately follows the \sphinxcode{\sphinxupquote{for}} statement in a loop.  In the loop example above, that’s the \sphinxcode{\sphinxupquote{index}} variable.  If there were any computation inside the loop that didn’t use the \sphinxcode{\sphinxupquote{index}} variable, we could bring that computation outside the loop, doing it once, before the loop, and saving time.

For example, in the final project some students did for MA346 in Spring 2020, some teams had a loop that processed a large database of baseball players, and tried to look their names up in a different database.  It went something like this:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{for} \PYG{n}{name} \PYG{o+ow}{in} \PYG{n}{baseball\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{player name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{:}
    \PYG{k}{if} \PYG{n}{name} \PYG{o+ow}{in} \PYG{n}{other\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Player}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{s Name}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} then do stuff here}
\end{sphinxVerbatim}

Because the two DataFrames were very large, this loop took literally hours to run on students’ laptops, and made it impossible for them to improve their code in time to finish the project.  The first thing I suggested was to change the code as follows.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{for} \PYG{n}{name} \PYG{o+ow}{in} \PYG{n}{baseball\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{player name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{:}
    \PYG{k}{if} \PYG{n}{name} \PYG{o+ow}{in} \PYG{n}{other\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Player}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{s Name}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{unique}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} then do stuff here}
\end{sphinxVerbatim}

The \sphinxcode{\sphinxupquote{.unique()}} function computes a smaller list from \sphinxcode{\sphinxupquote{other\_df{[}'name'{]}}}, in which each name shows up only once.  This meant a smaller search to do, and sped up the loop, but even so, it wasn’t fast enough.  It still took about 30 minutes, which made it hard for students to iteratively improve their code.

But notice that the loop variable, \sphinxcode{\sphinxupquote{name}}, doesn’t appear anywhere in the computation of \sphinxcode{\sphinxupquote{other\_df{[}"Player's Name"{]}.unique()}}.  So we’re asking Python to compute that list of unique names over and over, each time through the loop.  Let’s bring that outside the loop so we have to do it only once.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{unique\PYGZus{}name\PYGZus{}list} \PYG{o}{=} \PYG{n}{other\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Player}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{s Name}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{unique}\PYG{p}{(}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{name} \PYG{o+ow}{in} \PYG{n}{baseball\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{player name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{:}
    \PYG{k}{if} \PYG{n}{name} \PYG{o+ow}{in} \PYG{n}{unique\PYGZus{}name\PYGZus{}list}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} then do stuff here}
\end{sphinxVerbatim}

This loop ran much faster, and most students were able to use it to do the work of their final project.

Note that this advice, factoring out a computation that does not depend on the loop variable, is sort of the opposite of abstraction.  In abstraction, you make the list of all the variables that your computation \sphinxstyleemphasis{does} depend on, and move those up to the top, as input parameters.  Here we’re taking a look at which variables our computation \sphinxstyleemphasis{doesn’t} depend on, so that we can move the computation itself up to the top, so it is done outside the loop.


\subsection{Knowing how long you’ll have to wait}
\label{\detokenize{chapter-11-processing-rows:knowing-how-long-you-ll-have-to-wait}}
Few things are more frustrating than running a code cell and seeing the computer just sit there doing nothing.  We start to wonder whether it will take 15 seconds to process the data, and we should just have a little patience, or 15 minutes and we should go get a coffee, or 15 hours and we should give up and rewrite the code.  Which is it?  How can we tell except just waiting?

There are two easy ways to get some feedback as your loop is progressing.  The easiest one is to install the \sphinxcode{\sphinxupquote{tqdm}} module, whose purpose is to help you see a progress bar for a long\sphinxhyphen{}running loop.  After following \sphinxhref{https://github.com/tqdm/tqdm\#installation}{tqdm’s installation instructions} (using \sphinxcode{\sphinxupquote{pip}} or \sphinxcode{\sphinxupquote{conda}}), just import the module, then take the Series or list over which you’re looping and wrap it in \sphinxcode{\sphinxupquote{tqdm(...)}}, as in the example below.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{tqdm}\PYG{n+nn}{.}\PYG{n+nn}{notebook} \PYG{k+kn}{import} \PYG{n}{tqdm}

\PYG{n}{results} \PYG{o}{=} \PYG{p}{[} \PYG{p}{]}
\PYG{k}{for} \PYG{n}{index} \PYG{o+ow}{in} \PYG{n}{tqdm}\PYG{p}{(} \PYG{n}{revenues}\PYG{o}{.}\PYG{n}{index}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{p}{)}\PYG{p}{:}   \PYG{c+c1}{\PYGZsh{} \PYGZlt{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{} Notice tqdm here.}
    \PYG{k}{if} \PYG{n}{revenues}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{n}{index}\PYG{p}{]} \PYG{o}{\PYGZgt{}} \PYG{n}{revenues}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{n}{index}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{o+ow}{and} \PYGZbs{}
       \PYG{n}{revenues}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{n}{index}\PYG{p}{]} \PYG{o}{\PYGZgt{}} \PYG{n}{revenues}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{n}{index}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{:}
        \PYG{n}{results}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(} \PYG{n}{index} \PYG{p}{)}
\PYG{n}{rev\PYGZus{}quarters}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{n}{results}\PYG{p}{,}\PYG{p}{:}\PYG{p}{]}
\end{sphinxVerbatim}

While the computation is running, a progress bar shows up in the notebook, filling as the computation progresses.  It looks like the following example.

\sphinxincludegraphics{{tqdm-progress-bar}.png}

The numbers indicate that over 300 of the 1000 steps in that large loop are complete, and they have taken 12 seconds (written 00:12) and there are about 27 seconds left (00:27).  The loop completes about 25.02 iterations per second.  With a progress bar like this, even for a computation that might run for hours, you can tell very quickly how long you will have to wait, and whether it’s worth it to wait or if you need to speed up your loop instead.


\section{When the bottleneck is the dataset}
\label{\detokenize{chapter-11-processing-rows:when-the-bottleneck-is-the-dataset}}
Sometimes, you can’t get around the fact that you just have to process a lot of data, and that can be slow.  Unless you’re working for a company that will provide you with some powerful computing resources in the cloud on which to run your Jupyter notebook, so that it runs faster than it does on your laptop (or the free Colab/Deepnote machines), you’ll just have to run the slow code.  But there are still some ways to make this better.

\sphinxstylestrong{Don’t run it more than you have to.}  Often, the slow code is something that happens early your work, such as cleaning a huge dataset or searching through it for just the rows you need for your analysis.  Once you’ve written code that does this, save the result to a file with \sphinxcode{\sphinxupquote{pd.to\_csv()}} or \sphinxcode{\sphinxupquote{pd.to\_pickle()}} and don’t run that code again.

Don’t fall into the trap of thinking that all your code needs to be in one Python script or one Jupyter notebook.  If that slow code that cleaned your data never needs to be run again, then once you’ve run it and saved the output, save the script/notebook, close it, and start a new script or notebook to contain your data analysis code.  Then when you re\sphinxhyphen{}run your analysis, you don’t have to sit around and wait for the data cleaning to happen all over again!

This advice is especially important if the slow part of your work requires fetching data from the Internet.  Network downloads are the slowest and least predictable part of your work.  Once it’s been done correctly, don’t run it again.

\sphinxstylestrong{Do your work on a small dataset.}  If the dataset you have to analyze is still large enough that your analysis code itself runs slowly as well, try the following.  Near the top of your file, replace the actual data with a small sample of it, perhaps using code like this.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{patients} \PYG{o}{=} \PYG{n}{patients}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(} \PYG{l+m+mi}{3} \PYG{p}{)}
\PYG{n}{patients}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
       id  height  dose
5  100618  170.18   2.0
3      83  173.00   2.0
0  100615  182.88   2.0
\end{sphinxVerbatim}

Now the entire rest of my script or noteboook will operate on only this tiny DataFrame.  (Obviously, you’d want to choose a number larger than three in your code!  I’m doing a tiny example here.  You might reduce 100,000 rows to just 1,000, for example.)

Then as you create your data analysis code, which inevitably involves running it many times, you won’t have to wait for it to process all 100,000 rows of the data.  It can work on just 1,000 and run 100x faster.  When your analysis code works and you’re ready to write your report, delete the code that creates a small sample of the data and re\sphinxhyphen{}run your notebook from the start, now opearting on the whole dataset.  It will be slower, but you have to sit through that only once.

\sphinxstyleemphasis{Danger!}  Don’t forget to delete that cell when your code is polished and you want to do the real, final analysis!  I suggest adding a note in giant text at the end of your notebook saying something like, “Don’t forget, before you turn this in, USE THE WHOLE DATASET!”  Then you’ll remember to do that key step before you complete the project.

\sphinxstylestrong{If the dataset is truly huge,} so large that it can’t be stored in your computer’s memory all at once, then trying to load it will either generate out\sphinxhyphen{}of\sphinxhyphen{}memory errors or it will slow the process down enormously while the computer tries to use its hard drive as temporary extra memory storage.  In such cases, don’t forget the tip at the end of \sphinxhref{big-cheat-sheet\#chapter-1-using-iterators-in-pythonland}{this DataCamp chapter} about the \sphinxcode{\sphinxupquote{chunksize}} parameter.  It lets you process large files in smaller chunks.


\chapter{Concatenating and Merging DataFrames}
\label{\detokenize{chapter-12-concat-and-merge:concatenating-and-merging-dataframes}}\label{\detokenize{chapter-12-concat-and-merge::doc}}
See also the slides that summarize a portion of this content.


\section{Why join two datasets?}
\label{\detokenize{chapter-12-concat-and-merge:why-join-two-datasets}}
This chapter is about two ways to combine DataFrames together.  The concepts we’ll be discussing (concatenation and merging) are not unique to pandas DataFrames; they show up wherever tabular data is used, including in SQL.

Combining more than one dataset together is a crucial aspect of data work.  Let’s see two examples.

\sphinxstylestrong{Example 1.}  One of my friends runs \sphinxhref{https://secondnature.org/}{a nonprofit organization} that helps colleges and universities set climate action goals and track their progress toward keeping them.  He asked my graduate data science course in Fall 2019 to look at their database and come up with any insights.  Naturally, their database had records of all the climate goals and progress for schools they were working with, but it didn’t have much other information about those schools.  What if we wanted to analyze a variable they weren’t tracking, like endowment?  Or what if we wanted to look at schools that hadn’t get partnered with the nonprofit?  That information would need to be brought in from another dataset.  Until we do so, we can’t give interesting answers to the question the client posed.

\sphinxstylestrong{Example 2.}  One of my colleagues in the math department told me about a clever strategy one investment group used to predict the earnings of companies they were considering investing in.  They already had lots of data about each company, including the addresses of the company’s various offices and factories.  They could also purchase access to a large database of satellite images.  They used the addresses and some image\sphinxhyphen{}detection software to compute the number of cars in the parking lots of the company’s properties.  This turned out to be a very useful predictor of growth that they could access before their competing investors had the information.  It involved bringing together two datasets in a clever way.

In this chapter, we’ll discuss how to combine just two DataFrames, but the ideas apply if you have more than two.  For instance, to concatenate five DataFrames \sphinxcode{\sphinxupquote{df1}} through \sphinxcode{\sphinxupquote{df5}}, we can proceed in pairs, combining \sphinxcode{\sphinxupquote{df1}} and \sphinxcode{\sphinxupquote{df2}}, then combining that result with \sphinxcode{\sphinxupquote{df3}}, and so on until we have included \sphinxcode{\sphinxupquote{df5}}.

Let’s start by dicsussing concatenation, which is definitely the easier of the two concepts, before we tackle merging.  The English verb “concatenate” means to attach two things together, one after the end of the other.


\section{Concatenation is vertical}
\label{\detokenize{chapter-12-concat-and-merge:concatenation-is-vertical}}
DataFrames are tables of data, so when combining, we’ll either be stacking them vertically or horizontally.  Concatenation is vertical stacking.

It is an extremely common operation.  Very often what happens after you get some data is that (not surprisingly) you later get more of the same type of data.
\begin{itemize}
\item {} 
For instance, if you’re taking scientific measurements in a lab, one week you get a set of measurements, and the next week you get more data in the same format.

\item {} 
Or if you’re following a stock or other financial instrument, its prices one week form a dataset, then the next week, you see more data with the same format.

\end{itemize}

Because the standard way to organize tabular data is to put observations in rows, then getting more observations means we just need to add more rows onto the bottom of our previous table of data.  This is what concatenation is for.  Here’s an illustration using the stock prices example, with data that comes from Renewable Energy Group, Inc., whose 2020 data we’ve seen before in this course.

\sphinxincludegraphics{{concat-of-stock-data}.png}

There are two important things to notice in the picture.
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
All that’s happening is that we’re stacking data vertically.  It’s very straightforward!

\item {} 
In order for us to stack two DataFrames, they must have the same columns.  The column headers are highlighted in blue to emphasize that they’re the same in every table.

\end{enumerate}

(There are ways to deal with the case where new data comes in with different column headers; we’re covering the most common case here.)

The code to do this is extremely easy; it is a single call to the \sphinxcode{\sphinxupquote{pd.concat()}} function.  You provide a Python list of all the DataFrames to concatenate; in this case, we have just two.  We tell it to ignore the old indexes and create a new one, so that we don’t have duplicate index entries.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}

\PYG{n}{df\PYGZus{}jan} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZus{}static/regi\PYGZhy{}prices\PYGZhy{}jan\PYGZhy{}2020.csv}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{df\PYGZus{}feb} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZus{}static/regi\PYGZhy{}prices\PYGZhy{}feb\PYGZhy{}2020.csv}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}

\PYG{n}{df\PYGZus{}2mo} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{concat}\PYG{p}{(} \PYG{p}{[} \PYG{n}{df\PYGZus{}jan}\PYG{p}{,} \PYG{n}{df\PYGZus{}feb} \PYG{p}{]}\PYG{p}{,} \PYG{n}{ignore\PYGZus{}index}\PYG{o}{=}\PYG{k+kc}{True} \PYG{p}{)}
\PYG{n}{df\PYGZus{}2mo}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
       Date   Open   High    Low  Close
0  2\PYGZhy{}Jan\PYGZhy{}20  27.21  27.95  26.62  27.89
1  3\PYGZhy{}Jan\PYGZhy{}20  28.16  28.95  27.73  28.82
2  6\PYGZhy{}Jan\PYGZhy{}20  28.53  28.81  28.00  28.39
3  7\PYGZhy{}Jan\PYGZhy{}20  28.17  28.28  26.08  26.44
4  8\PYGZhy{}Jan\PYGZhy{}20  26.37  26.40  24.86  25.19
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df\PYGZus{}2mo}\PYG{o}{.}\PYG{n}{tail}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
         Date   Open   High    Low  Close
35  24\PYGZhy{}Feb\PYGZhy{}20  29.16  29.47  28.08  29.07
36  25\PYGZhy{}Feb\PYGZhy{}20  29.40  29.40  26.83  27.60
37  26\PYGZhy{}Feb\PYGZhy{}20  27.59  28.93  27.30  27.84
38  27\PYGZhy{}Feb\PYGZhy{}20  27.13  27.56  25.85  25.89
39  28\PYGZhy{}Feb\PYGZhy{}20  24.90  26.66  24.51  26.45
\end{sphinxVerbatim}

The \sphinxcode{\sphinxupquote{pd.concat()}} function is actually much more powerful than just this one little use to which we’ve put it here.  But we will discuss that more after we’ve discussed the more complex of the operations in this chapter, merging.


\section{Merging is horizontal}
\label{\detokenize{chapter-12-concat-and-merge:merging-is-horizontal}}
Concatenation was appropriate when we had new rows (that is, new observations) to add to our dataset.  But what if we had new columns instead?  Keep in mind that, under the standard way we organize tabular data, columns represent the \sphinxstyleemphasis{variables} in our dataset.  So getting new columns means learning more information about the rows we already had.

We saw a simple example of this last week; it was simple enough that we didn’t need to learn the full power of merging to handle it.  Recall that we had a dataset of home mortgage applications, and we wanted to add into it a variable that measured political affiliation of the state in which the mortgage took place.  We thus got a table that provided a measure of political alignment for each state, and we used that to \sphinxstyleemphasis{add a new column} to our old home mortgage dataset.  Each row in the mortgage dataset got a new variable measuring political alignment.  The table grew \sphinxstyleemphasis{horizontally} with new information from another table.

In fact, when we have only one column to add, the technique from last week’s class is easier than the full complexity of merging.  Recall how we did it:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} make a dictionary that maps state abbreviations to voting measurements}
\PYG{n}{repub\PYGZus{}votes\PYGZus{}in\PYGZus{}state} \PYG{o}{=} \PYG{n+nb}{dict}\PYG{p}{(} \PYG{n+nb}{zip}\PYG{p}{(} \PYG{n}{df\PYGZus{}election}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{State}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{df\PYGZus{}election}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Trump}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{p}{)} \PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} apply that dictionary to our home mortgage data to make a new column}
\PYG{n}{df\PYGZus{}mortgages}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Trump2016}\PYG{l+s+s1}{\PYGZpc{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{df\PYGZus{}mortgages}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{State}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{apply}\PYG{p}{(} \PYG{n}{repub\PYGZus{}votes\PYGZus{}in\PYGZus{}state} \PYG{p}{)}
\end{sphinxVerbatim}

But what if the situation is more complicated?  This can happen in several ways.  In each way, \sphinxcode{\sphinxupquote{pd.merge()}} is there to solve the problem.  Let’s look at each way that tables might grow horizontally.


\section{Adding many columns at once}
\label{\detokenize{chapter-12-concat-and-merge:adding-many-columns-at-once}}
The technique shown above, which we used last week in class, is easy if bringing in only one new column.  If we wanted to bring in many new columns, we’d need to apply that technique repeatedly, in a loop over those columns.  But \sphinxcode{\sphinxupquote{pd.merge()}} can do it all in one function call, and for the reasons we learned last week, that will probably be faster than a Python loop.

Let’s consider a concrete example to understand the idea of importing several new columns at once.  Consider a dataset we’ve seen before, tracking the number of confirmed COVID\sphinxhyphen{}19 cases over time in various countries.  Let’s say we wanted to see if the growth patterns in such a dataset were in any way related to health care information about the country, such as how much they spend on health care, how many doctors per capita, and so on.  We’ll need to bring in another dataset with all that information about each country, and import it in as new columns.  See the illustration below.

(All tables illustrated from here on will have “…” in the final rows and columns, to indicate that the table is really much bigger, and we’re showing only a portion in the illustration.)

\sphinxincludegraphics{{merge-of-health-data}.png}

The resulting DataFrame, on the bottom of the illustration, has all the data we want about each country, the COVID case data followed by the health data.

If the rows were not in exactly the same order in each DataFrame, the ones on the right will be reordered so that they match correctly with the rows on the left.  To do this, we need a unique ID for each row that is consistent across both datasets.  In this case, we would use the country name.

We’re making two important assumptions here.
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
The list of countries is exactly the same in both datasets, so we don’t have any leftover rows in either one.  This is rarely how actual data works; there’s usually some discrepancy, so we’ll discuss {\hyperref[\detokenize{chapter-12-concat-and-merge:when-there-is-no-match-for-some-rows}]{\emph{next}}} how to handle that.

\item {} 
The country names are spelled and formatted exactly the same in both datasets.  This is also not always true, so {\hyperref[\detokenize{chapter-12-concat-and-merge:ensuring-a-unique-id-appears-in-both-datasets}]{\emph{at the end of this chapter}}}, we’ll talk about how to fix that problem if and when it arises in your own work.

\end{enumerate}

This operation is called a \sphinxstyleemphasis{merge} in pandas or a \sphinxstyleemphasis{join} in SQL.  We could do it with code like the following.  We say we “merge \sphinxstyleemphasis{on}” the column we’re using as the unique ID.  So the illustration above is a merge on country name (or a join on country name).  In the left dataset, the column is called “Country/Region” and in the right dataset, it’s called “Country.”  So the code for this merge looks like the following.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df\PYGZus{}merged} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{merge}\PYG{p}{(} \PYG{n}{df\PYGZus{}cases}\PYG{p}{,} \PYG{n}{df\PYGZus{}health}\PYG{p}{,}
    \PYG{n}{left\PYGZus{}on}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Country/Region}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{right\PYGZus{}on}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Country}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\end{sphinxVerbatim}

If the column name had been the same in both DataFrames, we could have done it more succinctly.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df\PYGZus{}merged} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{merge}\PYG{p}{(} \PYG{n}{df\PYGZus{}cases}\PYG{p}{,} \PYG{n}{df\PYGZus{}health}\PYG{p}{,} \PYG{n}{on}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Country}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\end{sphinxVerbatim}


\section{When there is no match for some rows}
\label{\detokenize{chapter-12-concat-and-merge:when-there-is-no-match-for-some-rows}}
The first assumption mentioned above was that each row in the COVID dataset matched up with exactly one row in the health dataset.  The two datasets were the same size and had the same countries.  But what if this had not been the case?  Let’s consider two merging examples where the rows of the one dataset don’t match up perfectly with those of the other.  First, what if some rows in one dataset don’t match up with any rows from the other dataset?

Recall the example from the start of this chapter about my friend’s nonprofit.  I gave my students a comprehensive database from the U.S. government detailing lots of information about every institution of higher education in the U.S., over 7000 of them.  We wanted to merge that with the list of schools who had partnered with the climate nonprofit, of which there were fewer than 500.  Of course, the nonprofit hadn’t partnered with \sphinxstyleemphasis{every} school in the U.S.; that would be impressive!  So clearly some of the rows in the big dataset were not going to match with any of the rows in the climate dataset.  What do we do in that case?

Keeping in mind the goal of that project, we want to ensure that we keep in our dataset all the schools in the comprehensive dataset, because we will want to do analytics on those schools who \sphinxstyleemphasis{haven’t} signed up with the nonprofit.  There may be interesting patterns that help us see which schools tend not to sign up.  But the rows for those schools will not have any climate data to add, so there will be a lot of missing values in the merged dataset, as shown in the following illustration.

\sphinxincludegraphics{{merge-of-school-data}.png}

Because the comprehensive dataset has over 7000 rows and we add climate data for less than 500 schools, the vast majority of the rows (about 6500/7000, or 93\%) of them have no climate data, only missing values.  Those missing values are shown as blank cells in the illustration, but pandas would show them as NaNs.

But this is exactly how we wanted it, because then we can consider two subpopulations, the schools with climate data and the schools without.  We could investigate differences in their attributes and perhaps verify some such differences with hypothesis tests or other tools.

Because we used the \sphinxstyleemphasis{left} DataFrame as the definitive one, which we did not want to alter, and we brought the \sphinxstyleemphasis{right} DataFrame into it, we call this a \sphinxstyleemphasis{left join.}  The code for doing this operation is exactly like the previous \sphinxcode{\sphinxupquote{pd.merge()}} example, with one exception: we tell it that the left DataFrame is the definitive one, using the \sphinxcode{\sphinxupquote{how}} keyword.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df\PYGZus{}merged} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{merge}\PYG{p}{(} \PYG{n}{df\PYGZus{}big}\PYG{p}{,} \PYG{n}{df\PYGZus{}climate}\PYG{p}{,}
    \PYG{n}{left\PYGZus{}on}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{NAME}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{right\PYGZus{}on}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{fullname}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{how}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{left}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\end{sphinxVerbatim}

If we had chosen to do \sphinxcode{\sphinxupquote{how='right'}} instead, the right DataFrame would be considered the definitive one.  Any school from the left DataFrame that didn’t appear in the right DataFrame would be discarded, and we would end up with under 500 rows, precisely one row for each school in the climate nonprofit’s dataset.

Note that we’re still making the unrealistic assumption that the school names in the government dataset will match perfectly with those in the nonprofit’s dataset, and we’ll address that at the end of the chapter.

This example showed what it was like if some of the rows in the left dataset match up with \sphinxstyleemphasis{zero} rows in the right dataset.  But what if they match up with \sphinxstyleemphasis{many} rows in the right dataset?


\section{When there are many matches for some rows}
\label{\detokenize{chapter-12-concat-and-merge:when-there-are-many-matches-for-some-rows}}
Let’s consider another example, this one from sports.  We’ll use NFL football, but if you’re not familiar with the sport, the example will still make sense.  All you need to know is that each team has many players, and that each \sphinxstyleemphasis{play} is a small part of a football game that uses just some of the team’s players.  Some plays have a \sphinxstyleemphasis{receiver,} which is the player who catches the ball thrown to him (if any–sometimes the play does not involve throwing the ball).

As always in this chapter, imagine two datasets.  The first is the set of all NFL players in a certain year and their stats for that year.  (You can get these datasets online for free; here I’ll use a small sample of the players from the 2009 season.)  The second is the set of all plays that happened in that same season, in any game.  (The NFL lets you fetch this data from their website for free; again, I’ll use a small sample of plays from the 2009 season.)

Perhaps we have a theory we want to test about a team’s receivers.  We want to compare certain statistics about the receiver to how the receiver performs in certain plays.  (The details are unimportant.)  So we will need to combine the two datasets, one with player stats and one with the plays from the games.  We will want to match them up so that a row in the merged dataset contains the stats for the player who caught the ball, that is, the receiver for that play.

Now let’s consider how we will handle the many possibilities for how rows might match across the datasets.  First let’s consider rows that match many other rows; this might happen in two ways.
\begin{itemize}
\item {} 
\sphinxstylestrong{What if a player is the receiver in more than one play?}  (This happens all the time, of course.  Once a player is hired by a team, they often play in lots of games, and are involved in many plays.)  We will want the player’s stats to appear in \sphinxstyleemphasis{every} play for which the player was the receiver.  Good news!  This is how merges always work; if a row in one DataFrame matches many rows in the other, the row is always \sphinxstyleemphasis{copied.}

\item {} 
\sphinxstylestrong{What if a play has more than one receiver?}  This actually cannot happen, according to the rules of the NFL.  Once a player has caught the ball, they are not eligible to pass it to another player.  (If you’re familiar with football, don’t start talking about laterals; that’s not a pass!)  So we don’t have to consider this possibility.

\end{itemize}

So those two considerations don’t seem to change our merging code at all.  It seems like a standard merge will do what we want.

But what about a row in one dataset matching zero rows in the other dataset?  This, too, might happen in two ways.
\begin{itemize}
\item {} 
\sphinxstylestrong{What if a player is the receiver in no play?}  (This happens often also.  A player may be hired by a team, but is not as good as other players on the team, and thus does not yet get to play in real games.)  We will not want this player to appear at all in our merged dataset, because we care about receivers who showed up in actual plays.

\item {} 
\sphinxstylestrong{What if a play has no receiver?}  (This happens often also.  There are many types of plays and not all involve throwing.)  We will not want this play to appear in our merged dataset, because the analysis we want to do is about plays that have a receiver.

\end{itemize}

Putting these two considerations together, it does not seem like we want either a left join or a right join.  Recall that a left join keeps all the rows of the left table and a right join keeps all the rows of the right table.  In this case, however, we want to keep only rows that appear in \sphinxstyleemphasis{both} tables.  This is called an \sphinxstyleemphasis{inner join,} and you can see it working in the illustration below.

\sphinxincludegraphics{{merge-of-nfl-data}.png}

The code looks the same as before, but only the \sphinxcode{\sphinxupquote{how}} parameter has changed, now using the value \sphinxcode{\sphinxupquote{"inner"}} rather than \sphinxcode{\sphinxupquote{"left"}} or \sphinxcode{\sphinxupquote{"right"}}.  Actually, \sphinxcode{\sphinxupquote{"inner"}} is the default value for \sphinxcode{\sphinxupquote{pd.merge()}}, so you can omit it in this case, but I include it for emphasis.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df\PYGZus{}merged} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{merge}\PYG{p}{(} \PYG{n}{df\PYGZus{}players}\PYG{p}{,} \PYG{n}{df\PYGZus{}plays}\PYG{p}{,}
    \PYG{n}{left\PYGZus{}on}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Player}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{right\PYGZus{}on}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Receiver}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{how}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{inner}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\end{sphinxVerbatim}

Notice that we specifically say that we want the stats for the player who was the receiver in the play, by asking the merge to happen using the Player column from the left dataset and the Receiver column from the right dataset.

This kind of merge will not introduce any new missing values, because if a row didn’t exist in the left or right dataset, it was not included in the result.  That’s the definition of an inner join, and that’s why we chose to use that method in this case.


\section{When I want to keep all the rows}
\label{\detokenize{chapter-12-concat-and-merge:when-i-want-to-keep-all-the-rows}}
An inner join is not appropriate for all merging situations.  Consider a different example.

Let’s imagine that two Bentley professors found out they had done research on some of the same firms, and wanted to share data.  Let’s say Professor Adams had investigated the executives at a set of firms, and had information about those roles, while Professor Cordova had information about the marketing investments of a similar set of firms.

When putting their data together, they don’t yet know what questions they’re going to ask; they’ll probably start with some exploratory data analysis.  So they don’t want to throw away any of their data yet.

If they used an inner join, then they’d keep only the firms that appear in both datasets; that’s not what they want.  A left or right join would also discard some firms.  But they want to keep them all.  This is called an \sphinxstyleemphasis{outer join,} and it’s shown in the illustration below.

\sphinxincludegraphics{{merge-of-firm-data}.png}

(The split of the data into three categories, each of size 50, is just for this example.  A real example is unlikely to be separated so symmetrically.)

The “Firm” column in the merged dataset will contain each name only once, and the row will be of one of three types.
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
If it was in both datasets, then the row contains data in every column (as long as the original datasets did).

\item {} 
If it was in the left dataset, then the row contains data about executives, with missing values for marketing.

\item {} 
If it was in the right dataset, then the row contains data about marketing, with missing values for executives.

\end{enumerate}

(Obviously, if the firm was in neither dataset, it doesn’t show up in the merge.)

The code is the same as all the code we’ve seen up to this point, but with \sphinxcode{\sphinxupquote{how='outer'}}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df\PYGZus{}merged} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{merge}\PYG{p}{(} \PYG{n}{df\PYGZus{}execs}\PYG{p}{,} \PYG{n}{df\PYGZus{}marketing}\PYG{p}{,} \PYG{n}{on}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Firm}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{how}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{outer}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\end{sphinxVerbatim}


\section{Summary}
\label{\detokenize{chapter-12-concat-and-merge:summary}}
Before we tackle the challenging question of what happens if there is no unique ID to use for merging, let’s review where we’ve been and add some key details.

\begin{sphinxadmonition}{note}{Big Picture \sphinxhyphen{} Concat adds rows and merge adds columns (usually!)}

As I’ve introduced it here, \sphinxcode{\sphinxupquote{pd.concat()}} combines the rows of two DataFrames together and \sphinxcode{\sphinxupquote{pd.merge()}} combines the columns.  While \sphinxcode{\sphinxupquote{pd.concat()}} always adds rows, \sphinxcode{\sphinxupquote{pd.merge()}} may or may not, depending on whether you use left, right, inner, or outer joins.

Although \sphinxcode{\sphinxupquote{pd.concat()}} and \sphinxcode{\sphinxupquote{pd.merge()}} have tons of options that let you do merges and concatenations in the opposite direction from what I taught here (e.g., concat horizontally or merge vertically), this is almost never what is called for in a data project, due to the way we typically arrange tabular data.
\end{sphinxadmonition}

The \sphinxcode{\sphinxupquote{pd.concat()}} function is the easy one, and simply unites two datasets vertically.  The \sphinxcode{\sphinxupquote{pd.merge()}} function is the more complicated of the two.  Let’s imagine that we’ve called \sphinxcode{\sphinxupquote{pd.merge(A,B)}} for two DataFrames \sphinxcode{\sphinxupquote{A}} and \sphinxcode{\sphinxupquote{B}}.
\begin{itemize}
\item {} 
With \sphinxcode{\sphinxupquote{how='inner'}}, the default, it creates new rows for every pair of rows from \sphinxcode{\sphinxupquote{A}} and \sphinxcode{\sphinxupquote{B}} that match on the specified columns, and it discards everything else.

\item {} 
With \sphinxcode{\sphinxupquote{how='left'}}, it creates new rows for every pair of rows from \sphinxcode{\sphinxupquote{A}} and \sphinxcode{\sphinxupquote{B}} that match on the specified columns, plus it also keeps every row from \sphinxcode{\sphinxupquote{A}} that didn’t match anything from \sphinxcode{\sphinxupquote{B}}, and fills in their \sphinxcode{\sphinxupquote{B}} columns with missing values.  This sees \sphinxcode{\sphinxupquote{A}} as the important dataset, into which we’re bringing some information from \sphinxcode{\sphinxupquote{B}} where possible.

\item {} 
With \sphinxcode{\sphinxupquote{how='right'}}, the reverse happens.  But you don’t need this option if you prefer thinking of the left dataset as the important one, into which we’re bringing new columns on the right.  Instead of \sphinxcode{\sphinxupquote{pd.merge(A,B,how='right')}}, you can always just use \sphinxcode{\sphinxupquote{pd.merge(B,A,how='left')}} instead.

\item {} 
With \sphinxcode{\sphinxupquote{how='outer'}}, it creates new rows for every pair of rows from \sphinxcode{\sphinxupquote{A}} and \sphinxcode{\sphinxupquote{B}} that match on the specified columns, plus it also
\begin{itemize}
\item {} 
keeps every row from \sphinxcode{\sphinxupquote{A}} that didn’t match anything from \sphinxcode{\sphinxupquote{B}}, and fills in their \sphinxcode{\sphinxupquote{B}} columns with missing values, and

\item {} 
keeps every row from \sphinxcode{\sphinxupquote{B}} that didn’t match anything from \sphinxcode{\sphinxupquote{A}}, and fills in their \sphinxcode{\sphinxupquote{A}} columns with missing values.  This throws no data away.

\end{itemize}

\end{itemize}

And as a final reminder, we’re covering merging because it’s extremely common and useful to find that you have two related datasets or databases that you want to bring together, so that subsequent analyses can benefit from relating the data in the two sources.

And yet it’s not common for those two datasets to have been planned carefully enough in advance that they share a unique ID system for their rows.  More than likely, the two datasets were created by different teams, organizations, or software systems, and have quite different contents and formats.  So we come to the final section of this chapter, figuring out how to do a merge even when there isn’t an obvious unique ID column to use for merging.


\section{Ensuring a unique ID appears in both datasets}
\label{\detokenize{chapter-12-concat-and-merge:ensuring-a-unique-id-appears-in-both-datasets}}
Ensuring that the datasets you want to merge each have a column that will match perfectly with the other dataset is an essential step before merging.  Sometimes that step is extremely easy and sometimes it is very challenging.  In the examples above, we assumed that the datasets already had columns that would match up perfectly.

And that’s not always an unrealistic assumption.  For instance, when we merged the NPR voting records from 2016 into the home mortgage dataset in class, we merged on the two\sphinxhyphen{}letter abbreviation for each state.  This standard set of abbreviations was established many years ago and is used consistently everywhere U.S. states are mentioned, so it was reliable and required no work on our part.

But let’s consider some more complex cases, so you’re ready for them when you encounter them.


\subsection{Merging on multiple columns}
\label{\detokenize{chapter-12-concat-and-merge:merging-on-multiple-columns}}
If you don’t have a single column that works as a unique ID, but you have a set of columns that togther form a unique ID in the same way in each dataset, pandas supports merging on multiple columns.  For instance, if your datasets each have columns for first and last names of the people in an organization, and you’re confident that no names repeat (e.g., only one John Smith, only one Erin Jones, etc.), then you can tell pandas to use more than one column to identify rows when merging.  Just supply the list of column names when merging.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df\PYGZus{}merged} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{merge}\PYG{p}{(} \PYG{n}{df\PYGZus{}members}\PYG{p}{,} \PYG{n}{df\PYGZus{}activities}\PYG{p}{,}
    \PYG{n}{left\PYGZus{}on}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{First Name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Last Name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}
    \PYG{n}{right\PYGZus{}on}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Given name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Surname}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{p}{)}
\end{sphinxVerbatim}


\subsection{Changing the format of a column}
\label{\detokenize{chapter-12-concat-and-merge:changing-the-format-of-a-column}}
When you plan to merge two datasets, but no column is appropriate for the match, sometimes a quick computation of a new column will do the trick.

\sphinxstylestrong{Example:}  If you were merging a dataset of customers using their phone numbers, perhaps dataset A contains just the numeric values (e.g., 17818913171) and dataset B contains the phone numbers formatted for human readability (e.g., +1 (781) 891\sphinxhyphen{}3171).  You can create a new column in dataset B that removes all the spaces, plusses, minuses, and parentheses from the phone numbers, so that they’re ready to match with dataset A.


\subsection{Joining multiple columns into one}
\label{\detokenize{chapter-12-concat-and-merge:joining-multiple-columns-into-one}}
It may also be possible to compute an appropriate column for merging by combining more than one column together.

\sphinxstylestrong{Example:}  Let’s say you were merging two datasets about albums released by recording artists.  The artists have a unique ID in your datasets, but the albums don’t.  If you know that no artist released more than one album in the same month, you could combine together the artist’s unique ID with the month and year of the album’s release to form a unique ID for the album.  E.g., if The Beatles had ID 2789045 and you’re considering the Sgt. Pepper album (May 1967), then you would use the code 2789045\sphinxhyphen{}May\sphinxhyphen{}1967 for that album.  You could compute such a code for each row in each DataFrame.


\subsection{Sequences with different frequencies}
\label{\detokenize{chapter-12-concat-and-merge:sequences-with-different-frequencies}}
Another common problem is merging two types of time\sphinxhyphen{}based data that were reported on different time scales.  For instance, let’s say you are trying to study police activity and criminal activity in a city.  You have crime data in the form of daily records and police reports in terms of officers’ hourly shifts.  If you wanted to combine these two datasets based on time, the difference in reporting frequency means it’s not obvious how to do it.

So pandas provides two functions for helping with such situations.  These notes do not cover them in detail, but suggest you check out the \sphinxhref{https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.merge\_ordered.html}{documentation for \sphinxcode{\sphinxupquote{pd.merge\_orered()}}} and the \sphinxhref{https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.merge\_asof.html\#pandas.merge\_asof}{documentation for \sphinxcode{\sphinxupquote{pd.merge\_asof()}}} for more sophisticated handling of time\sphinxhyphen{}based merge data.


\subsection{What about unstandardized text?}
\label{\detokenize{chapter-12-concat-and-merge:what-about-unstandardized-text}}
This is more or less the hardest scenario.  For instance, in Fall 2019, when my students wanted to merge the government’s comprehensive database of universities with the climate commitments of the schools who were working with our nonprofit client, our best option was to merge on the institution’s name.  This is problematic due to variations in naming and spelling.  For instance, what if one dataset writes Bentley University and the other writes Bentley Univ.?  Or what if one dataset writes University of North Carolina at Chapel Hill and the other writes UNC Chapel Hill?  How is a computer to know how to match these up?  (That project actually involved merging several datasets about universities, and this same problem arose more than once!)

The short answer is that the computer will not figure this out, because \sphinxcode{\sphinxupquote{pd.merge()}} only matches on exact equality of IDs, and so you as the data scientist are in charge of somehow creating columns of unique IDs in both datasets that will match up perfectly.  This may require learning something about that domain.  In Fall 2019, my students and I spent time googling various schools whose names didn’t seem to appear in the government’s dataset to figure out why!

When you’re stuck trying to get two similar\sphinxhyphen{}but\sphinxhyphen{}not\sphinxhyphen{}the\sphinxhyphen{}same columns of text to try to match perfectly, I suggest the following method.  Whether this method is quick and easy or long and difficult varies significantly from one problem to the next.  But the outline is the same.
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Figure out the column in each dataset that is \sphinxstyleemphasis{closest} to being useful as a unique ID.  (In the university example, this was the university name in each dataset, which was written the same in both datasets for many schools, but definitely not all.)

\item {} 
Figure out which dataset is to be the definitive one; this is typically the larger dataset.  (In the university example, this was the comprehensive government dataset.)  We will use the merge column from this definitive dataset as the “official” ID for each row, and we must adjust the other dataset so that it uses these “official” IDs rather than its own versions/spellings.

\item {} 
Add a new column to the smaller dataset that contains the official unique ID \sphinxstyleemphasis{from the other, larger dataset} that it should match.  (In the university example, this means labeling each row in the nonprofit’s dataset with that school’s name as it appears in the government’s dataset.)  This is not always easy.

\item {} 
Run \sphinxcode{\sphinxupquote{pd.merge()}} and have it match the unique ID column in the larger dataset with this newly created column in the smaller dataset, which is now a perfect match.

\end{enumerate}

Notice that steps 1, 2, and 4 are quick and easy, but step 3 is where problems may or may not arise.  Depending on how well the chosen columns match in the two datasets, step 3 might take a short time or a long time.


\subsection{Extended Example}
\label{\detokenize{chapter-12-concat-and-merge:extended-example}}
Let’s actually try to merge two datasets of university data.  I will load here the comprehensive university dataset I mentioned, originally downloaded from \sphinxhref{https://data.world/kitedwards08/us-university-survey-2014}{here}, as well as a US News university rankings dataset, originally downloaded from \sphinxhref{https://data.world/education/university-rankings-2017}{here}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df\PYGZus{}big} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZus{}static/Colleges\PYGZus{}and\PYGZus{}Universities.csv}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{df\PYGZus{}big}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
            X          Y   FID  IPEDSID  \PYGZbs{}
0  \PYGZhy{}92.260490  34.759308  7001   107840   
1 \PYGZhy{}121.289431  38.713353  7002   112181   
2 \PYGZhy{}118.287070  34.101481  7003   116660   
3 \PYGZhy{}121.652662  36.700631  7004   125310   
4  \PYGZhy{}71.070737  42.369930  7005   164368   

                                       NAME                 ADDRESS  \PYGZbs{}
0                           Shorter College           604 Locust St   
1             Citrus Heights Beauty College          7518 Baird Way   
2  Joe Blasco Makeup Artist Training Center   1670 Hillhurst Avenue   
3                  Waynes College of Beauty  1271 North Main Street   
4        Hult International Business School      1 Education Street   

        ADDRESS2            CITY STATE    ZIP  ...          ALIAS SIZE\PYGZus{}SET  \PYGZbs{}
0  NOT AVAILABLE   N Little Rock    AR  72114  ...  NOT AVAILABLE       \PYGZhy{}3   
1  NOT AVAILABLE  Citris Heights    CA  95610  ...  NOT AVAILABLE       \PYGZhy{}3   
2  NOT AVAILABLE     Los Angeles    CA  90027  ...  NOT AVAILABLE       \PYGZhy{}3   
3  NOT AVAILABLE         Salinas    CA  93906  ...  NOT AVAILABLE       \PYGZhy{}3   
4  NOT AVAILABLE       Cambridge    MA  02141  ...  NOT AVAILABLE       \PYGZhy{}3   

   INST\PYGZus{}SIZE PT\PYGZus{}ENROLL  FT\PYGZus{}ENROLL TOT\PYGZus{}ENROLL  HOUSING DORM\PYGZus{}CAP  TOT\PYGZus{}EMPLOY  \PYGZbs{}
0          1        24         28         52        2        0          18   
1          1         6         24         30        2        0           9   
2          1         0         24         24        2        0          11   
3          1        18         16         34        2        0           9   
4          2         0       2243       2243        2        0         143   

      SHELTER\PYGZus{}ID  
0  NOT AVAILABLE  
1  NOT AVAILABLE  
2  NOT AVAILABLE  
3  NOT AVAILABLE  
4  NOT AVAILABLE  

[5 rows x 46 columns]
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df\PYGZus{}rank} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZus{}static/National Universities Rankings.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{encoding}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{latin}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{df\PYGZus{}rank}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                    Name       Location  Rank  \PYGZbs{}
0   Princeton University  Princeton, NJ     1   
1     Harvard University  Cambridge, MA     2   
2  University of Chicago    Chicago, IL     3   
3        Yale University  New Haven, CT     3   
4    Columbia University   New York, NY     5   

                                         Description Tuition and fees  \PYGZbs{}
0  Princeton, the fourth\PYGZhy{}oldest college in the Un...         \PYGZdl{}45,320    
1  Harvard is located in Cambridge, Massachusetts...         \PYGZdl{}47,074    
2  The University of Chicago, situated in Chicago...         \PYGZdl{}52,491    
3  Yale University, located in New Haven, Connect...         \PYGZdl{}49,480    
4  Columbia University, located in Manhattan\PYGZsq{}s Mo...         \PYGZdl{}55,056    

  In\PYGZhy{}state Undergrad Enrollment  
0      NaN                5,402  
1      NaN                6,699  
2      NaN                5,844  
3      NaN                5,532  
4      NaN                6,102  
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{len}\PYG{p}{(} \PYG{n}{df\PYGZus{}big} \PYG{p}{)}\PYG{p}{,} \PYG{n+nb}{len}\PYG{p}{(} \PYG{n}{df\PYGZus{}rank} \PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(7735, 231)
\end{sphinxVerbatim}

\sphinxstylestrong{Step 1.} Figure out the closest columns we have to making a match.  The only columns we could have a hope of using to uniquely identify these schools are their names.  No other column in the ranking dataset could possibly be a unique ID that would also be in the big dataset.

\sphinxstylestrong{Step 2.} Figure out which dataset is to be the definitive one.  Clearly, the comprehensive dataset should be the definitive one, and the rankings merged into it.  So the university names in the big dataset are what we’ll use as the schools’ official names.

\sphinxstylestrong{Step 3.} Add a new column to the ranking dataset and, in it, store the correct official school name for each row.  (Remember that official names come from the big dataset.)  This is the tricky part.

Let’s just get a sense of how many of the 231 rows in the ranking dataset have an exact match in the big dataset, and thus their official names are already in the ranking dataset.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{official\PYGZus{}names} \PYG{o}{=} \PYG{n+nb}{list}\PYG{p}{(} \PYG{n}{df\PYGZus{}big}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{NAME}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{p}{)}

\PYG{k}{def} \PYG{n+nf}{has\PYGZus{}exact\PYGZus{}match} \PYG{p}{(} \PYG{n}{name\PYGZus{}from\PYGZus{}rank\PYGZus{}df} \PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{n}{name\PYGZus{}from\PYGZus{}rank\PYGZus{}df} \PYG{o+ow}{in} \PYG{n}{official\PYGZus{}names}

\PYG{n+nb}{sum}\PYG{p}{(} \PYG{n}{df\PYGZus{}rank}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{apply}\PYG{p}{(} \PYG{n}{has\PYGZus{}exact\PYGZus{}match} \PYG{p}{)} \PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
141
\end{sphinxVerbatim}

Thus 90 schools do \sphinxstyleemphasis{not} have an exact match.  Those are the 90 we need to solve.  It would be tedious to match them up by hand, because there are 90.  So we will use a built\sphinxhyphen{}in Python text module to try to do some \sphinxstyleemphasis{approximate} string matching for us.  The Python module \sphinxcode{\sphinxupquote{difflib}} has a function called \sphinxcode{\sphinxupquote{get\_close\_matches()}} that will take a piece of text and a list of options, and give you the closest matches.  Here’s an example.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{difflib} \PYG{k+kn}{import} \PYG{n}{get\PYGZus{}close\PYGZus{}matches}
\PYG{n}{get\PYGZus{}close\PYGZus{}matches}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Python is cool}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
    \PYG{p}{[} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{this is not close}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{also not close}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
      \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Python is cruel}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Nathan is cool}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{]} \PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[\PYGZsq{}Python is cruel\PYGZsq{}, \PYGZsq{}Nathan is cool\PYGZsq{}]
\end{sphinxVerbatim}

Note that it doesn’t always find a good guess, if there isn’t one.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{get\PYGZus{}close\PYGZus{}matches}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{pork}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{p}{[} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{salad}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{lollipops}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{soda}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{]} \PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[]
\end{sphinxVerbatim}

Let’s use \sphinxcode{\sphinxupquote{get\_close\_matches()}} to create a function that will match up university names across the two datasets if they’re just off by a small amount.  This could automate some of the matching we’d otherwise have to do by hand for those 90 schools that didn’t match exactly.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{get\PYGZus{}closest\PYGZus{}official\PYGZus{}name} \PYG{p}{(} \PYG{n}{name\PYGZus{}from\PYGZus{}df\PYGZus{}rank} \PYG{p}{)}\PYG{p}{:}

    \PYG{c+c1}{\PYGZsh{} If there\PYGZsq{}s an exact match, we\PYGZsq{}re already done.}
    \PYG{k}{if} \PYG{n}{has\PYGZus{}exact\PYGZus{}match}\PYG{p}{(} \PYG{n}{name\PYGZus{}from\PYGZus{}df\PYGZus{}rank} \PYG{p}{)}\PYG{p}{:}
        \PYG{k}{return} \PYG{n}{name\PYGZus{}from\PYGZus{}df\PYGZus{}rank}
    
    \PYG{c+c1}{\PYGZsh{} Get the closest matches, if any.}
    \PYG{n}{close\PYGZus{}matches} \PYG{o}{=} \PYG{n}{get\PYGZus{}close\PYGZus{}matches}\PYG{p}{(} \PYG{n}{name\PYGZus{}from\PYGZus{}df\PYGZus{}rank}\PYG{p}{,} \PYG{n}{official\PYGZus{}names} \PYG{p}{)}
    
    \PYG{c+c1}{\PYGZsh{} If there weren\PYGZsq{}t any, return None}
    \PYG{k}{if} \PYG{n+nb}{len}\PYG{p}{(} \PYG{n}{close\PYGZus{}matches} \PYG{p}{)} \PYG{o}{==} \PYG{l+m+mi}{0}\PYG{p}{:}
        \PYG{k}{return} \PYG{k+kc}{None}
    
    \PYG{c+c1}{\PYGZsh{} Otherwise, return the first one}
    \PYG{k}{return} \PYG{n}{close\PYGZus{}matches}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} Test it}
\PYG{n}{get\PYGZus{}closest\PYGZus{}official\PYGZus{}name}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Bentley Universal}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsq{}Bentley University\PYGZsq{}
\end{sphinxVerbatim}

Let’s apply that function to every row in the small dataset.  Note that \sphinxcode{\sphinxupquote{get\_close\_matches()}} can be a bit slow, so the following code actually takes about 15 seconds to complete executing.  (It would be even slower if we didn’t have the first \sphinxcode{\sphinxupquote{if}} statement in \sphinxcode{\sphinxupquote{get\_closest\_official\_name()}}, which skips \sphinxcode{\sphinxupquote{get\_close\_matches()}} when it’s not needed.)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df\PYGZus{}rank}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Official Name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{df\PYGZus{}rank}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{apply}\PYG{p}{(} \PYG{n}{get\PYGZus{}closest\PYGZus{}official\PYGZus{}name} \PYG{p}{)}
\PYG{n}{df\PYGZus{}rank}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                    Name       Location  Rank  \PYGZbs{}
0   Princeton University  Princeton, NJ     1   
1     Harvard University  Cambridge, MA     2   
2  University of Chicago    Chicago, IL     3   
3        Yale University  New Haven, CT     3   
4    Columbia University   New York, NY     5   

                                         Description Tuition and fees  \PYGZbs{}
0  Princeton, the fourth\PYGZhy{}oldest college in the Un...         \PYGZdl{}45,320    
1  Harvard is located in Cambridge, Massachusetts...         \PYGZdl{}47,074    
2  The University of Chicago, situated in Chicago...         \PYGZdl{}52,491    
3  Yale University, located in New Haven, Connect...         \PYGZdl{}49,480    
4  Columbia University, located in Manhattan\PYGZsq{}s Mo...         \PYGZdl{}55,056    

  In\PYGZhy{}state Undergrad Enrollment          Official Name  
0      NaN                5,402   Princeton University  
1      NaN                6,699     Harvard University  
2      NaN                5,844  University of Chicago  
3      NaN                5,532        Yale University  
4      NaN                6,102     Coleman University  
\end{sphinxVerbatim}

The results are correct for the first four schools, which were exact matches, but not so good for Columbia.  The only way to check to see if this worked out well is to do a manual check, because only a human is going to be able to assess whether Columbia University and Coleman University are the same; Python did its best.

We can check by taking a glance over the following output, and noting which rows are wrong.  I don’t include the full output here of all 90 discrepancies, just to save space, but you can use \sphinxcode{\sphinxupquote{pd.set\_option( 'display.max\_rows', None )}} to see them all.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{rows\PYGZus{}with\PYGZus{}guesses} \PYG{o}{=} \PYG{n}{df\PYGZus{}rank}\PYG{p}{[} \PYG{n}{df\PYGZus{}rank}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{!=} \PYG{n}{df\PYGZus{}rank}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Official Name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{p}{]}
\PYG{n}{rows\PYGZus{}with\PYGZus{}guesses}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Official Name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                                         Name  \PYGZbs{}
4                         Columbia University   
18         Washington University in St. Louis   
21         University of California\PYGZhy{}\PYGZhy{}Berkeley   
24      University of California\PYGZhy{}\PYGZhy{}Los Angeles   
25                     University of Virginia   
..                                        ...   
222               New Mexico State University   
225       University of Massachusetts\PYGZhy{}\PYGZhy{}Boston   
226    University of Massachusetts\PYGZhy{}\PYGZhy{}Dartmouth   
227         University of Missouri\PYGZhy{}\PYGZhy{}St. Louis   
228  University of North Carolina\PYGZhy{}\PYGZhy{}Greensboro   

                                  Official Name  
4                            Coleman University  
18            Washington University in St Louis  
21            University of California\PYGZhy{}Berkeley  
24         University of California\PYGZhy{}Los Angeles  
25                        University of Georgia  
..                                          ...  
222          New Mexico State University\PYGZhy{}Grants  
225          University of Massachusetts\PYGZhy{}Boston  
226       University of Massachusetts\PYGZhy{}Dartmouth  
227             University of Missouri\PYGZhy{}St Louis  
228  University of North Carolina at Greensboro  

[90 rows x 2 columns]
\end{sphinxVerbatim}

We see that in many cases, it did a good job, such as in rows 18, 21, 24, and 225 through 228.  We know that rows 4 and 25 are wrong, but is row 222 wrong?  That all depends on whether Grants is the location of the main campus for New Mexico State University.  Now you see why my students and I ended up on Google!

After inspecting the full list of 90 discrepancies, I found 30 that I still needed to fix by hand.  So the computer had done two\sphinxhyphen{}thirds of its guessing job right, saving me some time.  But how do I manually correct the 30 mistakes I found?  For instance, how do I correct row 4, which clearly isn’t right?  I need to know the exact name of Columbia University in \sphinxcode{\sphinxupquote{df\_big}}.

Let’s do a search.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Show me all names containing Columbia...}
\PYG{n}{df\PYGZus{}big}\PYG{p}{[}\PYG{n}{df\PYGZus{}big}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{NAME}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{str}\PYG{o}{.}\PYG{n}{contains}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Columbia}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{NAME}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
60                      Paul Mitchell The School\PYGZhy{}Columbia
439                    American Career Institute√Columbia
619                                      Columbia College
668                             Virginia College\PYGZhy{}Columbia
750                          Columbia Southern University
872                              Centura College\PYGZhy{}Columbia
1366                University of Phoenix\PYGZhy{}Columbia Campus
1438                     ITT Technical Institute\PYGZhy{}Columbia
1610                      Southeastern Institute\PYGZhy{}Columbia
1907        Kenneth Shuler School of Cosmetology\PYGZhy{}Columbia
1975                        Columbia Theological Seminary
2059                    Regency Beauty Institute\PYGZhy{}Columbia
2099                    Remington College\PYGZhy{}Columbia Campus
2295                                     Columbia College
2583                                     Columbia College
2704                          Columbia College of Nursing
2958                    Columbia\PYGZhy{}Greene Community College
3346                               Lower Columbia College
3348                               Columbia Basin College
3404                                     Columbia College
3622                     Columbia Gorge Community College
3936                     Columbia State Community College
4042              Teachers College at Columbia University
4356        Columbiana County Career and Technical Center
4385                 Columbia Centro Universitario\PYGZhy{}Caguas
4509                            South University\PYGZhy{}Columbia
4666    University of the District of Columbia David A...
4719                    Columbia International University
4723    Kenneth Shuler School of Cosmetology and Nails...
4728                University of South Carolina\PYGZhy{}Columbia
4974               Lincoln College of Technology\PYGZhy{}Columbia
5027                                     Columbia College
5099                          Columbia Area Career Center
5371                             Columbia College\PYGZhy{}Chicago
5581               University of the District of Columbia
5664                           Columbia College Hollywood
5775              Strayer University\PYGZhy{}District of Columbia
6369                      University of Missouri\PYGZhy{}Columbia
6661          Columbia University in the City of New York
7589                  Columbia Centro Universitario\PYGZhy{}Yauco
Name: NAME, dtype: object
\end{sphinxVerbatim}

Holy cow!  Let’s try to narrow our search a bit…

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Just the rows with Columbia and University...}
\PYG{n}{df\PYGZus{}big}\PYG{p}{[}\PYG{n}{df\PYGZus{}big}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{NAME}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{str}\PYG{o}{.}\PYG{n}{contains}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Columbia}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
     \PYG{o}{\PYGZam{}} \PYG{n}{df\PYGZus{}big}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{NAME}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{str}\PYG{o}{.}\PYG{n}{contains}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{University}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{NAME}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
750                          Columbia Southern University
1366                University of Phoenix\PYGZhy{}Columbia Campus
4042              Teachers College at Columbia University
4509                            South University\PYGZhy{}Columbia
4666    University of the District of Columbia David A...
4719                    Columbia International University
4728                University of South Carolina\PYGZhy{}Columbia
5581               University of the District of Columbia
5775              Strayer University\PYGZhy{}District of Columbia
6369                      University of Missouri\PYGZhy{}Columbia
6661          Columbia University in the City of New York
Name: NAME, dtype: object
\end{sphinxVerbatim}

Aha, Columbia University in the City of New York was so long of a phrase that \sphinxcode{\sphinxupquote{get\_close\_matches()}} did not think it was “close” to Columbia University.  So now I’ve found that the entry for row 4 in \sphinxcode{\sphinxupquote{df\_rank{[}'Official Name'{]}}} should be Columbia University in the City of New York.  I can simply tell Python to change it.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df\PYGZus{}rank}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{l+m+mi}{4}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Official Name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Columbia University in the City of New York}\PYG{l+s+s1}{\PYGZsq{}}
\end{sphinxVerbatim}

When I’m done manually investigating the 30 schools that had to be fixed by hand, I will have 30 lines of code that look just like the one above, but for different schools.  Here’s a sample.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df\PYGZus{}rank}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{l+m+mi}{4}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Official Name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Columbia University in the City of New York}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{df\PYGZus{}rank}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{l+m+mi}{34}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Official Name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Georgia Institute of Technology\PYGZhy{}Main Campus}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{df\PYGZus{}rank}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{l+m+mi}{41}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Official Name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Tulane University of Louisiana}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{df\PYGZus{}rank}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{l+m+mi}{52}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Official Name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Pennsylvania State University\PYGZhy{}Main Campus}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{c+c1}{\PYGZsh{} and so on, for a total of 30 changes}
\end{sphinxVerbatim}

But if we’re trying to follow DRY principles, we notice that there’s definitely a lot of repeated code here.  We’re copying and pasting the \sphinxcode{\sphinxupquote{df\_rank.loc{[}...,'Official Name'{]} = '...'}} part each time.  We could simplify this by creating a Python dictionary with just our corrections.  Here I include all 30 corrections as they would be if we had carefully investigated each.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Store corrections in a dictionary:}
\PYG{n}{corrections} \PYG{o}{=} \PYG{p}{\PYGZob{}}
    \PYG{l+m+mi}{4}   \PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Columbia University in the City of New York}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
    \PYG{l+m+mi}{34}  \PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Georgia Institute of Technology\PYGZhy{}Main Campus}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
    \PYG{l+m+mi}{41}  \PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Tulane University of Louisiana}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
    \PYG{l+m+mi}{52}  \PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Pennsylvania State University\PYGZhy{}Main Campus}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
    \PYG{l+m+mi}{54}  \PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{University of Washington\PYGZhy{}Seattle Campus}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
    \PYG{l+m+mi}{60}  \PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Purdue University\PYGZhy{}Main Campus}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
    \PYG{l+m+mi}{68}  \PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{University of Pittsburgh\PYGZhy{}Pittsburgh Campus}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
    \PYG{l+m+mi}{77}  \PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Virginia Polytechnic Institute and State University}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
    \PYG{l+m+mi}{85}  \PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{SUNY at Binghamton}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
    \PYG{l+m+mi}{109} \PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{University of South Carolina\PYGZhy{}Columbia}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
    \PYG{l+m+mi}{112} \PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{University of Missouri\PYGZhy{}System Office}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
    \PYG{l+m+mi}{114} \PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{University of Oklahoma Norman Campus}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
    \PYG{l+m+mi}{130} \PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Colorado State University\PYGZhy{}Fort Collins}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
    \PYG{l+m+mi}{135} \PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Louisiana State University\PYGZhy{}System Office}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
    \PYG{l+m+mi}{146} \PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Ohio University\PYGZhy{}Main Campus}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
    \PYG{l+m+mi}{149} \PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{SUNY at Albany}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
    \PYG{l+m+mi}{153} \PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Oklahoma State University\PYGZhy{}Oklahoma City}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
    \PYG{l+m+mi}{162} \PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{University of South Florida\PYGZhy{}Main Campus}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
    \PYG{l+m+mi}{181} \PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{University of New Mexico\PYGZhy{}Main Campus}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
    \PYG{l+m+mi}{186} \PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Widener University\PYGZhy{}Main Campus}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
    \PYG{l+m+mi}{187} \PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Kent State University at Kent}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
    \PYG{l+m+mi}{189} \PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Pace University\PYGZhy{}New York}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
    \PYG{l+m+mi}{193} \PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Bowling Green State University\PYGZhy{}Main Campus}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
    \PYG{l+m+mi}{222} \PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{New Mexico State University\PYGZhy{}Main Campus}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{p}{\PYGZcb{}}

\PYG{c+c1}{\PYGZsh{} Apply all the corrections at once:}
\PYG{k}{for} \PYG{n}{row\PYGZus{}index}\PYG{p}{,} \PYG{n}{fixed\PYGZus{}name} \PYG{o+ow}{in} \PYG{n}{corrections}\PYG{o}{.}\PYG{n}{items}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{df\PYGZus{}rank}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{n}{row\PYGZus{}index}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Official Name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{fixed\PYGZus{}name}

\PYG{c+c1}{\PYGZsh{} See if at least the top 5 look right:}
\PYG{n}{df\PYGZus{}rank}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                    Name       Location  Rank  \PYGZbs{}
0   Princeton University  Princeton, NJ     1   
1     Harvard University  Cambridge, MA     2   
2  University of Chicago    Chicago, IL     3   
3        Yale University  New Haven, CT     3   
4    Columbia University   New York, NY     5   

                                         Description Tuition and fees  \PYGZbs{}
0  Princeton, the fourth\PYGZhy{}oldest college in the Un...         \PYGZdl{}45,320    
1  Harvard is located in Cambridge, Massachusetts...         \PYGZdl{}47,074    
2  The University of Chicago, situated in Chicago...         \PYGZdl{}52,491    
3  Yale University, located in New Haven, Connect...         \PYGZdl{}49,480    
4  Columbia University, located in Manhattan\PYGZsq{}s Mo...         \PYGZdl{}55,056    

  In\PYGZhy{}state Undergrad Enrollment                                Official Name  
0      NaN                5,402                         Princeton University  
1      NaN                6,699                           Harvard University  
2      NaN                5,844                        University of Chicago  
3      NaN                5,532                              Yale University  
4      NaN                6,102  Columbia University in the City of New York  
\end{sphinxVerbatim}

\sphinxstylestrong{Step 4.} And now that all corrections have been made, we can do the merge with confidence.  We take care to merge the main dataset’s \sphinxcode{\sphinxupquote{"NAME"}} column with the smaller dataset’s \sphinxcode{\sphinxupquote{"Official Name"}} column.  This merge will be a left join, because we do not want to discard a school just because it wasn’t in US News’s rankings.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df\PYGZus{}merged} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{merge}\PYG{p}{(} \PYG{n}{df\PYGZus{}big}\PYG{p}{,} \PYG{n}{df\PYGZus{}rank}\PYG{p}{,} \PYG{n}{left\PYGZus{}on}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{NAME}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{right\PYGZus{}on}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Official Name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{how}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{left}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{df\PYGZus{}merged}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
            X          Y   FID  IPEDSID  \PYGZbs{}
0  \PYGZhy{}92.260490  34.759308  7001   107840   
1 \PYGZhy{}121.289431  38.713353  7002   112181   
2 \PYGZhy{}118.287070  34.101481  7003   116660   
3 \PYGZhy{}121.652662  36.700631  7004   125310   
4  \PYGZhy{}71.070737  42.369930  7005   164368   

                                       NAME                 ADDRESS  \PYGZbs{}
0                           Shorter College           604 Locust St   
1             Citrus Heights Beauty College          7518 Baird Way   
2  Joe Blasco Makeup Artist Training Center   1670 Hillhurst Avenue   
3                  Waynes College of Beauty  1271 North Main Street   
4        Hult International Business School      1 Education Street   

        ADDRESS2            CITY STATE    ZIP  ... TOT\PYGZus{}EMPLOY     SHELTER\PYGZus{}ID  \PYGZbs{}
0  NOT AVAILABLE   N Little Rock    AR  72114  ...         18  NOT AVAILABLE   
1  NOT AVAILABLE  Citris Heights    CA  95610  ...          9  NOT AVAILABLE   
2  NOT AVAILABLE     Los Angeles    CA  90027  ...         11  NOT AVAILABLE   
3  NOT AVAILABLE         Salinas    CA  93906  ...          9  NOT AVAILABLE   
4  NOT AVAILABLE       Cambridge    MA  02141  ...        143  NOT AVAILABLE   

   Name Location  Rank Description  Tuition and fees In\PYGZhy{}state  \PYGZbs{}
0   NaN      NaN   NaN         NaN               NaN      NaN   
1   NaN      NaN   NaN         NaN               NaN      NaN   
2   NaN      NaN   NaN         NaN               NaN      NaN   
3   NaN      NaN   NaN         NaN               NaN      NaN   
4   NaN      NaN   NaN         NaN               NaN      NaN   

   Undergrad Enrollment  Official Name  
0                   NaN            NaN  
1                   NaN            NaN  
2                   NaN            NaN  
3                   NaN            NaN  
4                   NaN            NaN  

[5 rows x 54 columns]
\end{sphinxVerbatim}

Now we have one large dataset containing both the generic data and the ranking data.  Although we see all missing values for ranking columns above, this is just because the first five schools in the dataset didn’t happen to be ranked by US News.  This is not surprising; there were over 7700 schools in the dataset and only 231 were ranked by US News.  But we can see that the merge did go correctly if we inspect a row that had ranking data.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df\PYGZus{}merged}\PYG{p}{[}\PYG{n}{df\PYGZus{}merged}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{NAME}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Harvard University}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
              X          Y  FID  IPEDSID                NAME  \PYGZbs{}
5822 \PYGZhy{}71.118234  42.374172   87   166027  Harvard University   

                 ADDRESS       ADDRESS2       CITY STATE    ZIP  ...  \PYGZbs{}
5822  Massachusetts Hall  NOT AVAILABLE  Cambridge    MA  02138  ...   

     TOT\PYGZus{}EMPLOY     SHELTER\PYGZus{}ID                Name       Location  Rank  \PYGZbs{}
5822      17141  NOT AVAILABLE  Harvard University  Cambridge, MA   2.0   

                                            Description  Tuition and fees  \PYGZbs{}
5822  Harvard is located in Cambridge, Massachusetts...          \PYGZdl{}47,074    

     In\PYGZhy{}state  Undergrad Enrollment       Official Name  
5822      NaN                 6,699  Harvard University  

[1 rows x 54 columns]
\end{sphinxVerbatim}

This is one of the most challenging merges you might have to do, but it’s good to be prepared for the worst case scenario!


\chapter{Miscellaneous Munging Methods (ETL)}
\label{\detokenize{chapter-13-etl:miscellaneous-munging-methods-etl}}\label{\detokenize{chapter-13-etl::doc}}
See also the slides that summarize a portion of this content.


\section{What do these words mean?}
\label{\detokenize{chapter-13-etl:what-do-these-words-mean}}
ETL stands for “Extract, Transform, and Load.”  This is the standard term for all the work you may need to do with data to get it ready for actual analysis.  Before we get to make attractive visualizations or do useful analyses and produce insights, we have to get the data into a form that makes those things possible.  Think of the terms as having roughly these meanings:
\begin{itemize}
\item {} 
Extract = get data from the web, a database, or wherever it’s originally located (and maybe save it into a CSV file on our computer, for example)

\item {} 
Transform = manipulate the content of the data to make it more suitable for our needs (such as converting column data types, handling missing values, etc.)

\item {} 
Load = get the data into our Python script, notebook, or other analysis software (which can be an easy one\sphinxhyphen{}liner for small data, but is harder for big data)

\end{itemize}

While ETL is an official term, the slang term is “munging.”  The word is well\sphinxhyphen{}chosen, in that it sounds a little bit awkward and a little bit gross.  Like data manipulation often is.  When most people say “data munging,” they’re probably referring more to the “transform” part of ETL.

I suspect people say ETL when they’re speaking professionally and they say munging when they’re complaining to a friend.


\section{Why are we focusing on this?}
\label{\detokenize{chapter-13-etl:why-are-we-focusing-on-this}}
\begin{sphinxadmonition}{note}{Big Picture \sphinxhyphen{} Munging/ETL is a large portion of data work}

Many well\sphinxhyphen{}respected people in the data science community estimate that 70\% to 80\% of a data scientist’s time can be spent on ETL rather than on the more interesting work of modeling, analysis, visualization, and communication.
\end{sphinxadmonition}

While many people hear those high percentages and can’t believe it, I suspect that by this point in our course, that doesn’t sound at all unreasonable to you.  Just last week, our in\sphinxhyphen{}class exercise was to merge two datasets, which takes only two or three lines of Python code.  But the amount of work necessary to prepare the datasets for a useful merge was far greater.

In \sphinxstyleemphasis{The Data Science Design Manual,} Steven Skeina has a useful chapter on ETL.  He has a humorous way of expressing the idea that ETL is a huge part of data work:
\begin{quote}

Most data scientists spend much of their time cleaning and formatting data. The rest spend most of their time complaining that there is no data available to do what they want to do.
\end{quote}

In other words, you can buckle down and do the munging you need to get the data you want, or you can sit around and get nowhere.  Those are the options.

Well, okay, there is a more pleasant option.  You can advance far enough in an organization that you have data workers under you in the org chart, and you make them do the ETL and hand you the results so that you can do the interesting stuff.  But you have to put in your time as a new hire before you can rise to directing others, and even then, you’ll still have to work closely with those you supervise to be sure that their munging gives you the kind of result you can use.

Now, the variety of things that fall under the ETL category is truly enormous.  The reason for this is that the purpose of munging is to take ugliness and clean it up, and there are so many different types of ugliness in the world.  \sphinxhref{https://vita.had.co.nz/papers/tidy-data.pdf}{When discussing tidy data}, Hadley Wickham quotes Tolstoy:
\begin{quote}

Happy families are all alike; every unhappy family is unhappy in its own way.
\end{quote}

Because every dataset is unhappy in its own way, your munging toolbelt can never be too big.  And so we can’t possibly cover it all in this chapter.  Experience with datasets is the best teacher, and I intend this course to give you many experiences with new datasets.  But we will cover some key topics.


\section{Data provenance}
\label{\detokenize{chapter-13-etl:data-provenance}}

\subsection{What is provenance?}
\label{\detokenize{chapter-13-etl:what-is-provenance}}
If you’ve ever watched \sphinxhref{https://www.pbs.org/wgbh/roadshow/}{Antiques Roadshow} (or walked in while one of your grandparents was watching it), you’ll know that the value of an item can be significantly impacted by its \sphinxstyleemphasis{provenance,} which means its history and origins.  If the appraiser can verify that a particular antique item was part of an important event or story in the past, or that the item is officially documented as being genuine, then this increases the item’s value.

The value of data is also significantly impacted by its history and origins.  If we know how the data was collected and can read about the details of that process, that will probably significantly increase its usefulness to us.

For instance, imagine you get a dataset in which some numeric columns are entitled \sphinxcode{\sphinxupquote{EQ50}}, \sphinxcode{\sphinxupquote{EQ51}}, \sphinxcode{\sphinxupquote{EQ52}}, and so on.  You would probably not be able to use the numbers in those columns for any purpose, because you don’t know what they mean.  But what if you find out that the data came from an economic survey that happened every quarter, and measured the GDP of various U.S. states during that quarter, in units of millions of dollars.  The organization that did the work referred to such measurements as “Economic Quarters” or EQs for short, and started with EQ1 in January 1987, counting upwards from there.  We can therefore figure out that EQ50 must refer to the second quarter of 2000, and so on.  Formerly useless data now has meaning and could be used.


\subsection{Data vs. information}
\label{\detokenize{chapter-13-etl:data-vs-information}}
\begin{sphinxadmonition}{note}{Big Picture \sphinxhyphen{} Information = Data + Context}

The difference between \sphinxstyleemphasis{data} and \sphinxstyleemphasis{information} is context.  Data is raw numbers, while information is having those numbers in a context we understand, so that the numbers have meaning.  Data provenance can be the context that turns data into information.
\end{sphinxadmonition}

To make sense of data, that is, to have information, not just data, requires knowing something about the domain in which the data lives.  \sphinxhref{chapter-12-concat-and-merge\#when-there-are-many-matches-for-some-rows}{One of the examples in the previous chapter} was about data from American football.  If you’re not familiar with that sport, it’s harder to understand the example, so I was careful to explain in the chapter the few necessary football concepts you’d need.  If your dataset comes from finance, you’ll be better equipped to turn that data into information if you know something about finance.  If you’re working with economic data, you’ll do better if you know economics.

This is where Bentley students have an advantage in data science over students from other universities.  While some schools have excellent technical educations and may cover more programming or machine learning skills than a data degree from Bentley does, every Bentley graduate has undergone an extensive training in business.  If you’re planning on applying your data skills in the business world, you’ll have a broader knowledge of that domain than most students from, say, an engineering school or a computer science degree.


\subsection{Data dictionaries}
\label{\detokenize{chapter-13-etl:data-dictionaries}}
Anyone producing a dataset should take care to distribute with it a data dictionary, which is a human\sphinxhyphen{}readable explanation in clear language of the meaning of each column in the dataset.  We’ve referred very often to the home mortgage dataset in these notes; it comes with an extensive data dictionary provided by the Consumer Financial Protecion Bureau, and you can \sphinxhref{https://ffiec.cfpb.gov/documentation/2018/lar-data-fields/}{see it online here}.  Since the average person doesn’t know what column names like “lei” or “hoepa\_status” or “aus\sphinxhyphen{}4” might mean, it’s essential to be able to look them up in a data dictionary.

If your employer puts you in charge of creating a dataset to be used by others, be sure that you always couple it with a document explaining the meaning of each column.  If you find a dataset you’d like to use in your own work (whether it comes from the web for your use in MA346 or it comes from your company’s intranet when you have an internship or job), one of the first questions you should ask is where the data dictionary is.  Otherwise, how will you know what the data means?

If a dataset doesn’t come from a data dictionary, but you have personal access to the source of the data (such as another team within your company), you can organize a meeting to ask them where the data comes from and what its columns mean.  Documenting the results of such a meeting and storing it with the data in a data dictionary make that dataset more useful to everyone thereafter (and save everyone from repeating the same meeting later).  I had a meeting of exactly this type with the nonprofit organization that partnered with my graduate data science class in Fall 2019 to discuss their datasets.


\section{Missing values}
\label{\detokenize{chapter-13-etl:missing-values}}
This can be one of the most confusing aspects of data work for new students of data science, so let me begin by emphasizing four key points about missing values that you should always keep in mind.

\begin{sphinxadmonition}{note}{Big Picture \sphinxhyphen{} Summary of key points about missing values}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Missing values are extremely common, and are sometimes inevitable.

\item {} 
Sometimes missing values indicate a mistake or a problem, and sometimes they don’t.

\item {} 
Replacing missing values with actual values is called \sphinxstyleemphasis{imputation,} and there are \sphinxstyleemphasis{many} different ways to do it.

\item {} 
Sometimes imputation is the right thing to do with missing values, but sometimes it is the wrong thing to do.

\end{enumerate}
\end{sphinxadmonition}

Let’s think through the details of these important points.


\subsection{Why missing values are everywhere}
\label{\detokenize{chapter-13-etl:why-missing-values-are-everywhere}}
Missing values can and do appear in almost every type of dataset.  In the home mortgage dataset, for instance, anyone who didn’t fully complete the application will have some parts of their record in the database missing.  When compiling a comprehensive record of millions of mortgage applications, we simply can’t expect that everyone filled out the application completely!  Missing values are inevitable.

Even if you imagine a much more reliable source of data than human beings, such as a robotic sensor that’s programmed to take weather readings every hour on the hour, things can still go wrong.  The sensor can fail and not collect data for a few hours until someone replaces it and reconnects it.  The people in charge of the experiment can accidentally delete or lose some data files.  The hard drive on which the data is stored can malfunction so that not all data can be recovered.  Missing values can happen anywhere.


\subsection{Are missing values bad?}
\label{\detokenize{chapter-13-etl:are-missing-values-bad}}
Sometimes missing values occur in a dataset because of a problem.  Consider the examples given in the previous paragraph.  A broken sensor that fails to report data for a few hours means that something went wrong, something we wish hadn’t happened, but now our data is incomplete because of that problem.  The missing values reflect that problem.

But sometimes missing values are inserted into a dataset intentionally, because the creator of the dataset wants to communicate that a certain piece of data is unavailable.  For instance, in my football dataset, if the Receiver column in the Plays table has some missing entries, that means that there was no receiver involved in the play.  The missing values are communicating something intentional, sensible, and correct.  \sphinxstyleemphasis{Missing values don’t always indicate a problem.}

Even in the example of the failed sensor, where the missing values indicate a problem, that doesn’t mean that they should be removed or filled in with actual values.  Those missing values are truthfully stating what data was collected and what data was not collected.  Altering them would mean that our dataset would no longer be telling the truth about its origins.  If you’re sworn in on the witness stand, and you’re asked who committed the robbery, and you honestly don’t know the answer, the truthful thing to do is to say that you don’t know!  Making up an answer is clearly a deceptive thing to do in that situation, and it is often a deceptive thing to do with data as well.  \sphinxstyleemphasis{Resist the urge to “solve” missing values by always filling them in.}  Sometimes they’re telling an important truth.

In fact, this is why NumPy has the built\sphinxhyphen{}in value \sphinxcode{\sphinxupquote{np.nan}}, Python has \sphinxcode{\sphinxupquote{None}}, R has \sphinxcode{\sphinxupquote{NA}}, and Julia has \sphinxcode{\sphinxupquote{missing}}.  These languages all recognize the legitimacy of missing values, and give you a way to express them when you need to.

Notice the connection between these issues and data provenance.  If we know where the data came from and how it was obtained, we might be able to make sense of the missing values, and they can have important meaning for us, even though they’re missing.


\subsection{Should I ever remove missing values?}
\label{\detokenize{chapter-13-etl:should-i-ever-remove-missing-values}}
\sphinxstylestrong{Example 1: Removing missing values}

Some circumstances demand that we remove missing values.  Consider the following (real) dataset of the number of home runs hit per game in each Major League Baseball World Series in the 1990s.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(} \PYG{p}{\PYGZob{}}
    \PYG{c+c1}{\PYGZsh{} This data was collected by hand from pages on baseball\PYGZhy{}reference.com.}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Year}\PYG{l+s+s2}{\PYGZdq{}} \PYG{p}{:} \PYG{p}{[} \PYG{l+m+mi}{1990}\PYG{p}{,} \PYG{l+m+mi}{1991}\PYG{p}{,} \PYG{l+m+mi}{1992}\PYG{p}{,} \PYG{l+m+mi}{1993}\PYG{p}{,} \PYG{l+m+mi}{1994}\PYG{p}{,} \PYG{l+m+mi}{1995}\PYG{p}{,} \PYG{l+m+mi}{1996}\PYG{p}{,} \PYG{l+m+mi}{1997}\PYG{p}{,} \PYG{l+m+mi}{1998}\PYG{p}{,} \PYG{l+m+mi}{1999} \PYG{p}{]}\PYG{p}{,}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{HR}\PYG{l+s+s2}{\PYGZdq{}} \PYG{p}{:} \PYG{p}{[} \PYG{l+m+mi}{6}\PYG{p}{,} \PYG{l+m+mi}{16}\PYG{p}{,} \PYG{l+m+mi}{9}\PYG{p}{,} \PYG{l+m+mi}{13}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{nan}\PYG{p}{,} \PYG{l+m+mi}{13}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{,} \PYG{l+m+mi}{15}\PYG{p}{,} \PYG{l+m+mi}{9}\PYG{p}{,} \PYG{l+m+mi}{6} \PYG{p}{]}\PYG{p}{,}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZsh{}Games}\PYG{l+s+s2}{\PYGZdq{}} \PYG{p}{:} \PYG{p}{[} \PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{7}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{nan}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{,} \PYG{l+m+mi}{7}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{4} \PYG{p}{]}
\PYG{p}{\PYGZcb{}} \PYG{p}{)}
\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{HR/Game}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{HR}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{/} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZsh{}Games}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
   Year    HR  \PYGZsh{}Games   HR/Game
0  1990   6.0     4.0  1.500000
1  1991  16.0     7.0  2.285714
2  1992   9.0     6.0  1.500000
3  1993  13.0     6.0  2.166667
4  1994   NaN     NaN       NaN
5  1995  13.0     6.0  2.166667
6  1996   6.0     6.0  1.000000
7  1997  15.0     7.0  2.142857
8  1998   9.0     4.0  2.250000
9  1999   6.0     4.0  1.500000
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Year}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{HR/Game}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Home Runs per Games in World Series}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xticks}\PYG{p}{(} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Year}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Year}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{HR/Game}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylim}\PYG{p}{(} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mf}{2.5} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{chapter-13-etl_5_0}.png}

Assume you were trying to show that this number was not going convincingly up or down throughout the 1990s (a made\sphinxhyphen{}up research question just as an example).  You’re considering fitting a linear model to the data and showing that its slope is close to zero (perhaps even not statistically significantly different from zero).  Let’s try.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{stats} \PYG{k}{as} \PYG{n+nn}{stats}
\PYG{n}{stats}\PYG{o}{.}\PYG{n}{linregress}\PYG{p}{(} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Year}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{HR/Game}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
LinregressResult(slope=nan, intercept=nan, rvalue=nan, pvalue=nan, stderr=nan)
\end{sphinxVerbatim}

This has clearly failed, giving us all missing values in our linear model.  The reason, no doubt, is the missing value in our data.  (There was no World Series in 1994 due to a players’ strike.)

So in this case, the missing values are clearly causing a problem with what we want to do with the data.  And since we can fit a linear model to the data that remains, it would be perfectly acceptable to drop the one row that has missing values and proceed with the nine rows that remain.  \sphinxstyleemphasis{This is a case in which removing the missing values makes sense.}

But we do not remove them from the original dataset; we simply don’t include them in the data used to create the linear model.  The original dataset stays intact.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df\PYGZus{}without\PYGZus{}94} \PYG{o}{=} \PYG{n}{df}\PYG{o}{.}\PYG{n}{dropna}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{stats}\PYG{o}{.}\PYG{n}{linregress}\PYG{p}{(} \PYG{n}{df\PYGZus{}without\PYGZus{}94}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Year}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{df\PYGZus{}without\PYGZus{}94}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{HR/Game}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
LinregressResult(slope=\PYGZhy{}0.0012387387387387322, intercept=4.305389317889305, rvalue=\PYGZhy{}0.0085545966495754, pvalue=0.9825737815654227, stderr=0.054728717410681665)
\end{sphinxVerbatim}

Now we have an actual linear model.  (We’re not going to analyze it here; that wasn’t the point of this example.)

\sphinxstylestrong{Example 2:  Not removing missing values}

Let’s say you’re working for a small\sphinxhyphen{}but\sphinxhyphen{}growing automobile sales organization.  They’ve just opened their second location and they’ve realized that their growth has far outpaced their record\sphinxhyphen{}keeping.  They’ve got some spreadsheets about sales and commissions for their various employees, but it’s not comprehensive because they haven’t been organized about record\sphinxhyphen{}keeping in the past.  They’ve asked you to organize it into a database.

Let’s say you realize the data isn’t that huge, so you can probably fit it in one spreadsheet.  You begin by creating a private Google Sheet and sharing the link with all the sales managers, asking them to paste in all the historic data on which they have records, to create a shared dataset that’s as comprehensive as possible.  You start with columns for month, employee, manager, number of sales, commission, and others.  When the task is done, you notice that many rows have missing values for the number of sales and commission columns.  The managers knew the employees were working there that month, but they’d lost the relevant historical data in the intervening years.

If you were to remove those rows from the dataset, it could make it seem as if the employee was not a part of the company or team at the time.  Thus even though those rows contain missing values, they are still communicating other important information.  In this case, you would decide not to remove the rows, even though they won’t contribute much to any later analysis.

Any decision like this made when constructing a dataset should be documented in its data dictionary.

\sphinxstylestrong{Example 3: Actually adding missing values}

In the home mortgage dataset with which we’re very familiar, some columns (such as interest rate) contain mostly numerical data, but occasionally the word Exempt in place of a number.  This makes it impossible to do any computations on such columns, such as \sphinxcode{\sphinxupquote{df{[}'interest\_rate'{]}.mean()}}, because the column is text, not numeric.

In this case, it can be valuable to replace the word Exempt with the actual missing value \sphinxcode{\sphinxupquote{np.nan}} throughout the column, so that it can then be converted to type \sphinxcode{\sphinxupquote{float}}.  In doing so, you should carefully document that all Exempt entries have become missing values, in order to facilitate analysis.  \sphinxstyleemphasis{This is a situation in which missing values are actually intentionally added!}

If you needed to track which rows had originally been Exempt, you could retain the original interest rate column for reference, creating a new one as you do the replacement.  Alternately, you could create a new column that records simply a single boolean value for “interest rate exempt” so that you can tell missing values from Exempt values.

Elsewhere in the same mortgage dataset, we find cases in which numbers like 999 were used for applicants’ ages.  Clearly these are not correct values, and should be treated as a lack of data, rather than legitimate data.  Consider the alternatives for how to handle them:


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\sphinxstyletheadfamily 
If we leave numbers like 999 in the data
&\sphinxstyletheadfamily 
If we replace them with missing values
\\
\hline
Statistics about age, like mean, median, etc., will be very wrong
&
Statistics about age will be much more accurate
\\
\hline
The number of missing values in the dataset will be very small
&
The number of missing values in the data will be much more accurate
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}


\subsection{When I need to remove missing values, how do I?}
\label{\detokenize{chapter-13-etl:when-i-need-to-remove-missing-values-how-do-i}}
Removing missing values is called \sphinxstyleemphasis{data imputation,} which is simply the technical word for filling in values where there were none.  Imputation is an enormous area of statistics to which we cannot do justice in this chapter, but let’s see why sometimes imputing values is essential.

In the baseball example above, we saw that some model\sphinxhyphen{}fitting procedures can’t work with missing values, and we need to remove them from consideration.  Now let’s assume we were fitting some (more complex) model to the property values in the mortgage dataset.  If we need to drop any row in which the property value is missing, how might that cause problems?

The question takes us right back to data provenance:  \sphinxstyleemphasis{Why} is the property value missing on the mortgage application?  Let’s say we investigate and find that this is usually because the application was not completed by the potential borrower.  The question then arises: Are all borrowers equally likely to quit an application half way through?  If they are, then perhaps dropping such rows from the data is an acceptable move.

But the government publishes the data to help combat discrimination in lending.  What if we were to look at the proportion of incomplete applications and find that it’s much higher for certain ethnic groups, especially in certain locations?  Perhaps they’re not completing the application because they’re facing discrimination in the process and don’t have the energy or ability to fight it.  If that’s the case, then dropping rows with missing property values will \sphinxstyleemphasis{significantly reduce the representation of those ethnic groups in our data.}  Our model will unintentionally favor the other ethnic groups.  Not only will it make bad predictions (so we’ve done our data work wrong) but it will help to further the discrimination the dataset was trying to prevent (so we’ve made an ethical mistake as well)!

So if we find that the missing values are not spread evenly across groups within our data, we can’t in good conscience drop those rows.  Instead, we have to find some way to insert realistic or feasible values in place of the missing values.  Here are a few common ways to do so:
\begin{itemize}
\item {} 
\sphinxstylestrong{Mean substitution} \sphinxhyphen{} Replace each missing property value with the mean property value across all rows.

\item {} 
\sphinxstylestrong{Model\sphinxhyphen{}based substitution} \sphinxhyphen{} Create a simple model that predicts property values based on other things, such as zip code, and use it to fill in each missing value.

\item {} 
\sphinxstylestrong{Random imputation} \sphinxhyphen{} Replace each missing property value with a randomly chosen property value from elsewhere in the dataset, or randomly chosen from other similar records (e.g., in the same state, or the same race, or the same income bracket, etc.).

\end{itemize}

Again, many statistical concerns arise when doing imputation that we cannot cover in this short chapter of notes.  This is merely an introduction to the fact that this practice is an important one.


\section{All the other munging things}
\label{\detokenize{chapter-13-etl:all-the-other-munging-things}}
As I said at the outset, it’s not possible to cover everything you might need to do with data.  But here are a few essentials to keep in mind.

When using data, keep in mind the units on every number, in terms as precise as you possibly can.  You can insert these units as comments in your code.  There are famous stories of \sphinxhref{https://en.wikipedia.org/wiki/Mars\_Climate\_Orbiter\#Cause\_of\_failure}{tens of millions of dollars lost in spacecraft} when units were not checked correctly in computer code, so these tiny details are not unimportant!

In \sphinxstyleemphasis{The Data Science Design Manual} quoted earlier, the author suggests several types of unit discrepencies to pay attention to.
\begin{itemize}
\item {} 
differing standards of measurement, such as pounds vs. kilograms, or USD vs. GBP

\item {} 
the time value of money, such as USD in January 2017 vs. USD in February 2017

\item {} 
fluctuations in value, such as the price of gold at noon today vs. at 1pm today

\item {} 
discrepencies in time zones, such as the price of gold at noon today in London vs. noon today in New York

\item {} 
discrepencies in the units themselves, such as “shares of stock” before and after a stock split

\end{itemize}

Another common units error to be aware of is the difference between percentages and proportions.  For instance, 15\% is equal to the proportion 0.15.  When reporting such a value to a human reader, such as in a table of results, the percent is typically the more user\sphinxhyphen{}friendly choice.  When using such a value in a computation, such as multiplying to apply a percentage or proportion to a total quantity, the only correct choice is the proportion.  That is, 15\% of 200 people is not \(15\times200=3000\), but \(0.15\times200=30\).

Comments in code to track units can help with discrepencies like these.  See the code below that takes care with units as we adjust movie revenues for inflation in the following dataset.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df\PYGZus{}films} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(} \PYG{p}{\PYGZob{}}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Title}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{:} \PYG{p}{[} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Avengers: Endgame}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{The Lion King}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{The Hunger Games}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Finding Dory}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{]}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Year}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{:} \PYG{p}{[} \PYG{l+m+mi}{2019}\PYG{p}{,} \PYG{l+m+mi}{2019}\PYG{p}{,} \PYG{l+m+mi}{2012}\PYG{p}{,} \PYG{l+m+mi}{2016} \PYG{p}{]}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Opening Weekend (M\PYGZdl{})}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{:} \PYG{p}{[} \PYG{l+m+mf}{357.115}\PYG{p}{,} \PYG{l+m+mf}{191.771}\PYG{p}{,} \PYG{l+m+mf}{152.536}\PYG{p}{,} \PYG{l+m+mf}{135.060} \PYG{p}{]}
\PYG{p}{\PYGZcb{}} \PYG{p}{)}
\PYG{n}{df\PYGZus{}films}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
               Title  Year  Opening Weekend (M\PYGZdl{})
0  Avengers: Endgame  2019               357.115
1      The Lion King  2019               191.771
2   The Hunger Games  2012               152.536
3       Finding Dory  2016               135.060
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{avg\PYGZus{}annual\PYGZus{}inflation} \PYG{o}{=} \PYG{l+m+mi}{3}                                \PYG{c+c1}{\PYGZsh{} An approximate percentage}
\PYG{n}{inflation\PYGZus{}factor} \PYG{o}{=} \PYG{l+m+mi}{1} \PYG{o}{+} \PYG{n}{avg\PYGZus{}annual\PYGZus{}inflation}\PYG{o}{/}\PYG{l+m+mi}{100}         \PYG{c+c1}{\PYGZsh{} Useful as an annual multiplier}
\PYG{n}{df\PYGZus{}films}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Years since film}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mi}{2020} \PYG{o}{\PYGZhy{}} \PYG{n}{df\PYGZus{}films}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Year}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}  \PYG{c+c1}{\PYGZsh{} Number of years elapsed}
\PYG{n}{df\PYGZus{}films}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Inflation factor}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{inflation\PYGZus{}factor} \PYG{o}{*}\PYG{o}{*} \PYG{n}{df\PYGZus{}films}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Years since film}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
                                                        \PYG{c+c1}{\PYGZsh{} Multiplier to apply inflation}
\PYG{n}{df\PYGZus{}films}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Opening Weekend (M\PYGZdl{}2020)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{df\PYGZus{}films}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Opening Weekend (M\PYGZdl{})}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYGZbs{}
    \PYG{o}{*} \PYG{n}{df\PYGZus{}films}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Inflation factor}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}                      \PYG{c+c1}{\PYGZsh{} Converted to today\PYGZsq{}s dollars}
\PYG{n}{df\PYGZus{}films}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
               Title  Year  Opening Weekend (M\PYGZdl{})  Years since film  \PYGZbs{}
0  Avengers: Endgame  2019               357.115                 1   
1      The Lion King  2019               191.771                 1   
2   The Hunger Games  2012               152.536                 8   
3       Finding Dory  2016               135.060                 4   

   Inflation factor  Opening Weekend (M\PYGZdl{}2020)  
0          1.030000                367.828450  
1          1.030000                197.524130  
2          1.266770                193.228041  
3          1.125509                152.011220  
\end{sphinxVerbatim}

Before we finish discussing ETL, we should talk about file formats, which are a crucial part of the whole process.


\section{Reading data files}
\label{\detokenize{chapter-13-etl:reading-data-files}}
As you know from \sphinxhref{big-cheat-sheet\#before-week-8}{the DataCamp assignment that corresponds to this chapter}, there are many ways to read data into pandas.  Since you’ve learned some of the technical details from DataCamp, let’s look at the relative pros and cons of each file format here, and add a few pieces of advice that didn’t appear in the DataCamp lessons.  We start with the easiest file formats and work our way up.


\subsection{Easy formats to read: CSV and TSV}
\label{\detokenize{chapter-13-etl:easy-formats-to-read-csv-and-tsv}}
We’ve been using \sphinxcode{\sphinxupquote{pd.read\_csv()}} for ages, so there is no surprise here, and you’ve had to deal with its \sphinxcode{\sphinxupquote{encoding}} parameter in the past as well.  It has tons of optional parameters, but the one introduced in the latest DataCamp lessons was \sphinxcode{\sphinxupquote{sep}}, useful for reading TSV (tab\sphinxhyphen{}separated values) files, by choosing \sphinxcode{\sphinxupquote{sep="\textbackslash{}t"}}.

One piece of advice to add to DataCamp:  If you find the URL of a CSV file on the web, you can include that URL as the input parameter to \sphinxcode{\sphinxupquote{pd.read\_csv()}}, and it will download and read the file for you in one shot, without your having to manually download the file.
\begin{itemize}
\item {} 
Pro: It automatically gets the latest version of the file every time you run your code.

\item {} 
Con: It accesses the Internet (which can sometimes be slow) every time you run your code.

\item {} 
Con: If the file is removed from the web, your code no longer functions.

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Providing a URL directly to pd.read\PYGZus{}csv():}
\PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://www1.ncdc.noaa.gov/pub/data/cdo/samples/PRECIP\PYGZus{}HLY\PYGZus{}sample\PYGZus{}csv.csv}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
       STATION     STATION\PYGZus{}NAME  ELEVATION  LATITUDE  LONGITUDE  \PYGZbs{}
0  COOP:310301  ASHEVILLE NC US      682.1   35.5954   \PYGZhy{}82.5568   
1  COOP:310301  ASHEVILLE NC US      682.1   35.5954   \PYGZhy{}82.5568   
2  COOP:310301  ASHEVILLE NC US      682.1   35.5954   \PYGZhy{}82.5568   

             DATE   HPCP Measurement Flag Quality Flag  
0  20100101 00:00  99999                ]               
1  20100101 01:00      0                g               
2  20100102 06:00      1                                
\end{sphinxVerbatim}


\subsection{Pretty easy format to read: XLSX}
\label{\detokenize{chapter-13-etl:pretty-easy-format-to-read-xlsx}}
The \sphinxcode{\sphinxupquote{pd.read\_excel()}} function is nearly as easy to use as \sphinxcode{\sphinxupquote{pd.read\_csv()}}, with a few exceptions documented below.  You can give this function a URL also, if there’s a publicly accessible Excel file on the web you want to download.  The same pros and cons apply when providing a URL to \sphinxcode{\sphinxupquote{pd.read\_excel()}} as they do for \sphinxcode{\sphinxupquote{pd.read\_csv()}}, as discussed above.
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
If you’re running Python on a cloud service, you’ll need the \sphinxcode{\sphinxupquote{xlrd}} module to be installed to add Excel support to pandas.  (You can tell if it’s not when a \sphinxcode{\sphinxupquote{pd.read\_excel()}} call fails with an error about the missing module.)  If you’re on your local computer with an Anaconda installation, you already have this module.  Otherwise, you need to run \sphinxcode{\sphinxupquote{pip install xlrd}} to add it.

\item {} 
You need to remember that this function returns a Python list of DataFrames, unless you choose one specific sheet, with \sphinxcode{\sphinxupquote{sheet\_name='Name'}} or choose one by index, with \sphinxcode{\sphinxupquote{sheet\_name=0}}, for example.

\item {} 
Excel spreadsheets may not have the data in the top left, so parameters like \sphinxcode{\sphinxupquote{usecols}} and \sphinxcode{\sphinxupquote{skiprows}} are often needed.

\end{enumerate}


\subsection{Easy format to read with occasional problems: HTML}
\label{\detokenize{chapter-13-etl:easy-format-to-read-with-occasional-problems-html}}
Pandas can often automatically extract tables from web pages.  Simply call \sphinxcode{\sphinxupquote{pd.read\_html()}} and give it the URL of the page containing the table or tables.  It has the same output type as \sphinxcode{\sphinxupquote{pd.read\_excel()}} does: a Python list of pandas DataFrames.  See the pros and cons listed under \sphinxcode{\sphinxupquote{pd.read\_csv()}} for providing live web URLs when reading data.

Furthermore, depending on the quality of the web site, this function may or may not do its job.  If the HTML page is not structured particularly cleanly, I’ve had \sphinxcode{\sphinxupquote{pd.read\_html()}} fail to find one or more of the tables.  I’ve had to instead write code that downloads the HTML code, splits it wherever a \sphinxcode{\sphinxupquote{<table...>}} tag begins, and extract the tables from those pieces with \sphinxcode{\sphinxupquote{pd.read\_html()}}.  This is annoying, but occasionally necessary.

Note that if you don’t need to get live data from the web, but are content with downloading the data once at the start of your project, there are many ways to extract tables from web pages.  You can often select the table and copy\sphinxhyphen{}paste into Excel, although that sometimes brings along undesired formatting that can cause problems.  There are Google Chrome extensions that specialize in extracting tables from web pages to make them easier to paste cleanly into Excel.


\subsection{Not an easy format to read: JSON}
\label{\detokenize{chapter-13-etl:not-an-easy-format-to-read-json}}
Although this format is not easy, it is powerful, and this is why it’s very prevalent on the web.  It can represent a huge variety of different types of data, not just tabular data.  It is flexible enough to represent tabular data in a variety of ways, but also hierarchical data of any kind.  Due to its complexity, we will not fully review this here; refer to \sphinxhref{big-cheat-sheet\#chapter-4-importing-json-data-and-working-with-apis}{the appropriate section of our course’s coding cheat sheet} for some information, or \sphinxhref{https://learn.datacamp.com/courses/streamlined-data-ingestion-with-pandas}{the corresponding DataCamp course}.


\subsection{Not an easy source to read: SQL}
\label{\detokenize{chapter-13-etl:not-an-easy-source-to-read-sql}}
Rather than dive into the enormous topic of SQL databases here, I will suggest two ways that you can learn more:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Your next (and final) DataCamp assignment, for next week, will do some introductory coverage of this content.

\item {} 
Bentley has an entire course on SQL databases, CS350, which I recommend.

\end{enumerate}

Now let’s consider which file format to use when you need to create a file rather than read one.


\section{Writing data files}
\label{\detokenize{chapter-13-etl:writing-data-files}}
As with all types of communication, it’s essential to consider your audience when choosing a file type.  Who will use your file?


\subsection{For a nontechnical audience, create an Excel file.}
\label{\detokenize{chapter-13-etl:for-a-nontechnical-audience-create-an-excel-file}}
If sharing your data with non\sphinxhyphen{}technical people, they will want to simply double\sphinxhyphen{}click the file and see its contents.  The easiest way to ensure this happens is to create an Excel file.  (To make it even easier, you can upload the file to SharePoint or Google Sheets and send only the link.  This is especially valuable if you suspect the recipient doesn’t have Excel installed.)

Just as when reading Excel files, you must have the \sphinxcode{\sphinxupquote{xlrd}} module installed; {\hyperref[\detokenize{chapter-13-etl:pretty-easy-format-to-read-xlsx}]{\emph{see above for details}}}.  If you want to create an Excel file with just one sheet in it, you can make a single call to \sphinxcode{\sphinxupquote{df.to\_excel()}}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df\PYGZus{}films}\PYG{o}{.}\PYG{n}{to\PYGZus{}excel}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Opening Weekends.xlsx}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\end{sphinxVerbatim}

If you want to put several DataFrames into one Excel file, as different sheets in the workbook, then you need to get a little more fancy.  The indentation in the following code is essential (as always with Python).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{with} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{ExcelWriter}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Two Things.xlsx}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)} \PYG{k}{as} \PYG{n}{writer}\PYG{p}{:}        \PYG{c+c1}{\PYGZsh{} Open the file.}
    \PYG{n}{df}\PYG{o}{.}\PYG{n}{to\PYGZus{}excel}\PYG{p}{(} \PYG{n}{writer}\PYG{p}{,} \PYG{n}{sheet\PYGZus{}name}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{World Series Data}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Write one sheet.}
    \PYG{n}{df\PYGZus{}films}\PYG{o}{.}\PYG{n}{to\PYGZus{}excel}\PYG{p}{(} \PYG{n}{writer}\PYG{p}{,} \PYG{n}{sheet\PYGZus{}name}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Film Data}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}    \PYG{c+c1}{\PYGZsh{} Write the other.}
\end{sphinxVerbatim}

Python’s \sphinxcode{\sphinxupquote{with}} statement lets you create a resource (in this case a new, open file) and Python will automatically close it up for you when you’re done using it.  At the end of the two indented lines, Python will close the file, so that other applications can open it.

For more details, see \sphinxhref{https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to\_excel.html}{the documentation for \sphinxcode{\sphinxupquote{df.to\_excel()}}}.


\subsection{For a technical audience, usually use a CSV file.}
\label{\detokenize{chapter-13-etl:for-a-technical-audience-usually-use-a-csv-file}}
If sharing data with other data workers, who are likely to use Python, R, or some similarly nerdy tool, you probably want to create a CSV file.  The reason is simple: You know that this is one of the easiest file types to import in your code, so make life easy for your coworkers, too.  Just call \sphinxcode{\sphinxupquote{pd.to\_csv( 'my\sphinxhyphen{}filename.csv' )}} to save your DataFrame.  Although you can use the \sphinxcode{\sphinxupquote{sep="\textbackslash{}t"}} parameter to create a TSV file, this is rarely what your coworkers want, so it’s to be generally avoided.

But note that you can lose a lot of important information this way!  You may be familiar with how Excel complains if you try to save an Excel workbook in CSV format, letting you know that you’re losing information, such as formatting and formulas.  Any information in your DataFrame other than the text contents of the cells will be lost when saving as CSV.  For instance, if you’ve converted a column to a categorial variable, that won’t be obvious when the data is saved to CSV, and it will be re\sphinxhyphen{}imported as plain text.

For that reason, we have the following option.


\subsection{For archiving your own work, use a Pickle file.}
\label{\detokenize{chapter-13-etl:for-archiving-your-own-work-use-a-pickle-file}}
Python has always had a way to store any Python object in a file, perfectly intact for later loading, using the Pickle format.  The standard extension for this is \sphinxcode{\sphinxupquote{.pkl}}.  (That’s P\sphinxhyphen{}K\sphinxhyphen{}L, not P\sphinxhyphen{}K\sphinxhyphen{}one, because it’s short for PicKLe.)  The name comes, of course, from the fact that pickling vegetables stores them on the shelf long\sphinxhyphen{}term, and yet when you eventually open them later, they’re fine.  Similarly, you can store Python objects in a file long\sphinxhyphen{}term, open them later, and they’re fine.

Because Python guarantees that any object you pickle to a file will come back from that file in exactly the same form, you can pickle entire DataFrames and know that every little detail will be preserved, even things that won’t get saved correctly to CSV or Excel files, like categorical data types.

This is a great way to obey the advice \sphinxhref{chapter-11-processing-rows\#when-the-bottleneck-is-the-dataset}{at the end of the Chapter 11 notes}.  If you load a big dataset and do a bunch of data cleaning work, and your code is a little slow to run, just save your work to a file right then.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{o}{.}\PYG{n}{to\PYGZus{}pickle}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{cleaned\PYGZhy{}dataset.pkl}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\end{sphinxVerbatim}

Then start a new Python script or Jupyter notebook and load the DataFrame you just saved.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}pickle}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{saved\PYGZhy{}for\PYGZhy{}later.pkl}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\end{sphinxVerbatim}

Now do all your analysis work in that second script or notebook, and whenever you have to re\sphinxhyphen{}run your analysis from the beginning, you won’t have to wait for all the data cleaning code to get run again.

We won’t discuss in these notes the creation of HTML or JSON files from Python.  Although there is occasional value in it, it’s much less commonly useful than reading those formats, which we covered above.  We also won’t discuss the creation of SQL databases, but here are three ways you can learn more about that.
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
SQL queries can be used to create or modify tables, and we’ll see a bit about running SQL queries through Python in the upcoming DataCamp homework.

\item {} 
Those interested in learning SQL deeply can take Bentley’s CS350 course.

\item {} 
Consider forming a team for one of the Learning On Your Own activities shown below.

\end{enumerate}

\begin{sphinxadmonition}{note}{Learning on Your Own \sphinxhyphen{} SQL in Jupyter}

Look up the \sphinxcode{\sphinxupquote{ipython\sphinxhyphen{}sql}} extension to Jupyter and research the following essential parts, then reporting on them in some sensible medium for your classmates.
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
How to install it and load it.

\item {} 
How to connect to a database.

\item {} 
How to use both \sphinxcode{\sphinxupquote{\%sql}} and \sphinxcode{\sphinxupquote{\%\%sql}} commands.

\item {} 
How to store the results of database queries in pandas DataFrames.

\end{enumerate}
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Learning on Your Own \sphinxhyphen{} SQLite in Python}

Python actually comes with a built\sphinxhyphen{}in SQLite database module, which you can use by doing \sphinxcode{\sphinxupquote{import sqlite3 as sl}}, without even any installation step.  Check out \sphinxhref{https://towardsdatascience.com/do-you-know-python-has-a-built-in-database-d553989c87bd}{this blog post} for more information, and report on its key features to the class.
\end{sphinxadmonition}


\chapter{Dashboards}
\label{\detokenize{chapter-14-dashboards:dashboards}}\label{\detokenize{chapter-14-dashboards::doc}}
See also the slides that summarize a portion of this content.


\section{What’s a dashboard and why do we have them?}
\label{\detokenize{chapter-14-dashboards:what-s-a-dashboard-and-why-do-we-have-them}}
You’ve been learning a lot of data manipulation and analysis in Python.  But Python is an environment that only data professionals and scientists tend to dive into.  What happens when you want to let non\sphinxhyphen{}technical people browse your work?

Most of the time, we write reports or create slide decks to share our results.  But sometimes the experience of exploring the data is more powerful than a static report or pre\sphinxhyphen{}packaged slide deck can ever be.  Sometimes the manager who asked for the analysis wants to experiment with various parameter values themself, especially if they were a data analyst once, too.

This is where \sphinxstyleemphasis{dashboards} come in.  A quick \sphinxhref{https://www.google.com/search?q=data+dashboard\&tbm=isch}{Google image search for “data dashboards”} will show you dozens of examples of what dashboards look like.  Their purpose is to let the user explore the data using inputs like buttons and sliders, and seeing outputs that are typically data visualizations and summaries.  Dashboards don’t give the user anywhere near as much flexibility as you have in Python, but they’re much easier and faster.

\begin{sphinxadmonition}{note}{Big Picture \sphinxhyphen{} Uses for data dashboards}

There are many reasons why you might prepare a data dashboard.  Here are a few.
\begin{itemize}
\item {} 
\sphinxstylestrong{There are many different inputs to which you could apply an analysis, and you want to let the user explore each.}  For instance, you recently built a visualization for comparing property values in home mortgage applications across two races.  But it could be more powerful if we let the user choose two races, and the analysis would automatically be repeated for those two.  The user could choose values that matter to them personally or professionally.

\item {} 
\sphinxstylestrong{An analysis has a tuning parameter that might benefit from exploration by an expert.}  For instance, let’s say you have a model that takes as input a price for a new insurance product, and forecasts adoption rates and various probabilities associated with profits and losses under various conditions.  The person ultimately in charge of making the decision on product price might like to move a slider that controls the price input, and take their time to consider each of the possible scenarios in your model’s output.

\item {} 
\sphinxstylestrong{Some projects are not a data analysis, but just a data showcase.}  I mentioned in a previous class that a friend of mine runs a nonprofit that helps universities make, track, and keep climate commitments.  \sphinxhref{https://reporting.secondnature.org/}{Their data dashboard is here.}  It doesn’t do any analysis, but makes their data transparent and interactive, for anyone to explore for their own purposes.

\item {} 
\sphinxstylestrong{Another team wants to see what you’re doing, but they don’t want to read your code.}  To quickly share what you’ve been working on without forcing the recipient to dive into all of your Python code, you can wrap your work in a dashboard and share it on the web.  This lets you get feedback from other teams in your organization about your team’s work.

\end{itemize}
\end{sphinxadmonition}

There are many tools for creating dashboards.  One of the most popular is \sphinxhref{https://www.tableau.com/trial/dashboards}{Tableau}, but we are not studying it in this course for two reasons.  First, it is proprietary software, which makes it less transferable knowledge than free tools.  Second, it is much easier to learn Tableau later on your own than it is to learn Python.  There are many Tableau training opportunities available online and in the corporate world should you need them.

There are also Python\sphinxhyphen{}specific frameworks for creating dashboards.  The easiest one I’ve found to get started with is what we will learn today, \sphinxhref{https://www.streamlit.io/}{Streamlit}.  But a few others are mentioned at the end of this chapter for you to explore on your own if you desire.  These course notes assume that you have already done the following three things.  They’re necessary in order to follow along with the content here.
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Installed Streamlit (\sphinxcode{\sphinxupquote{pip install streamlit}})

\item {} 
\sphinxhref{https://signup.heroku.com/}{Registered} for a Heroku account

\item {} 
Installed the Heroku \sphinxhref{https://devcenter.heroku.com/articles/getting-started-with-python?singlepage=true\#set-up}{command\sphinxhyphen{}line interface}

\end{enumerate}


\section{Our running example}
\label{\detokenize{chapter-14-dashboards:our-running-example}}
In this section, I will do a small data visualization that we will turn into a dashboard throughout the rest of these course notes, so that you can see an example of how to convert existing code into a dashboard.  The small amount of code (and explanation of that code) that we will convert into a dashboard appears between the two horizontal lines below.


\bigskip\hrule\bigskip



\subsection{Example begins here}
\label{\detokenize{chapter-14-dashboards:example-begins-here}}
In statistics, the Central Limit Theorem (CLT) says that if have several random variables, and we define a new random variable to be their sum, then that new random variable has a shape vaguely like a normal distribution.  Furthermore, if you increase the number of random variables in the sum, then that sum becomes even more precisely like a normal distribution.  Let’s see this in action with some Python simulations.

First, we’ll need NumPy to generate random numbers for us, and we’ll use the generic uniform distribution for this simulation.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(} \PYG{l+m+mi}{10} \PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Make sure we can generate 10 random values in [0,1]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
array([0.02842214, 0.81119674, 0.53157605, 0.86342282, 0.93218777,
       0.42328714, 0.34510009, 0.08866329, 0.23075973, 0.34200095])
\end{sphinxVerbatim}

The CLT says that we can make a new random variable by summing those numbers.  Let’s do so.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{my\PYGZus{}random\PYGZus{}variable} \PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(} \PYG{l+m+mi}{10} \PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}

\PYG{n}{my\PYGZus{}random\PYGZus{}variable}\PYG{p}{(}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Test it}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
4.86366937905601
\end{sphinxVerbatim}

That random variable is supposed to look sort of like a bell\sphinxhyphen{}shaped curve.  Let’s sample 1000 values from it and make a histogram to see if that’s true.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{n}{sample} \PYG{o}{=} \PYG{p}{[} \PYG{n}{my\PYGZus{}random\PYGZus{}variable}\PYG{p}{(}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1000}\PYG{p}{)} \PYG{p}{]}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{hist}\PYG{p}{(} \PYG{n}{sample}\PYG{p}{,} \PYG{n}{bins}\PYG{o}{=}\PYG{l+m+mi}{30} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{chapter-14-dashboards_7_0}.png}

Yes, that looks like a bell curve!  Its mean and standard deviation are as follows.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(} \PYG{n}{sample} \PYG{p}{)}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{std}\PYG{p}{(} \PYG{n}{sample} \PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(4.989276203690257, 0.9204788579003654)
\end{sphinxVerbatim}


\bigskip\hrule\bigskip


That’s it.  Between those two horizontal lines is a little statistics experiment that we will want to turn into a dashboard.  In doing so, we’ll make it much more interactive than it is when it’s sitting, pre\sphinxhyphen{}computed, on a web page or PDF of these course notes.


\section{Step 1: We need a Python script}
\label{\detokenize{chapter-14-dashboards:step-1-we-need-a-python-script}}
In this course, I don’t make any restrictions on whether you program in Python scripts (\sphinxcode{\sphinxupquote{.py}}) or Jupyter notebooks (\sphinxcode{\sphinxupquote{.ipynb}}).  But Streamlit is an exception; it forces us to use Python scripts.  If you’re already in the habit of doing all your data work in Python scripts, then you can skim the rest of this section and pick up in Step 2.  But if you’re usually a Jupyter user, the good news is that you can convert a notebook into a Python script in just a few clicks, using Jupyter itself.
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
From the File menu, choose Export Notebook As…, and choose Export Notebook to Executable Script.

\item {} 
This will download the result as a Python script to your computer.  The browser may warn you that it’s dangerous to download and run Python scripts.  Although that’s true in general, if you wrote the script, it should be safe for you to download!

\item {} 
The file will probably end up in your downloads folder, and you’ll want to move it from there to wherever you keep your course work.

\end{enumerate}

If you’re a Jupyter user, you can still edit the Python script using Jupyter.  In addition to letting you edit notebooks, Jupyter supports editing of Python files and many other file types.

If I take the example shown above and run this process on it, I get the following Python script.  Notice how Jupyter turns all the Markdown content of my notebook into Python comments and marks each cell as a numbered input (\sphinxcode{\sphinxupquote{In{[}1{]}}}, \sphinxcode{\sphinxupquote{In{[}2{]}}}, etc.).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} \PYGZsh{}\PYGZsh{}\PYGZsh{} Example begins here}
\PYG{c+c1}{\PYGZsh{}}
\PYG{c+c1}{\PYGZsh{} In statistics, the Central Limit Theorem (CLT) says that if have several random variables, and we define a new random variable to be their sum, then that new random variable has a shape vaguely like a normal distribution.  Furthermore, if you increase the number of random variables in the sum, then that sum becomes even more precisely like a normal distribution.  Let\PYGZsq{}s see this in action with some Python simulations.}
\PYG{c+c1}{\PYGZsh{}}
\PYG{c+c1}{\PYGZsh{} First, we\PYGZsq{}ll need NumPy to generate random numbers for us, and we\PYGZsq{}ll use the generic uniform distribution for this simulation.}

\PYG{c+c1}{\PYGZsh{} In[1]:}


\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(} \PYG{l+m+mi}{10} \PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Make sure we can generate 10 random values in [0,1]}


\PYG{c+c1}{\PYGZsh{} The CLT says that we can make a new random variable by summing those numbers.  Let\PYGZsq{}s do so.}

\PYG{c+c1}{\PYGZsh{} In[2]:}


\PYG{k}{def} \PYG{n+nf}{my\PYGZus{}random\PYGZus{}variable} \PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(} \PYG{l+m+mi}{10} \PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}

\PYG{n}{my\PYGZus{}random\PYGZus{}variable}\PYG{p}{(}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Test it}


\PYG{c+c1}{\PYGZsh{} That random variable is supposed to look sort of like a bell\PYGZhy{}shaped curve.  Let\PYGZsq{}s sample 1000 values from it and make a histogram to see if that\PYGZsq{}s true.}

\PYG{c+c1}{\PYGZsh{} In[4]:}


\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{n}{sample} \PYG{o}{=} \PYG{p}{[} \PYG{n}{my\PYGZus{}random\PYGZus{}variable}\PYG{p}{(}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1000}\PYG{p}{)} \PYG{p}{]}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{hist}\PYG{p}{(} \PYG{n}{sample}\PYG{p}{,} \PYG{n}{bins}\PYG{o}{=}\PYG{l+m+mi}{30} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}


\PYG{c+c1}{\PYGZsh{} Yes, that looks like a bell curve!  Its mean and standard deviation are as follows.}

\PYG{c+c1}{\PYGZsh{} In[5]:}


\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(} \PYG{n}{sample} \PYG{p}{)}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{std}\PYG{p}{(} \PYG{n}{sample} \PYG{p}{)}
\end{sphinxVerbatim}

Running that Python script will produce the same plot that’s shown in this Jupyter notebook.  If you’re using a Python IDE, it will typically appear in that IDE.  If you’re running it from the terminal, it will probably pop up a separate Python window showing the plot, and your script will terminate once you’ve closed the window.

Although comments in code are great, the comments above look like they belong in an interactive notebook that someone would read, with the output and pictures included.  So they’re not the kind of comments we need in a Python script.  To make things more succinct, I’m going to delete them.  That produces the following Python code.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(} \PYG{l+m+mi}{10} \PYG{p}{)}

\PYG{k}{def} \PYG{n+nf}{my\PYGZus{}random\PYGZus{}variable} \PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(} \PYG{l+m+mi}{10} \PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}

\PYG{n}{my\PYGZus{}random\PYGZus{}variable}\PYG{p}{(}\PYG{p}{)}

\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{n}{sample} \PYG{o}{=} \PYG{p}{[} \PYG{n}{my\PYGZus{}random\PYGZus{}variable}\PYG{p}{(}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1000}\PYG{p}{)} \PYG{p}{]}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{hist}\PYG{p}{(} \PYG{n}{sample}\PYG{p}{,} \PYG{n}{bins}\PYG{o}{=}\PYG{l+m+mi}{30} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}

\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(} \PYG{n}{sample} \PYG{p}{)}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{std}\PYG{p}{(} \PYG{n}{sample} \PYG{p}{)}
\end{sphinxVerbatim}

Notice also that three lines in the code don’t actually do anything.  In Jupyter, a line of code like \sphinxcode{\sphinxupquote{np.random.rand( 10 )}} (the second line of the above code), when placed at the end of a cell, will show its output to us in the notebook.  But in a Python script, without a \sphinxcode{\sphinxupquote{print()}} function call, it won’t show us anything.  But that line of code, together with the \sphinxcode{\sphinxupquote{my\_random\_variable()}} line later, were both done as little tests to see if our code was working correctly.  We don’t need to see their output in our Python script, so I’ll delete those lines.

However, the final line of code may be interesting to us, because the CLT actually speaks about the mean and standard deviation of the resulting random variable.  So we might like to see that value.  I’ll add some \sphinxcode{\sphinxupquote{print()}} function calls so that our script displays those values in a readable way.  I’ll also clean it up by moving all the imports the top.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}

\PYG{k}{def} \PYG{n+nf}{my\PYGZus{}random\PYGZus{}variable} \PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(} \PYG{l+m+mi}{10} \PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}

\PYG{n}{sample} \PYG{o}{=} \PYG{p}{[} \PYG{n}{my\PYGZus{}random\PYGZus{}variable}\PYG{p}{(}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1000}\PYG{p}{)} \PYG{p}{]}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{hist}\PYG{p}{(} \PYG{n}{sample}\PYG{p}{,} \PYG{n}{bins}\PYG{o}{=}\PYG{l+m+mi}{30} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Mean:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(} \PYG{n}{sample} \PYG{p}{)} \PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Standard deviation:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{std}\PYG{p}{(} \PYG{n}{sample} \PYG{p}{)} \PYG{p}{)}
\end{sphinxVerbatim}

Before proceeding to Step 2, be sure that you can successfully run your newly created Python script and verify that it generates the output you want.  Once you’ve done so, you have a Python script that we’re ready to bring into Streamlit.


\section{Step 2. Converting your script to use Streamlit}
\label{\detokenize{chapter-14-dashboards:step-2-converting-your-script-to-use-streamlit}}
This step is very easy, but the results are not very spectacular (yet).  You simply take your existing Python script and make the following easy changes.
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Add \sphinxcode{\sphinxupquote{import streamlit as st}} to the top of the file, before anything else.  (This imports all the Streamlit tools into your script.)

\item {} 
Change any \sphinxcode{\sphinxupquote{print()}} function call in your script to \sphinxcode{\sphinxupquote{st.write()}} instead.  (This replaces ordinary Python printing, which goes to the terminal, with Streamlit printing, which will go to the dashboard you’re creating.)

\item {} 
Change any \sphinxcode{\sphinxupquote{plt.show()}} function call in your script to \sphinxcode{\sphinxupquote{st.pyplot(plt.gcf())}} instead.  (This replaces ordinary Python plotting, which appears in its own window or in your IDE, with Streamlit plotting, which will go to the dashboard you’re creating.)

\end{enumerate}

If we make these changes to the script above, we get the following result.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{streamlit} \PYG{k}{as} \PYG{n+nn}{st}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}

\PYG{k}{def} \PYG{n+nf}{my\PYGZus{}random\PYGZus{}variable} \PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(} \PYG{l+m+mi}{10} \PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}

\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{n}{sample} \PYG{o}{=} \PYG{p}{[} \PYG{n}{my\PYGZus{}random\PYGZus{}variable}\PYG{p}{(}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1000}\PYG{p}{)} \PYG{p}{]}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{hist}\PYG{p}{(} \PYG{n}{sample}\PYG{p}{,} \PYG{n}{bins}\PYG{o}{=}\PYG{l+m+mi}{30} \PYG{p}{)}
\PYG{n}{st}\PYG{o}{.}\PYG{n}{pyplot}\PYG{p}{(}\PYG{n}{plt}\PYG{o}{.}\PYG{n}{gcf}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{st}\PYG{o}{.}\PYG{n}{write}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Mean:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(} \PYG{n}{sample} \PYG{p}{)} \PYG{p}{)}
\PYG{n}{st}\PYG{o}{.}\PYG{n}{write}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Standard deviation:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{std}\PYG{p}{(} \PYG{n}{sample} \PYG{p}{)} \PYG{p}{)}
\end{sphinxVerbatim}

But now this script cannot be run with Python alone; now it must be run using Streamlit, which provides the entire context of a web page and automatic reloading of your script as needed, etc.  Thus you need to run the following command from your computer’s terminal, in the same folder as your script.  My script is called \sphinxcode{\sphinxupquote{central\sphinxhyphen{}limit\sphinxhyphen{}theorem.py}}, but yours will have a different name.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{streamlit} \PYG{n}{run} \PYG{n}{central}\PYG{o}{\PYGZhy{}}\PYG{n}{limit}\PYG{o}{\PYGZhy{}}\PYG{n}{theorem}\PYG{o}{.}\PYG{n}{py}
\end{sphinxVerbatim}

If you’re using Jupyter to edit your Python script, you can start a new terminal from the Launcher in Jupyter, and type the above command in it.  You may need to use \sphinxcode{\sphinxupquote{cd}} to switch into the appropriate folder.  If you’re not familiar with changing folders using \sphinxcode{\sphinxupquote{cd}}, you may benefit from a tutorial on basic command line use.  Here is \sphinxhref{https://swcarpentry.github.io/shell-novice/02-filedir/index.html}{one for Unix and Mac} and here is \sphinxhref{https://www.computerhope.com/issues/chusedos.htm}{one for Windows}.

When I do so, it opens a page in my browser showing me a tiny little dashboard app!  Here is a screenshot.

\sphinxincludegraphics{{streamlit-screenshot-1}.png}

You may notice that the terminal in which you ran \sphinxcode{\sphinxupquote{streamlit run ...}} did not return you immediately to your prompt.  The Streamlit environment is still running, and will let you refresh your app if you update the Python script on which it’s built.  If you want to stop the Streamlit environment (say, when you’re done working) you can go to the terminal and press Ctrl+C.

In fact, let’s try updating the Python script now.  Make a small change, such as changing the text “Mean:” to “Sample mean:” and then saving the Python script.  The web page should pop up a small information indicator in the top right, like the one shown below.

\sphinxincludegraphics{{streamlit-menu}.png}

It’s asking if you want to rebuild your app since it changed.  I click “Always rerun” so that, in the future, every time I update my Python script \sphinxstyleemphasis{and save it,} the dashboard app I’m building will automatically be reloaded without any effort on my part.


\section{Step 3. Abstraction}
\label{\detokenize{chapter-14-dashboards:step-3-abstraction}}
Recall from {\hyperref[\detokenize{chapter-7-abstraction::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Chapter 7 of these notes}}}} the techniques we have for making code more general.  These include noting when a specific computation may need to be done more than once with varying inputs, and turning that code into a function.

When using Streamlit to build a dashboard, the power it provides is that it will run our \sphinxstyleemphasis{entire Python script} as many times as needed, with inputs chosen by the user.  To make this happen, we first need to choose which parts of our script are going to become parameters that the user can change.  The first step in this process is to give each of those values names and turn them into variables that we declare at the top of our script.

In the small example we’re using in this chapter, I have just two places where I will turn constants into parameters.  One constant is how many uniform random variables we will include in our sum (formerly fixed at 10) and the other is how large will be the sample we use to create our histogram (formerly fixed at 1000).  Here is a new version of the code in which those two parameters are declared up front.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{streamlit} \PYG{k}{as} \PYG{n+nn}{st}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}

\PYG{n}{num\PYGZus{}random\PYGZus{}variables\PYGZus{}to\PYGZus{}sum} \PYG{o}{=} \PYG{l+m+mi}{10}    \PYG{c+c1}{\PYGZsh{} These two lines of code are new.}
\PYG{n}{sample\PYGZus{}size} \PYG{o}{=} \PYG{l+m+mi}{1000}                  \PYG{c+c1}{\PYGZsh{} Notice where they\PYGZsq{}re used, below.}

\PYG{k}{def} \PYG{n+nf}{my\PYGZus{}random\PYGZus{}variable} \PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(} \PYG{n}{num\PYGZus{}random\PYGZus{}variables\PYGZus{}to\PYGZus{}sum} \PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}

\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{n}{sample} \PYG{o}{=} \PYG{p}{[} \PYG{n}{my\PYGZus{}random\PYGZus{}variable}\PYG{p}{(}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{sample\PYGZus{}size}\PYG{p}{)} \PYG{p}{]}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{hist}\PYG{p}{(} \PYG{n}{sample} \PYG{p}{)}
\PYG{n}{st}\PYG{o}{.}\PYG{n}{pyplot}\PYG{p}{(}\PYG{n}{plt}\PYG{o}{.}\PYG{n}{gcf}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{st}\PYG{o}{.}\PYG{n}{write}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sample Mean:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(} \PYG{n}{sample} \PYG{p}{)} \PYG{p}{)}
\PYG{n}{st}\PYG{o}{.}\PYG{n}{write}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sample Standard Deviation:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{std}\PYG{p}{(} \PYG{n}{sample} \PYG{p}{)} \PYG{p}{)}
\end{sphinxVerbatim}

If you save this Python script and revisit your dashboard app page, nothing should have changed, because the code produces the same results (except, of course, for small random variation inherent in the simulation).


\section{Step 4. Creating input controls}
\label{\detokenize{chapter-14-dashboards:step-4-creating-input-controls}}
Streamlit makes it extremely easy to turn code like \sphinxcode{\sphinxupquote{sample\_size = 1000}} into code that lets the user of your dashboard choose the value of \sphinxcode{\sphinxupquote{sample\_size}}.  The most common way to let the user choose a number is with a slider input, which you can create in Streamlit with \sphinxcode{\sphinxupquote{st.slider()}}.  You simply replace the constant 1000 with a call to the \sphinxcode{\sphinxupquote{st.slider()}} function, and Streamlit automatically builds the user interface for you!

The function call looks like \sphinxcode{\sphinxupquote{st.slider("Prompt",min,max,default,step)}}, where the parameters have the following meanings.
\begin{itemize}
\item {} 
The prompt is a string that will appear in the dashboard, explaining to the user what the slider does.

\item {} 
The min and max values are required, and they determine the leftmost and rightmost values on the slider.

\item {} 
The default value is optional, but it can be used to specify where the slider begins when the dashboard is first loaded.

\item {} 
The step value is optional, but it says how far the slider moves in a single step.  For instance, if you want the user to only be able to choose whole numbers, set step to 1, so that they cannot move the slider in between whole numbers.

\end{itemize}

I add two \sphinxcode{\sphinxupquote{st.slider()}} calls to our code as follows.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{streamlit} \PYG{k}{as} \PYG{n+nn}{st}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}

\PYG{n}{num\PYGZus{}random\PYGZus{}variables\PYGZus{}to\PYGZus{}sum} \PYG{o}{=} \PYG{n}{st}\PYG{o}{.}\PYG{n}{slider}\PYG{p}{(}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Include this many uniform random variables in the sum:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
    \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{100}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{1} \PYG{p}{)}
\PYG{n}{sample\PYGZus{}size} \PYG{o}{=} \PYG{n}{st}\PYG{o}{.}\PYG{n}{slider}\PYG{p}{(}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Create a histogram from a sample of this size:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
    \PYG{l+m+mi}{100}\PYG{p}{,} \PYG{l+m+mi}{10000}\PYG{p}{,} \PYG{l+m+mi}{1000}\PYG{p}{,} \PYG{l+m+mi}{100} \PYG{p}{)}

\PYG{k}{def} \PYG{n+nf}{my\PYGZus{}random\PYGZus{}variable} \PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(} \PYG{n}{num\PYGZus{}random\PYGZus{}variables\PYGZus{}to\PYGZus{}sum} \PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}

\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{n}{sample} \PYG{o}{=} \PYG{p}{[} \PYG{n}{my\PYGZus{}random\PYGZus{}variable}\PYG{p}{(}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{sample\PYGZus{}size}\PYG{p}{)} \PYG{p}{]}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{hist}\PYG{p}{(} \PYG{n}{sample} \PYG{p}{)}
\PYG{n}{st}\PYG{o}{.}\PYG{n}{pyplot}\PYG{p}{(}\PYG{n}{plt}\PYG{o}{.}\PYG{n}{gcf}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{st}\PYG{o}{.}\PYG{n}{write}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sample Mean:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(} \PYG{n}{sample} \PYG{p}{)} \PYG{p}{)}
\PYG{n}{st}\PYG{o}{.}\PYG{n}{write}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sample Standard Deviation:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{std}\PYG{p}{(} \PYG{n}{sample} \PYG{p}{)} \PYG{p}{)}
\end{sphinxVerbatim}

This adds the following user interface to the top of my dashboard app.  It’s finally interactive!

\sphinxincludegraphics{{streamlit-sliders}.png}

If you drag the sliders, you see the output update immediately.  \sphinxstyleemphasis{When you change an input slider, Streamlit automatically re\sphinxhyphen{}runs your entire Python script and updates the output in the page, typically very quickly.}  This happens every time you move one of the sliders.


\section{Step 5. Increasing Awesomeness}
\label{\detokenize{chapter-14-dashboards:step-5-increasing-awesomeness}}
The above example was simple, but there are many ways you could make the application better.  Here are a few examples.


\subsection{New kinds of controls}
\label{\detokenize{chapter-14-dashboards:new-kinds-of-controls}}
A slider is not the only type of input.  Two other common ones you might want to have are detailed here, but a \sphinxhref{https://docs.streamlit.io/en/stable/api.html\#display-interactive-widgets}{comprehensive list} appears on the Streamlit website.

To create a text box that accepts only numerical inputs, use \sphinxcode{\sphinxupquote{value = st.number\_input("Prompt",min,max,default)}}.  The only required paramater is the text prompt, and the other three are optional.

\sphinxincludegraphics{{streamlit-number-input}.png}

To create a drop\sphinxhyphen{}down list from which you can pick a value, use \sphinxcode{\sphinxupquote{choice = st.selectbox("Prompt",("List","of","options"))}}.  By default, the first one is selected, but you can change it with an optional third parameter, the index of the default option.

\sphinxincludegraphics{{streamlit-selectbox}.png}


\subsection{Improve output clarity}
\label{\detokenize{chapter-14-dashboards:improve-output-clarity}}
The very simple app we’ve built so far would not make a lot of sense to anyone visiting it for the first time.  The explanation of the CLT from our notebook is gone, and the output shown in the dashboard is not explained.  We can add explanations back to our app with the \sphinxcode{\sphinxupquote{st.write()}} command.  Provide it a string of Markdown content, just as you would put into a Jupyter notebook cell, and it will include it in your app.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{st}\PYG{o}{.}\PYG{n}{write}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}\PYGZsq{}\PYGZsq{}}

\PYG{l+s+s1}{\PYGZsh{} This would be a heading}

\PYG{l+s+s1}{Don}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{t forget that you can use Python triple quotes to make a string}
\PYG{l+s+s1}{last over several lines, so that you can write as much as you want.}

\PYG{l+s+s1}{![A hilarious photo](http://www.imgur.com/your\PYGZhy{}hilarious\PYGZhy{}photo\PYGZhy{}url\PYGZhy{}here)}

\PYG{l+s+s1}{\PYGZsq{}\PYGZsq{}\PYGZsq{}} \PYG{p}{)}
\end{sphinxVerbatim}

You can also improve output clarity by moving some things into the app’s sidebar, which sits on the left by default, like on many websites.  You do this by replacing the relevant instances of \sphinxcode{\sphinxupquote{st}} in your code with \sphinxcode{\sphinxupquote{st.sidebar}}.  For example, while \sphinxcode{\sphinxupquote{st.slider("Choose a value:",1,100)}} places a slider in the main part of the app, \sphinxcode{\sphinxupquote{st.sidebar.slider("Choose a value:",1,100)}} puts it in the sidebar.

The one exception to this is that \sphinxcode{\sphinxupquote{st.write()}} does not have a sidebar version; there is no \sphinxcode{\sphinxupquote{st.sidebar.write()}}.  You can, however, still use \sphinxcode{\sphinxupquote{st.sidebar.markdown()}} to display any kind of Markdown content in the sidebar.

If you don’t want to have to deal with Markdown syntax, you can always call specific Streamlit functions like \sphinxcode{\sphinxupquote{st.title()}}, \sphinxcode{\sphinxupquote{st.header()}}, \sphinxcode{\sphinxupquote{st.subheader()}}, and \sphinxcode{\sphinxupquote{st.text()}}.

Applying these techniques to my dashboard can make it look much more clean and understandable.  Here are the results, in code and in a screenshot.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{streamlit} \PYG{k}{as} \PYG{n+nn}{st}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}

\PYG{n}{st}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Central Limit Theorem Example}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}

\PYG{n}{st}\PYG{o}{.}\PYG{n}{sidebar}\PYG{o}{.}\PYG{n}{markdown}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}\PYGZsq{}\PYGZsq{}}
\PYG{l+s+s1}{The Central Limit Theorem assumes you have a collection \PYGZdl{}X\PYGZus{}1,}\PYG{l+s+s1}{\PYGZbs{}}\PYG{l+s+s1}{ldots,X\PYGZus{}n\PYGZdl{}}
\PYG{l+s+s1}{of random variables that you will sum to create a new random variable}
\PYG{l+s+s1}{\PYGZdl{}X=}\PYG{l+s+s1}{\PYGZbs{}}\PYG{l+s+s1}{sum\PYGZus{}}\PYG{l+s+s1}{\PYGZob{}}\PYG{l+s+s1}{i=1\PYGZcb{}\PYGZca{}n X\PYGZus{}i\PYGZdl{}.  Here we will sum several}
\PYG{l+s+s1}{uniform random variables on the interval \PYGZdl{}[0,1]\PYGZdl{}.}
\PYG{l+s+s1}{You may choose the value of \PYGZdl{}n\PYGZdl{} here.}
\PYG{l+s+s1}{\PYGZsq{}\PYGZsq{}\PYGZsq{}} \PYG{p}{)}

\PYG{n}{num\PYGZus{}random\PYGZus{}variables\PYGZus{}to\PYGZus{}sum} \PYG{o}{=} \PYG{n}{st}\PYG{o}{.}\PYG{n}{sidebar}\PYG{o}{.}\PYG{n}{slider}\PYG{p}{(}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{How many uniform random variables should we include in the sum?}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
    \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{100}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{1} \PYG{p}{)}

\PYG{n}{st}\PYG{o}{.}\PYG{n}{sidebar}\PYG{o}{.}\PYG{n}{markdown}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}\PYGZsq{}\PYGZsq{}}
\PYG{l+s+s1}{The Central Limit Theorem says that the new random variable \PYGZdl{}X\PYGZdl{} will}
\PYG{l+s+s1}{be approximately normally distributed.  To visualize this, we will sample}
\PYG{l+s+s1}{many values from \PYGZdl{}X\PYGZdl{} and create a histogram.  It should look more and more}
\PYG{l+s+s1}{like a bell curve as we increase \PYGZdl{}n\PYGZdl{}.}
\PYG{l+s+s1}{\PYGZsq{}\PYGZsq{}\PYGZsq{}} \PYG{p}{)}

\PYG{n}{sample\PYGZus{}size} \PYG{o}{=} \PYG{n}{st}\PYG{o}{.}\PYG{n}{sidebar}\PYG{o}{.}\PYG{n}{slider}\PYG{p}{(}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{How large of a sample should we use to create the histogram?}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
    \PYG{l+m+mi}{100}\PYG{p}{,} \PYG{l+m+mi}{10000}\PYG{p}{,} \PYG{l+m+mi}{1000}\PYG{p}{,} \PYG{l+m+mi}{100} \PYG{p}{)}

\PYG{k}{def} \PYG{n+nf}{my\PYGZus{}random\PYGZus{}variable} \PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(} \PYG{n}{num\PYGZus{}random\PYGZus{}variables\PYGZus{}to\PYGZus{}sum} \PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}

\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{n}{sample} \PYG{o}{=} \PYG{p}{[} \PYG{n}{my\PYGZus{}random\PYGZus{}variable}\PYG{p}{(}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{sample\PYGZus{}size}\PYG{p}{)} \PYG{p}{]}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{hist}\PYG{p}{(} \PYG{n}{sample}\PYG{p}{,} \PYG{n}{bins}\PYG{o}{=}\PYG{l+m+mi}{30} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(} \PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Histogram of a sample of size }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{sample\PYGZus{}size}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{ from X}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(} \PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{X = the sum of }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{num\PYGZus{}random\PYGZus{}variables\PYGZus{}to\PYGZus{}sum}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{ uniform random variables on [0,1]}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Frequency}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{st}\PYG{o}{.}\PYG{n}{pyplot}\PYG{p}{(}\PYG{n}{plt}\PYG{o}{.}\PYG{n}{gcf}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{st}\PYG{o}{.}\PYG{n}{write}\PYG{p}{(} \PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}\PYGZsq{}\PYGZsq{}}
\PYG{l+s+s1}{Because each \PYGZdl{}}\PYG{l+s+se}{\PYGZbs{}\PYGZbs{}}\PYG{l+s+s1}{mu\PYGZus{}}\PYG{l+s+se}{\PYGZob{}\PYGZob{}}\PYG{l+s+s1}{X\PYGZus{}i}\PYG{l+s+se}{\PYGZcb{}\PYGZcb{}}\PYG{l+s+s1}{=0.5\PYGZdl{} and \PYGZdl{}n=}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{num\PYGZus{}random\PYGZus{}variables\PYGZus{}to\PYGZus{}sum}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZdl{},}
\PYG{l+s+s1}{we conclude \PYGZdl{}}\PYG{l+s+se}{\PYGZbs{}\PYGZbs{}}\PYG{l+s+s1}{mu\PYGZus{}X=0.5}\PYG{l+s+se}{\PYGZbs{}\PYGZbs{}}\PYG{l+s+s1}{times}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{num\PYGZus{}random\PYGZus{}variables\PYGZus{}to\PYGZus{}sum}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{=}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{num\PYGZus{}random\PYGZus{}variables\PYGZus{}to\PYGZus{}sum}\PYG{o}{*}\PYG{l+m+mf}{0.5}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZdl{}.}

\PYG{l+s+s1}{Mean of our sample is \PYGZdl{}}\PYG{l+s+se}{\PYGZbs{}\PYGZbs{}}\PYG{l+s+s1}{bar}\PYG{l+s+se}{\PYGZob{}\PYGZob{}}\PYG{l+s+s1}{x}\PYG{l+s+se}{\PYGZcb{}\PYGZcb{}}\PYG{l+s+s1}{=}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(} \PYG{n}{sample} \PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+se}{\PYGZbs{}\PYGZbs{}}\PYG{l+s+s1}{approx}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{num\PYGZus{}random\PYGZus{}variables\PYGZus{}to\PYGZus{}sum}\PYG{o}{*}\PYG{l+m+mf}{0.5}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZdl{}.}
\PYG{l+s+s1}{\PYGZsq{}\PYGZsq{}\PYGZsq{}} \PYG{p}{)}
\end{sphinxVerbatim}

\sphinxincludegraphics{{streamlit-screenshot-2}.png}


\subsection{Improve speed}
\label{\detokenize{chapter-14-dashboards:improve-speed}}
If your dashboard loads a large dataset, or takes any other action that may consume a lot of time, then whenever a user adjusts any of the input controls, the dashboard will take a long time to respond, because it must load the entire dataset again, or whatever the long computation is.  You can improve this behavior by telling Streamlit to cache (remember) the value of the lengthy computation so that it doesn’t unnecessarily redo it.

For instance, recall the many times we’ve loaded the (somewhat large) sample of mortgage applications with code like the following.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{practice\PYGZhy{}project\PYGZhy{}dataset\PYGZhy{}1.csv}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\end{sphinxVerbatim}

We can tell Streamlit to cache the results of this by taking the slow or complex code and moving into its own function, one that takes no parameters and returns the result of the lengthy computation.  We then call the function to get the result.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{load\PYGZus{}mortgage\PYGZus{}data} \PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{practice\PYGZhy{}project\PYGZhy{}dataset\PYGZhy{}1.csv}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}

\PYG{n}{df} \PYG{o}{=} \PYG{n}{load\PYGZus{}mortgage\PYGZus{}data}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

Of course, this behaves exactly like it did before, but now we can add a special Streamlit flag that enables caching for the function we’ve written.  The code looks like the following; the only new part is the first line.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nd}{@st}\PYG{o}{.}\PYG{n}{cache}
\PYG{k}{def} \PYG{n+nf}{load\PYGZus{}mortgage\PYGZus{}data} \PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{practice\PYGZhy{}project\PYGZhy{}dataset\PYGZhy{}1.csv}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}

\PYG{n}{df} \PYG{o}{=} \PYG{n}{load\PYGZus{}mortgage\PYGZus{}data}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

The \sphinxcode{\sphinxupquote{@st.cache}} code tells Streamlit that the function immediately after it should be run only once, the first time the dashboard is launched, and any later attempt to call to the same function can just use the previously\sphinxhyphen{}loaded value, without actually re\sphinxhyphen{}running the function at all.

More information on Streamlit caching is available \sphinxhref{https://docs.streamlit.io/en/stable/api.html\#optimize-performance}{here}.

You now know how to create dashboard apps with a good bit of flexibility and sophistication!  The next question is how to deploy them to the web.  The second half of today’s notes answer that question.


\section{Making your dashboard into a Heroku app}
\label{\detokenize{chapter-14-dashboards:making-your-dashboard-into-a-heroku-app}}
You can deploy a Streamlit dashboard to a website in many different ways.  Here, we will cover a tool used by many developers to deploy websites to a free cloud hosting platorm, Heroku.  While Heroku has paid plans for websites and web apps that get a lot of traffic, the free plan is far more than we need for our purposes.  As mentioned {\hyperref[\detokenize{chapter-14-dashboards:whats-a-dashboard-and-why-do-we-have-them}]{\emph{above}}}, I expect that you’ve already registered for a Heroku account and installed their command line interface tools.  (The lessons I give below are inspired by a very helpful blog post I read on this technology; thanks to \sphinxhref{https://gilberttanner.com/blog/deploying-your-streamlit-dashboard-with-heroku}{Gilbert Tanner} for the original information.)


\subsection{Step 1. Make your project a git repository}
\label{\detokenize{chapter-14-dashboards:step-1-make-your-project-a-git-repository}}
This step has several parts.  Parts 1\sphinxhyphen{}3 need to be done only once.  Part 4 must be done any time you change your app and want to prepare to deploy a new version to Heroku.
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxstylestrong{Make sure your files are in a folder by themselves, dedicated to this dashboard project.}  In the case of the example dashboard I’ve been using in this chapter, that’s just one file, my Python script.  If you had data files, images, or Python modules to go with your Python script, you’d move them into that folder, too.

\item {} 
\sphinxstylestrong{Change the name of your main Python script to \sphinxcode{\sphinxupquote{app.py}}.}  Although this is not required, it will make the instructions simpler from here on.

\item {} 
\sphinxstylestrong{Turn that folder into a git repository.}  How to do this was covered in \sphinxhref{chapter-8-version-control\#create-a-repository}{this section of the Chapter 8 notes}.

\item {} 
\sphinxstylestrong{Commit all the files to the git repository.}  How to do a commit was covered in \sphinxhref{chapter-8-version-control\#make-a-commit}{this section of that same chapter}.

\end{enumerate}


\subsection{Step 2. Connect your repository to Heroku}
\label{\detokenize{chapter-14-dashboards:step-2-connect-your-repository-to-heroku}}
This step has two parts, and it needs to be done only once.
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxstylestrong{Log in to Heroku on the command line.}  Go to any terminal and run the command \sphinxcode{\sphinxupquote{heroku login}}.  For instance, if you’re using Jupyter, you can open a new Launcher, run a terminal, and then execute that command.  It should launch your default browser and prompt you to log in there.

\item {} 
\sphinxstylestrong{Tell your git repository about Heroku.}  While still in the terminal, change into the directory where your dashboard project is located and run \sphinxcode{\sphinxupquote{heroku create}}.  This will create an online virtual machine in which you can deploy and run your Heroku app.  If you’re unsure about how to change directories in the terminal, see the tutorials linked to {\hyperref[\detokenize{chapter-14-dashboards:Step-2.-Converting-your-script-to-use-Streamlit}]{\emph{above}}}.

\end{enumerate}

Your online virtual machine will have a funny name, like \sphinxcode{\sphinxupquote{careful\sphinxhyphen{}muskrat\sphinxhyphen{}17.herokuapp.com}}.  This is fine for the little test we’re running here, but if you make a nice dashboard and want it to have a better name, you can always \sphinxhref{https://devcenter.heroku.com/articles/renaming-apps}{change the name later}.


\subsection{Step 3. Add files Heroku will need}
\label{\detokenize{chapter-14-dashboards:step-3-add-files-heroku-will-need}}
Soon, we will push your git repository to Heroku, and expect Heroku to run your app.  But Heroku is a very generic tool; it’s not for Streamlit apps only, nor even for just Python apps.  So we cannot expect Heroku to know what to do with our Python script.  We will need to tell it how to set itself up with the necessary Python modules and how to run our dashboard once it has done so.  This requires putting three configuration files into our git repository.  This step needs to be done only once.

\sphinxstylestrong{Requirements:} Create a new text file in your project folder and call it \sphinxcode{\sphinxupquote{requirements.txt}}.  (You can create text files in Jupyter from the launcher; just choose Text File.)  Requirements files are a Python standard, and list all the Python modules a project will need.  Your \sphinxcode{\sphinxupquote{requirements.txt}} file will need to list any Python module your dashboard uses.  Since mine uses pandas, matplotlib, and Streamlit, I will list those, with their current versions at the time of this writing.  The versions may be newer when you read this.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
pandas==1.0.1
matplotlib==3.1.3
streamlit==0.57.2
\end{sphinxVerbatim}

\sphinxstylestrong{Setup:} The following file will seem quite cryptic to most readers.  Its contents are mostly unimportant for our purposes here.  The short explanation is that Heroku virtual machines run Linux, and the following command is written in the language of the Linux command line, and tells Heroku how to set up your app.  Note that the only change you’ll want to make is to replace the text \sphinxcode{\sphinxupquote{your\sphinxhyphen{}email@bentley.edu}} with your actual email address.

Save this in a new text file called \sphinxcode{\sphinxupquote{setup.sh}} (where the “sh” is short for “shell,” the word for the Linux command line).  It is crucial that the file end in \sphinxcode{\sphinxupquote{.sh}}, not \sphinxcode{\sphinxupquote{.txt}}, even though it is a plain text file.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
mkdir \PYGZhy{}p \PYGZti{}/.streamlit/

\PYG{n+nb}{echo} \PYG{l+s+s2}{\PYGZdq{}\PYGZbs{}}
\PYG{l+s+s2}{[general]\PYGZbs{}n\PYGZbs{}}
\PYG{l+s+s2}{email = \PYGZbs{}\PYGZdq{}your\PYGZhy{}email@bentley.edu\PYGZbs{}\PYGZdq{}\PYGZbs{}n\PYGZbs{}}
\PYG{l+s+s2}{\PYGZdq{}} \PYGZgt{} \PYGZti{}/.streamlit/credentials.toml

\PYG{n+nb}{echo} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZbs{}}
\PYG{l+s+s2}{[server]\PYGZbs{}n\PYGZbs{}}
\PYG{l+s+s2}{headless = true\PYGZbs{}n\PYGZbs{}}
\PYG{l+s+s2}{enableCORS=false\PYGZbs{}n\PYGZbs{}}
\PYG{l+s+s2}{port = }\PYG{n+nv}{\PYGZdl{}PORT}\PYG{l+s+s2}{\PYGZbs{}n\PYGZbs{}}
\PYG{l+s+s2}{\PYGZdq{}} \PYGZgt{} \PYGZti{}/.streamlit/config.toml
\end{sphinxVerbatim}

\sphinxstylestrong{Procfile:} Heroku needs to know what commands to run to get a web app started, and it expects to find them in a file called \sphinxcode{\sphinxupquote{Procfile}} (with no extension and a capital P).  It is also a plain text file.  Notice that it tells Heroku to run the \sphinxcode{\sphinxupquote{setup.sh}} file we provided, then run our Streamlit app.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
web: sh setup.sh \PYG{o}{\PYGZam{}\PYGZam{}} streamlit run app.py
\end{sphinxVerbatim}

Once you’ve placed all three of these files into your dashboard project folder, commit all of them to your git repository.

Although this step is a bit of a hassle, the good news is that once you’ve done it for one Streamlit project, you can easily just copy these three files to any other Streamlit project you work on, potentially unchanged, so that you don’t have to recreate them afresh each time.  The only thing that might change is adding new modules to the \sphinxcode{\sphinxupquote{requirements.txt}} file, if needed, based on your projects.


\subsection{Step 4. Deploy your app to the web}
\label{\detokenize{chapter-14-dashboards:step-4-deploy-your-app-to-the-web}}
You deploy an app to the web with a simple git push.  (Recall that git pushes and pulls were explained in \sphinxhref{chapter-8-version-control\#what-if-i-want-to-collaborate}{the chapter on version control}.)  You can do this in one of two ways.

In the version control chapter, I suggested using the GitHub Desktop app to push changes to the web, by just clicking the “Publish branch” button.  But when you publish to Heroku, it prints a lot of useful information about whether your app deployed successfully or not and why.  If you use the GitHub Desktop app, you won’t see that information; it will all be hidden from you.  So instead, I recommend using the terminal for this task as well.

Assuming your terminal is still in your project folder, issue the command \sphinxcode{\sphinxupquote{git push heroku master}}.  You will need to wait while Heroku does all the setup for your app, but it will print a lot of messages explaining what it’s doing.  If all goes well, the final line of your output will look like the following.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
remote: \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZgt{} Launching...
remote:        Released v1
remote:        https://careful\PYGZhy{}muskrat\PYGZhy{}17.herokuapp.com/ deployed to Heroku
remote:
remote: Verifying deploy... done.
To https://git.heroku.com/careful\PYGZhy{}muskrat\PYGZhy{}17.git
 * [new branch]      master \PYGZhy{}\PYGZgt{} master
\end{sphinxVerbatim}

You can copy and paste the \sphinxcode{\sphinxupquote{https://...}} URL into your browser to visit the dashboard.  To skip the copy\sphinxhyphen{}and\sphinxhyphen{}paste step, you can just run \sphinxcode{\sphinxupquote{heroku open}} in the terminal and it will launch the app in your browser for you.  You can share its URL with anyone in the world and they can see the dashboard online as well.

To see the dashboard I’ve been building throughout this tutorial, visit \sphinxurl{https://clt-example.herokuapp.com/}.


\subsection{Updating your app later}
\label{\detokenize{chapter-14-dashboards:updating-your-app-later}}
If you later make changes to the dashboard on your computer and want to update the web version to reflect those changes, you’ll just need to repeat two of the instructions from above.
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Commit all your changes to your git repository.

\item {} 
Run \sphinxcode{\sphinxupquote{git push heroku master}} again.

\end{enumerate}

This will re\sphinxhyphen{}deploy an updated version of the app, and if you then refresh your browser, you should see the new version.


\section{Closing remarks}
\label{\detokenize{chapter-14-dashboards:closing-remarks}}
One of the requirements for your final project in MA346 is to include an online dashboard as part of your work.  Early in your project work it would be helpful to think through two strategic questions to help make that possible.  First, be sure to choose a project that lends itself well to having some part of its work showcased in a dashboard.  Second, ensure that at least one person on the project team is familiar enough with the content of this chapter to do it well for the final project.  I realize that this chapter’s content is rather technical, but not everyone in the class must master it fully.  But at least one person from each team must do so!

Finally, there are several opportunities for Learning On Your Own projects you can do based on this chapter’s content.  Here are a few.

\begin{sphinxadmonition}{note}{Learning on Your Own \sphinxhyphen{} Alternative to Streamlit: Dash}

A more flexible and powerful dashboard module for Python is called \sphinxhref{https://pypi.org/project/dash/}{Dash}.  However, Dash requires deeper programming knowledge than Streamlit does, so we chose to use Streamlit.  Take a few Dash tutorials, such as \sphinxhref{https://www.datacamp.com/community/tutorials/learn-build-dash-python}{this one on DataCamp}, build an app or two, and report back to the class on its strengths and weaknesses, plus where the reader could go to learn more.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Learning on Your Own \sphinxhyphen{} Alternative to Streamlit: Voila}

\sphinxhref{https://github.com/voila-dashboards/voila}{Voilà} is a different type of dashboard toolkit; it converts your Jupyter notebook directly into a dashboard.  However, it seems more complex to use than Streamlit, so it wasn’t my choice for this course.  As in the previous LOYO, try a Voilà tutorial, build an app using it, and report back to the class on its strengths and weaknesses, plus where the reader could go to learn more.
\end{sphinxadmonition}


\chapter{Relations as Graphs \sphinxhyphen{} Network Analysis}
\label{\detokenize{chapter-15-networks:relations-as-graphs-network-analysis}}\label{\detokenize{chapter-15-networks::doc}}
See also the slides that summarize a portion of this content.


\section{What is a graph?}
\label{\detokenize{chapter-15-networks:what-is-a-graph}}
In mathematics, the word \sphinxstyleemphasis{graph} has two meanings.  The more common one is what most people learned in algebra class, which is the graph of a function, on the ordinary Cartesian plane of \(x\) and \(y\) axes.  The less common meaning is a visualization of an interconnected network of objects.  In such a network, the objects being connected are called \sphinxstyleemphasis{nodes} or \sphinxstyleemphasis{vertices,} and the connections are called \sphinxstyleemphasis{edges,} \sphinxstyleemphasis{arrows,} or \sphinxstyleemphasis{links.}  While we call it a graph in mathematics, data scientists might refer to it instead as \sphinxstyleemphasis{network data.}

Let’s start with a small, pretend example.  Let’s say we spoke to five friends and asked them which of the others they’d turn to for advice about an important life decision.  We could depict their answers with a picture like the following.

\sphinxincludegraphics{{friends-graph}.png}

In that image, the five friends are shown in ovals; these are the vertices of the graph.  The connections among them are the edges of the graph, representing the friends’ answers to the question about advice.  For instance, the arrow from Augustus to Cyrano says that Augustus would consult Cyrano when needing advice about an important life decision, but the absence of an arrow from Beatriz to Englebert means that she would not consult him.

Now that we’ve seen a small (but pretend) example, let’s consider some more realistic examples.
\begin{itemize}
\item {} 
To prepare for class, you were asked to consider a spreadsheet recording shipping records between every two U.S. states in the year 1997.  In that data, the vertices were the 50 states and the edges were the records of how much money/weight of goods were shipped.

\item {} 
In today’s notes, we’ll also look at a spreadsheet created by marine biologists recording the interaction among a community of dolphins living off Doubtful Sound in New Zealand.  The vertices of that network are the dolphins and the edges represent social interactions among them.  (The data comes from \sphinxhref{http://www-personal.umich.edu/~mejn/netdata/}{Mark Newman’s website}, which cites the biologists who collected it.)

\item {} 
One of the largest examples of network data has as its vertices the collection of all pages on the internet, and edges are links between them.  Google does linear algebra\sphinxhyphen{}based computations on this enormous graph regularly, to update their search engine to reflect the latest changes on the web.

\end{itemize}

\begin{sphinxadmonition}{note}{Big Picture \sphinxhyphen{} A graph depicts a binary relation of a set with itself}

Notice that a graph is nothing but a picture representing a \sphinxstyleemphasis{binary relation,} a term we first defined in {\hyperref[\detokenize{chapter-2-mathematical-foundations::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{the notes on Chapter 2}}}}.  In the case of a graph, the two sets involved in the relation are the same; we’re connecting friends to friends in the picture above, or states to states with shipping information, or pages to pages with hyperlinks.  In a graph, the relation connects the set of vertices to itself, not to some other set.
\end{sphinxadmonition}

The graph of five friends shown above is a \sphinxstyleemphasis{directed graph,} because the edges have arrowheads to indicate that they make sense in only one direction.  While Augustus said he would seek advice from Cyrano, Cyrano did not say the same about Augustus.

In an \sphinxstyleemphasis{undirected graph,} every connection goes in both directions.  For instance, the relation recorded in the dolphin data is about spending time together.  If dolphin A is spending time with dolphin B, then the reverse is obviously also true.  So in the dolphin data, all connections go in both directions, and we can thus draw them without arrowheads; they are all “two\sphinxhyphen{}way streets.”


\section{Storing graphs in tables}
\label{\detokenize{chapter-15-networks:storing-graphs-in-tables}}
In our course, we store almost all of our data in tables, such as pandas DataFrames, CSV files, etc.  How can a graph be represented in a table?  There are two primary ways.

First, we can use an \sphinxstyleemphasis{adjacency matrix,} which is a table that tells which vertices are adjacent.  Its row headings are the vertices in the network, and the column headings are the same vertices again.  Each entry says whether the row connects to the column.  Here’s the adjacency matrix for the five friends graph shown above.


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|T|}
\hline


&\sphinxstyletheadfamily 
Augustus
&\sphinxstyletheadfamily 
Beatriz
&\sphinxstyletheadfamily 
Cyrano
&\sphinxstyletheadfamily 
Dauphine
&\sphinxstyletheadfamily 
Englebert
\\
\hline
\sphinxstylestrong{Augustus}
&
False
&
False
&
True
&
False
&
False
\\
\hline
\sphinxstylestrong{Beatriz}
&
False
&
False
&
True
&
False
&
False
\\
\hline
\sphinxstylestrong{Cyrano}
&
False
&
True
&
False
&
False
&
True
\\
\hline
\sphinxstylestrong{Dauphine}
&
False
&
False
&
True
&
False
&
False
\\
\hline
\sphinxstylestrong{Englebert}
&
True
&
True
&
False
&
False
&
False
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

Order is important here.  If we want to know whether Augustus \(\to\) Cyrano, we must look in the Augustus \sphinxstylestrong{row} and the Cyrano \sphinxstylestrong{column.}  (This is not hard to remember, because the row headings are actually visually to the left of the column headings, which fits the “row \(\to\) column” orientation of the arrow.)

Alternately, we could represent a relation the way we’ve discussed in the past.  We can just store in a table the list of pairs that make up the relation.  Each row in such a table represents an arrow in the graph.  For the five friends, that table looks like the following.


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\sphinxstyletheadfamily 
From
&\sphinxstyletheadfamily 
To
\\
\hline
Augustus
&
Cyrano
\\
\hline
Beatriz
&
Cyrano
\\
\hline
Cyrano
&
Beatriz
\\
\hline
Cyrano
&
Englebert
\\
\hline
Dauphine
&
Cyrano
\\
\hline
Englebert
&
Augustus
\\
\hline
Englebert
&
Beatriz
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

We will call this kind of table a \sphinxstyleemphasis{list of ordered pairs,} because the ordering of each pair often matters.  From Augustus to Cyrano is not the same as from Cyrano to Augustus.  We can also call it an \sphinxstyleemphasis{edge list,} because the connections in a graph are called edges.

\begin{sphinxadmonition}{note}{Big Picture \sphinxhyphen{} How pivoting/melting impacts graph data}

These two ways to store the data are very related.  If we just imagine a third column for this last table, “From,” “To,” and “Connected (True/False),” then the two tables could be converted from one to the other using pivot and melt from {\hyperref[\detokenize{chapter-6-single-table-verbs::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Chapter 6 of the course notes}}}}.  In that chapter, we learned that it is typically easier for a computer to process melted (tall) data, but easier for humans to read pivoted (wide) data.  To make our computations easier, we will work with this second, two\sphinxhyphen{}column form.
\end{sphinxadmonition}

But storing network data as a table of edges does have one small disadvantage:  It doesn’t make it obvious what the complete set of vertices is.  For instance, just given the second (tall) table shown above, we can’t be sure how many friends there are.  Is it just these five, or is there a sixth friend, or a seventh?  Imagine another friend, Fatima, who has unusual opinions, so she wouldn’t go to any of the friends for advice, nor would they go to her.  She wouldn’t show up in any of the edges, so we wouldn’t see her in the data.

Thus if we store a graph as an edge list, we will also need a separate list of the graph’s vertices.  That list will include every vertex mentioned in the edge list, and possibly some others.


\section{Loading network data}
\label{\detokenize{chapter-15-networks:loading-network-data}}

\subsection{Dolphin dataset}
\label{\detokenize{chapter-15-networks:dolphin-dataset}}
I’ve included the dolphin community data with these course notes.  You can download it here (as an Excel workbook).  I’ll explore it below to show you how it’s structured.

First, what sheets are stored in the workbook?

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{n}{sheets} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}excel}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZus{}static/dolphins.xlsx}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{sheet\PYGZus{}name}\PYG{o}{=}\PYG{k+kc}{None} \PYG{p}{)}
\PYG{n}{sheets}\PYG{o}{.}\PYG{n}{keys}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
dict\PYGZus{}keys([\PYGZsq{}ids\PYGZus{}and\PYGZus{}names\PYGZsq{}, \PYGZsq{}relationships\PYGZsq{}])
\end{sphinxVerbatim}

There are two sheets in the workbook, one called \sphinxcode{\sphinxupquote{"ids\_and\_names"}} and one called \sphinxcode{\sphinxupquote{"relationships"}}.  Let’s take a look at both.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df1} \PYG{o}{=} \PYG{n}{sheets}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ids\PYGZus{}and\PYGZus{}names}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{df1}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
   id        name
0   0        Beak
1   1  Beescratch
2   2      Bumper
3   3         CCL
4   4       Cross
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df2} \PYG{o}{=} \PYG{n}{sheets}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relationships}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{df2}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
   source  target
0       8       3
1       9       5
2       9       6
3      10       0
4      10       2
\end{sphinxVerbatim}

It seems as if the first sheet gives each dolphin, by name, a unique ID, while the second sheet shows the social connections of which dolphins spend time with which other ones.  This is just how we discussed storing the data above; there is a list of vertices in the first table and a list of edges in the second table.

But the data is not formatted conveniently.  The second table would be more convenient if it included dolphin names instead of IDs.  Let’s use Python dictionaries and \sphinxcode{\sphinxupquote{map()}} to fix it.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{convert\PYGZus{}id\PYGZus{}to\PYGZus{}name} \PYG{o}{=} \PYG{n+nb}{dict}\PYG{p}{(} \PYG{n+nb}{zip}\PYG{p}{(} \PYG{n}{df1}\PYG{o}{.}\PYG{n}{id}\PYG{p}{,} \PYG{n}{df1}\PYG{o}{.}\PYG{n}{name} \PYG{p}{)} \PYG{p}{)}
\PYG{n}{df2}\PYG{o}{.}\PYG{n}{source} \PYG{o}{=} \PYG{n}{df2}\PYG{o}{.}\PYG{n}{source}\PYG{o}{.}\PYG{n}{map}\PYG{p}{(} \PYG{n}{convert\PYGZus{}id\PYGZus{}to\PYGZus{}name} \PYG{p}{)}
\PYG{n}{df2}\PYG{o}{.}\PYG{n}{target} \PYG{o}{=} \PYG{n}{df2}\PYG{o}{.}\PYG{n}{target}\PYG{o}{.}\PYG{n}{map}\PYG{p}{(} \PYG{n}{convert\PYGZus{}id\PYGZus{}to\PYGZus{}name} \PYG{p}{)}
\PYG{n}{df2}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
    source  target
0   Double     CCL
1  Feather    DN16
2  Feather    DN21
3     Fish    Beak
4     Fish  Bumper
\end{sphinxVerbatim}


\subsection{Python’s NetworkX module}
\label{\detokenize{chapter-15-networks:python-s-networkx-module}}
Your Anaconda installation came with the \sphinxcode{\sphinxupquote{networkx}} module for working with network data in Python.  If you have a non\sphinxhyphen{}Anaconda setup on your machine, or you plan to work in a cloud provider that doesn’t have it installed by default, you can install it with \sphinxcode{\sphinxupquote{pip install networkx}}.  The standard way to import it is using the abbreviation \sphinxcode{\sphinxupquote{nx}}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{networkx} \PYG{k}{as} \PYG{n+nn}{nx}
\end{sphinxVerbatim}

This module lets us turn tables of data (like the edge list for dolphins we just saw) into Python \sphinxcode{\sphinxupquote{Graph}} objects, which we can use for both computation and visualization.  The first step in creating a \sphinxcode{\sphinxupquote{Graph}} object is always the same; just call the \sphinxcode{\sphinxupquote{nx.Graph()}} function and it will create a new, empty graph with no vertices and no edges.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{dolphins} \PYG{o}{=} \PYG{n}{nx}\PYG{o}{.}\PYG{n}{Graph}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

We will now add vertices and edges to that graph.  Let’s start with the vertices.  Each NetworkX \sphinxcode{\sphinxupquote{Graph}} object lets you add vertices with the function \sphinxcode{\sphinxupquote{.add\_nodes\_from(your\_list)}}.  We will use that function to add all the dolphin names to our graph.  We can even check the size of the graph to be sure it worked.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{dolphins}\PYG{o}{.}\PYG{n}{add\PYGZus{}nodes\PYGZus{}from}\PYG{p}{(} \PYG{n}{df1}\PYG{o}{.}\PYG{n}{name} \PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} the column of all dolphin names}
\PYG{n+nb}{len}\PYG{p}{(} \PYG{n}{dolphins} \PYG{p}{)}                      \PYG{c+c1}{\PYGZsh{} how many nodes do we have now?}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
62
\end{sphinxVerbatim}

Similarly, we can add edges with the function \sphinxcode{\sphinxupquote{.add\_edges\_from(your\_list)}}, but the list must be a list of ordered pairs.  For instance, in our dolphin data case, we’d want it to be something like \sphinxcode{\sphinxupquote{{[}('Double','CCL'),('Feather','DN16'),('Feather','DN21'),...{]}}} and so on.  But we don’t want to have to type out the entire dolphin relationships table as ordered pairs; it’s too big!

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{len}\PYG{p}{(} \PYG{n}{df2} \PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
159
\end{sphinxVerbatim}

Fortunately, we can use the same trick we do when creating a dictionary from two columns.  Recall that \sphinxcode{\sphinxupquote{zip()}} takes two columns and converts them into a list of pairs; we often used this to create a dictionary with the trick \sphinxcode{\sphinxupquote{dict(zip(df.col1,df.col2))}}.  We can use it with \sphinxcode{\sphinxupquote{list()}} instead of \sphinxcode{\sphinxupquote{dict()}} to create a list of the ordered pairs rather than a dictionary.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{edges} \PYG{o}{=} \PYG{n+nb}{list}\PYG{p}{(} \PYG{n+nb}{zip}\PYG{p}{(} \PYG{n}{df2}\PYG{o}{.}\PYG{n}{source}\PYG{p}{,} \PYG{n}{df2}\PYG{o}{.}\PYG{n}{target} \PYG{p}{)} \PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} get the list of edges}
\PYG{n}{edges}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{10}\PYG{p}{]}                                     \PYG{c+c1}{\PYGZsh{} see if we did it right}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[(\PYGZsq{}Double\PYGZsq{}, \PYGZsq{}CCL\PYGZsq{}),
 (\PYGZsq{}Feather\PYGZsq{}, \PYGZsq{}DN16\PYGZsq{}),
 (\PYGZsq{}Feather\PYGZsq{}, \PYGZsq{}DN21\PYGZsq{}),
 (\PYGZsq{}Fish\PYGZsq{}, \PYGZsq{}Beak\PYGZsq{}),
 (\PYGZsq{}Fish\PYGZsq{}, \PYGZsq{}Bumper\PYGZsq{}),
 (\PYGZsq{}Gallatin\PYGZsq{}, \PYGZsq{}DN16\PYGZsq{}),
 (\PYGZsq{}Gallatin\PYGZsq{}, \PYGZsq{}DN21\PYGZsq{}),
 (\PYGZsq{}Gallatin\PYGZsq{}, \PYGZsq{}Feather\PYGZsq{}),
 (\PYGZsq{}Grin\PYGZsq{}, \PYGZsq{}Beak\PYGZsq{}),
 (\PYGZsq{}Grin\PYGZsq{}, \PYGZsq{}CCL\PYGZsq{})]
\end{sphinxVerbatim}

It looks like we did.  Let’s add these to the dolphin graph.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{dolphins}\PYG{o}{.}\PYG{n}{add\PYGZus{}edges\PYGZus{}from}\PYG{p}{(} \PYG{n}{edges} \PYG{p}{)}
\end{sphinxVerbatim}

We now have our dolphin data loaded into a NetworkX \sphinxcode{\sphinxupquote{Graph}} object.  This enables both computation and visualization, and we’ll consider each of those in its own section, below.


\section{Computations on graphs}
\label{\detokenize{chapter-15-networks:computations-on-graphs}}
There are a great many computations that can be done on graphs; we will only scratch the surface here.  You can learn more about graphs in MA267 at Bentley, and you can learn more about the capabilities of the NetworkX module through its documentation, \sphinxhref{https://networkx.github.io/documentation/}{here}.  But this section gives a few example computations that make sense for network data.

We can ask how dense the network is, which is a measure of what proportion of its possible connections it actually has.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{nx}\PYG{o}{.}\PYG{n}{density}\PYG{p}{(} \PYG{n}{dolphins} \PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
0.08408249603384453
\end{sphinxVerbatim}

Of all the possible social relationships among the dolphins (every possible pair that might hang out together), this network has only about 8.4\% of those connections.

The number of connections any one particular dolphin has is called the \sphinxstyleemphasis{degree} of that vertex.  We can ask for a histogram of the degrees across the network.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{nx}\PYG{o}{.}\PYG{n}{degree\PYGZus{}histogram}\PYG{p}{(} \PYG{n}{dolphins} \PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[0, 9, 6, 6, 5, 8, 8, 7, 4, 4, 2, 2, 1]
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{n}{degrees} \PYG{o}{=} \PYG{n}{nx}\PYG{o}{.}\PYG{n}{degree\PYGZus{}histogram}\PYG{p}{(} \PYG{n}{dolphins} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{bar}\PYG{p}{(} \PYG{n}{x}\PYG{o}{=}\PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{degrees}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{height}\PYG{o}{=}\PYG{n}{degrees} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Number of friends, x}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Number dolphins with x friends}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Degree histogram for dolphin network}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{chapter-15-networks_27_0}.png}

As you can see, no dolphin had zero friends, and thus we know that all dolphins appeared in some edge in the network.

We see that some dolphins had many more social associations.  If this were a network of humans, we might ask which people were the most influential in the social network.  There are many ways to measure influencce.  One way is by a notion called “betweenness,” which considers all the paths through which information might flow in a network, and asks which vertices are on the largest proportion of those paths.  This measure is called “betweenness centrality” and can be used to rank the vertices in a network by a measure of their importance.

Although it doesn’t make a lot of sense to measure this for dolphins (as opposed to humans), the code below illustrates how do to the computation.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{bc} \PYG{o}{=} \PYG{n}{nx}\PYG{o}{.}\PYG{n}{betweenness\PYGZus{}centrality}\PYG{p}{(} \PYG{n}{dolphins} \PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} this is a big dictionary}
\PYG{n}{bc} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{Series}\PYG{p}{(} \PYG{n}{bc} \PYG{p}{)}                        \PYG{c+c1}{\PYGZsh{} now it\PYGZsq{}s a pandas Series}
\PYG{n}{bc}\PYG{o}{.}\PYG{n}{sort\PYGZus{}values}\PYG{p}{(} \PYG{n}{ascending}\PYG{o}{=}\PYG{k+kc}{False} \PYG{p}{)}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}    \PYG{c+c1}{\PYGZsh{} let\PYGZsq{}s see the top values}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
SN100         0.248237
Beescratch    0.213324
SN9           0.143150
SN4           0.138570
DN63          0.118239
dtype: float64
\end{sphinxVerbatim}

Although the particular numbers don’t have units we can easily interpret, higher numbers are vertices that sit along a higher proportion of the network’s pathways.  There are many other ways to measure important nodes in a network; these are called centrality measures, and the full list of ways that NetworkX supports them appears \sphinxhref{https://networkx.github.io/documentation/stable/reference/algorithms/centrality.html}{here}.

Let us turn now to how we can visualize the dolphin network.


\section{Visualization of graphs}
\label{\detokenize{chapter-15-networks:visualization-of-graphs}}

\subsection{Drawing the dolphin network}
\label{\detokenize{chapter-15-networks:drawing-the-dolphin-network}}
The NetworkX module has a small number of graph\sphinxhyphen{}drawing features, but they will be sufficient for our needs here.  The simplest method is to just call \sphinxcode{\sphinxupquote{nx.draw( your\_graph )}}, but there are many options to help make it more attractive.  The simplest form of the dolphin network looks like this.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{nx}\PYG{o}{.}\PYG{n}{draw}\PYG{p}{(} \PYG{n}{dolphins} \PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{chapter-15-networks_32_0}.png}

While this shows us the general structure of the 62 dolphins involved, there’s a lot it doesn’t answer.  But first, let’s notice what we can from this picture.

On one side of the graph, we see a dense cluster of about 20\sphinxhyphen{}30 dolphins who seem very social, and interact with one another more than most other dolphins in the network.  There is a smaller cluster of about 10 or so on the other side that are also densely connected.  Other than that, most dolphins have relatively few social connections.  The dolphins in the center are an indirect social bridge between the two groups.  But which dolphins are they?  The vertices in the graph aren’t labeled.

The \sphinxcode{\sphinxupquote{nx.draw()}} function takes many optional parameters, and you can see them all \sphinxhref{https://networkx.github.io/documentation/stable/reference/generated/networkx.drawing.nx\_pylab.draw\_networkx.html}{here}.  In this case, we might want to label the vertices with the name of the dolphin, and increase the size of the figure.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{,}\PYG{l+m+mi}{10}\PYG{p}{)} \PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} 10in x 10in}
\PYG{n}{nx}\PYG{o}{.}\PYG{n}{draw}\PYG{p}{(} \PYG{n}{dolphins}\PYG{p}{,} \PYG{n}{with\PYGZus{}labels}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{font\PYGZus{}weight}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{bold}\PYG{l+s+s2}{\PYGZdq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{chapter-15-networks_34_0}.png}

Because it is often difficult to lay out a graph in an attractive way on a two\sphinxhyphen{}dimensional drawing, there is some random experimentation involved in most network drawing algorithms, including the one used by NetworkX.  So we see that the layout of the vertices is not exactly the same.  The two clusters of dolphins may not be laid out in the same locations or orientations in this graph and in the previous one.  In fact, every time I run the code, it looks a little different!


\subsection{Better drawing tools}
\label{\detokenize{chapter-15-networks:better-drawing-tools}}
NetworkX emphasizes that there are much more powerful graph\sphinxhyphen{}drawing software packages available.  For instance, you might download \sphinxhref{https://gephi.org/}{Gephi} or \sphinxhref{https://cytoscape.org/}{Cytoscape} if you need to make more aesthetically pleasing images from your network data.  To export your network from Python to that software, use the following code.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{nx}\PYG{o}{.}\PYG{n}{write\PYGZus{}graphml}\PYG{p}{(} \PYG{n}{dolphins}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{dolphins.graphml}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\end{sphinxVerbatim}

You can then import the \sphinxcode{\sphinxupquote{dolphins.graphml}} file into either of those other pieces of software to visualize it more conveniently.  Similarly, if you have data exported from either of those pieces of software that you want to bring into Python for use with NetworkX, you can use the \sphinxhref{https://networkx.github.io/documentation/networkx-1.10/reference/generated/networkx.readwrite.graphml.read\_graphml.html}{\sphinxcode{\sphinxupquote{nx.read\_graphml()}} function}.


\subsection{Drawing larger networks}
\label{\detokenize{chapter-15-networks:drawing-larger-networks}}
The dolphin network was fairly small (62 vertices) and fairly sparse (most dolphins socializing with only a few others).  But a larger or more dense network will be much harder to visualize, because there will be too many vertices or edges to draw in a way that a human can make sense of.  We will see an example of this in class when we consider the shipping data mentioned at the start of this chapter.  In that situation, we will find it useful to sort the connections in the network based on some information about them (like the amount shipped), and draw only the most important connections.


\section{Directed draphs in NetworkX}
\label{\detokenize{chapter-15-networks:directed-draphs-in-networkx}}
The beginning of this chapter distinguished directed graphs (like the friends network, where arrows went one way only) from undirected graphs (like the dolphins network, where each relationship was reciprocal).  To work with a directed graph in NetworkX, there are a few changes to what we learned above.

First, you create a directed graph not with \sphinxcode{\sphinxupquote{nx.Graph()}} but with \sphinxcode{\sphinxupquote{nx.DiGraph()}}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{friends} \PYG{o}{=} \PYG{n}{nx}\PYG{o}{.}\PYG{n}{DiGraph}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

But we can add vertices and edges exactly the same way as we did with the dolphins.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{friends}\PYG{o}{.}\PYG{n}{add\PYGZus{}nodes\PYGZus{}from}\PYG{p}{(} \PYG{p}{[} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Augustus}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Beatriz}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Cyrano}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Dauphine}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Englebert}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{]} \PYG{p}{)}
\PYG{n}{friends}\PYG{o}{.}\PYG{n}{add\PYGZus{}edges\PYGZus{}from}\PYG{p}{(} \PYG{p}{[}
    \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Augustus}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Cyrano}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}
    \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Beatriz}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Cyrano}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}
    \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Cyrano}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Beatriz}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}
    \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Cyrano}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Englebert}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}
    \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Dauphine}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Cyrano}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}
    \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Englebert}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Augustus}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}
    \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Englebert}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Beatriz}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{p}{]} \PYG{p}{)}
\end{sphinxVerbatim}

However, some computations make sense only in the context of a directed graph.  For instance, we can measure the \sphinxstyleemphasis{reciprocity} of a directed graph, which asks how many of its edges are two\sphinxhyphen{}directional.  In the friends case, only the Beatriz\(\to\)Cyrano connection is reciprocated (Cyrano\(\to\)Beatriz); all the others are one\sphinxhyphen{}directional.  So we expect a low proportion as the result.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{nx}\PYG{o}{.}\PYG{n}{reciprocity}\PYG{p}{(} \PYG{n}{friends} \PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
0.2857142857142857
\end{sphinxVerbatim}

There are seven edges in the network, and two of them are part of a reciprocated relationship, so the reciprocity is \(\frac27\approx0.285714\).

We can draw directed graphs using the same tools as we used for undirected graphs.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{nx}\PYG{o}{.}\PYG{n}{draw}\PYG{p}{(} \PYG{n}{friends}\PYG{p}{,} \PYG{n}{with\PYGZus{}labels}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{node\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{5000} \PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{chapter-15-networks_43_0}.png}

If you plan to use network data in your final project for this course and would like to learn more about the power of NetworkX, including both computations and visualizations, I recommend Chapter 8 of \sphinxhref{https://link.springer.com/book/10.1007/978-3-319-50017-1}{this book}.


\chapter{Relations as Matrices}
\label{\detokenize{chapter-16-matrices:relations-as-matrices}}\label{\detokenize{chapter-16-matrices::doc}}
See also the slides that summarize a portion of this content.

\sphinxstyleemphasis{Thanks to Jeff Leader’s chapter on Linear Algebra in \sphinxhref{https://ds4m.github.io/}{Data Science for Mathematicians} for the ideas and example described in this chapter.}


\section{Using matrices for relations other than networks}
\label{\detokenize{chapter-16-matrices:using-matrices-for-relations-other-than-networks}}
In {\hyperref[\detokenize{chapter-15-networks::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{chapter 15 of the notes}}}}, we discussed two ways to store network data.  We talked about a table of edges, which listed each connection in the network as its own row in the dataframe, and we talked about an adjacency matrix, which was a table of 0\sphinxhyphen{}or\sphinxhyphen{}1 entries indicating whether the row heading was connected to the column heading.

These same patterns can be used for data about other types of relations as well, not only networks.  Recall that a network is always a relation that connects a set to itself.  For instance, in the shipping network, both the row and column headings were the same set, the 50 U.S. states.


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|T|}
\hline


&\sphinxstyletheadfamily 
\sphinxstylestrong{MA}
&\sphinxstyletheadfamily 
\sphinxstylestrong{NY}
&\sphinxstyletheadfamily 
\sphinxstylestrong{CT}
&\sphinxstyletheadfamily 
\sphinxstylestrong{NH}
&\sphinxstyletheadfamily 
etc.
\\
\hline
\sphinxstylestrong{MA}
&
…
&
…
&
…
&
…
&

\\
\hline
\sphinxstylestrong{NY}
&
…
&
…
&
…
&
…
&

\\
\hline
\sphinxstylestrong{CT}
&
…
&
…
&
…
&
…
&

\\
\hline
\sphinxstylestrong{NY}
&
…
&
…
&
…
&
…
&

\\
\hline
etc.
&

&

&

&

&

\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

But we can create adjacency matrices that let us store other types of relations as well.  For example, let’s imagine we were doing a shipping network for the facilities owned by a single company, including both manufacturing and retail properties.  Let’s say we’re still tracking shipping, but only of newly manufactured products, which get shipped from manufacturing properties to retail properties, never the other way around.  Thus our factories would be the row headings and our retail outlets the column headings.


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|}
\hline


&\sphinxstyletheadfamily 
Store 1
&\sphinxstyletheadfamily 
Store 2
&\sphinxstyletheadfamily 
Store 3
&\sphinxstyletheadfamily 
etc.
\\
\hline
\sphinxstylestrong{Factory 1}
&
58
&
0
&
21
&
…
\\
\hline
\sphinxstylestrong{Factory 2}
&
19
&
35
&
5
&
…
\\
\hline
\sphinxstylestrong{Factory 3}
&
80
&
0
&
119
&
…
\\
\hline
etc.
&
…
&
…
&
…
&

\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

This is the format for an adjacency matrix for \sphinxstyleemphasis{any} kind of binary relation between any two sets.  Just as when we were dealing with network data, we can choose the data type that goes in the matrix.  If it is boolean (true/false or 0/1) then we are storing only whether an edge exists between the row heading and the column heading.  But if we store something more detailed (like the numeric values in the example above) then we have more information; in that case, it’s measuring the quantity of materials shipped from the source to the destination.

If we think of the data stored in an edge list instead, then with networks, the two columns come from the same set (both are lists of dolphins, or both are lists of U.S. states), but when we consider any kind of relation, then the two columns can be different sets.  In the example above, one column would be manufacturing locations and the other would be retail locations.


\section{Pivoting an edge list}
\label{\detokenize{chapter-16-matrices:pivoting-an-edge-list}}
Recall from {\hyperref[\detokenize{chapter-15-networks::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{the chapter 15 notes}}}} that if you have an edge list, you can turn it into an adjacency matrix with a single pivot command.  For instance, if we had the following edge list among a few factories and stores, we can create an adjacency matrix with the code shown.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}

\PYG{n}{edge\PYGZus{}list} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(} \PYG{p}{\PYGZob{}}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{From}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{:} \PYG{p}{[} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Factory 1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Factory 2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Factory 2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Factory 3}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{]}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{To}\PYG{l+s+s1}{\PYGZsq{}}   \PYG{p}{:} \PYG{p}{[} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Store 1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Store 1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Store 2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Store 2}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{]}
\PYG{p}{\PYGZcb{}} \PYG{p}{)}

\PYG{n}{edge\PYGZus{}list}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
        From       To
0  Factory 1  Store 1
1  Factory 2  Store 1
2  Factory 2  Store 2
3  Factory 3  Store 2
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{edge\PYGZus{}list}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Connected}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mi}{1}
\PYG{n}{matrix} \PYG{o}{=} \PYG{n}{edge\PYGZus{}list}\PYG{o}{.}\PYG{n}{pivot}\PYG{p}{(} \PYG{n}{index}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{From}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{To}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{values}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Connected}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{matrix}\PYG{o}{.}\PYG{n}{fillna}\PYG{p}{(} \PYG{l+m+mi}{0} \PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
To         Store 1  Store 2
From                       
Factory 1      1.0      0.0
Factory 2      1.0      1.0
Factory 3      0.0      1.0
\end{sphinxVerbatim}


\section{Recommender systems}
\label{\detokenize{chapter-16-matrices:recommender-systems}}
\begin{sphinxadmonition}{note}{Big Picture \sphinxhyphen{} What is a recommender system?}

A \sphinxstyleemphasis{recommender system} is an algorithm that can recommend to a customer a product they might like.  Amazon has had this feature (“Customers who bought this product also liked…”) since the early 2000s, and in the late 2000s, \sphinxhref{https://www.netflixprize.com/}{Netflix ran a \$1,000,000 prize} for creating the best movie recommender system.  In such a system, the input is some knowledge about a customer’s preferences about products, and the output should be a ranked list of products to recommend to that customer.  In Netflix’s case, it was movies, but it can be any set of products.
\end{sphinxadmonition}

To get a feeling for how this works, we’ll do a tiny example, as if Netflix were a tiny organization with 7 movies and 6 customers.  We’ll label the customers A,B,C,…,F and the movies G,H,I,…,M.  To make things more concrete, we’ll use the movies Godzilla, Hamlet, Ishtar, JFK, King Kong, Lincoln, and Macbeth.  (See the link at the top of this file for the original source of this example.)

We’ll assume that in this tiny movie preferences example, users indicate which movies they liked with a binary response (1 meaning they liked it, and 0 meaning they did not, which might mean disliked or didn’t watch or anything).  Let’s work with the following matrix of preferences.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}

\PYG{n}{prefs} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(} \PYG{p}{\PYGZob{}}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Godzilla}\PYG{l+s+s1}{\PYGZsq{}}  \PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Hamlet}\PYG{l+s+s1}{\PYGZsq{}}    \PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Ishtar}\PYG{l+s+s1}{\PYGZsq{}}    \PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{JFK}\PYG{l+s+s1}{\PYGZsq{}}       \PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{King Kong}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Lincoln}\PYG{l+s+s1}{\PYGZsq{}}   \PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Macbeth}\PYG{l+s+s1}{\PYGZsq{}}   \PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}
\PYG{p}{\PYGZcb{}}\PYG{p}{,} \PYG{n}{index}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{A}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{B}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{C}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{D}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{E}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{F}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{p}{)}

\PYG{n}{prefs}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
   Godzilla  Hamlet  Ishtar  JFK  King Kong  Lincoln  Macbeth
A         1       0       0    0          1        0        0
B         0       1       0    1          0        1        1
C         0       0       1    0          1        0        0
D         1       1       0    0          1        0        0
E         1       0       0    1          0        1        0
F         0       0       1    0          1        0        0
\end{sphinxVerbatim}

When a new user (let’s say user X) joins this tiny movie watching service, we will want to ask user X for their movie preferences, compare them to the preferences of existing users, and then use the similarities we find to recommend new movies.  Of course, normally this is done on a much larger scale; this is a tiny example.  (We will work with a much more realistically large example in class.)

Let’s imagine that user X joins the club and indicates that they like Godzilla, JFK, and Macbeth.  We represent user X’s preferences as a Pandas series that could be another row in the preferences DataFrame, if we chose to add it.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{X} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{Series}\PYG{p}{(} \PYG{p}{[} \PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{index}\PYG{o}{=}\PYG{n}{prefs}\PYG{o}{.}\PYG{n}{columns} \PYG{p}{)}
\PYG{n}{X}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Godzilla     1
Hamlet       0
Ishtar       0
JFK          1
King Kong    0
Lincoln      0
Macbeth      1
dtype: int64
\end{sphinxVerbatim}


\section{A tiny amount of linear algebra}
\label{\detokenize{chapter-16-matrices:a-tiny-amount-of-linear-algebra}}
There is an entire subject within mathematics that studies matrices and how to work with them.  Perhaps you have seen matrix multiplication in another course, either outside of Bentley or in MA239 (Linear Algebra) or a small amount in MA307 (Computer Graphics).  We cannot dive deeply into the mathematics of matrix operations here, but we will give a few key facts.

A pandas DataFrame is a grid of data, and when that grid contains only numbers, it can be referred to as a \sphinxstyleemphasis{matrix.}  A single pandas Series of numbers can be referred to as a \sphinxstyleemphasis{vector.}  These are the standard terms from linear algebra for grids and lists of numbers, respectively.  Throughout the rest of this chapter, because we’ll be dealing only with numerical data, I may say “matrix” or “DataFrame” to mean the same thing, and I may say “vector” or “pandas Series” to mean the same thing.

First, a matrix can be multiplied by another matrix or vector.  The important step for us here is the multiplication of the preferences matrix for all users with the preferences vector for user X.  Such a multiplication is a combination of the columns in the matrix, using the vector as the weights when combining.  In Python, the symbol for matrix multiplication is \sphinxcode{\sphinxupquote{@}}, so we can do the computation as follows.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{prefs} \PYG{o}{@} \PYG{n}{X}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
A    1
B    2
C    0
D    1
E    2
F    0
dtype: int64
\end{sphinxVerbatim}

Notice that this is indeed a combination of the columns of the preferences matrix, but it combined only the columns for the movies user X liked.  That is, you can think of the \sphinxcode{\sphinxupquote{X}} vector as saying, “I’ll take 1 copy of the Godzilla column, plus one copy of the JFK column, plus one copy of the Macbeth column.”  The result is a vector that tells us how user X compares to our existing set of users.  It seems user X is most similar to users B and E, sort of similar to users A and D, and not similar to users C or F.

Again, this notion of matrix\sphinxhyphen{}by\sphinxhyphen{}vector multiplication is part of a rich subject within mathematics, and we’re only dipping our toe in here.  To learn more, see the linear algebra course recommended above, MA239.


\section{Normalizing rows}
\label{\detokenize{chapter-16-matrices:normalizing-rows}}
There is a bit of a problem, however, with the method just described.  What if user A really liked movies, and clicked the “like” button very often, so that most of the first row of our matrix were ones instead of zeros?  Then no matter who user X was, they would probably get ranked as at least a little bit similar to user A.  In fact, everyone would.  This is probably not what we want, because it means that people who click the “like” button a lot will have their preferences dominating the movie recommendations.

So instead, we will scale each row of the preferences matrix down.  The standard way to do this begins by treating each one as a vector in \(n\)\sphinxhyphen{}dimensional space; in this case we have 7 columns, so we’re considering 7\sphinxhyphen{}dimensional space.  (Don’t try to picture it; nobody can.)  The length of any vector \((v_1,v_2,\ldots,v_n)\) is computed as \(\sqrt{v_1^2+v_2^2+\cdots+v_n^2}\), and this feature is built into numpy as the standard “linear algebra norm” for a vector.

For example, the length of user X’s vector is \(\sqrt{1^2+0^2+0^2+1^2+0^2+0^2+1^2}=\sqrt{3}\approx1.732\).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{linalg}\PYG{o}{.}\PYG{n}{norm}\PYG{p}{(} \PYG{n}{X} \PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
1.7320508075688772
\end{sphinxVerbatim}

Once we have the length (or norm) of a vector, we can divide the vector by that length to ensure that the vector’s new length is 1.  This makes all the vectors have the same length (or magnitude), and thus makes the preferences matrix more “fair,” because no one user gets to dominate it.  If you put in more likes, then each of your overall scores is reduced so that your ratings’ magnitude matches everyone else’s.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{normalized\PYGZus{}X} \PYG{o}{=} \PYG{n}{X} \PYG{o}{/} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linalg}\PYG{o}{.}\PYG{n}{norm}\PYG{p}{(} \PYG{n}{X} \PYG{p}{)}
\PYG{n}{normalized\PYGZus{}X}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Godzilla     0.57735
Hamlet       0.00000
Ishtar       0.00000
JFK          0.57735
King Kong    0.00000
Lincoln      0.00000
Macbeth      0.57735
dtype: float64
\end{sphinxVerbatim}

We can apply this to each row of our preferences matrix as follows.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{norms\PYGZus{}of\PYGZus{}rows} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linalg}\PYG{o}{.}\PYG{n}{norm}\PYG{p}{(} \PYG{n}{prefs}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1} \PYG{p}{)}
\PYG{n}{norms\PYGZus{}of\PYGZus{}rows}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
array([1.41421356, 2.        , 1.41421356, 1.73205081, 1.73205081,
       1.41421356])
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{normalized\PYGZus{}prefs} \PYG{o}{=} \PYG{n}{prefs}\PYG{o}{.}\PYG{n}{div}\PYG{p}{(} \PYG{n}{norms\PYGZus{}of\PYGZus{}rows}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0} \PYG{p}{)}
\PYG{n}{normalized\PYGZus{}prefs}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
   Godzilla   Hamlet    Ishtar      JFK  King Kong  Lincoln  Macbeth
A  0.707107  0.00000  0.000000  0.00000   0.707107  0.00000      0.0
B  0.000000  0.50000  0.000000  0.50000   0.000000  0.50000      0.5
C  0.000000  0.00000  0.707107  0.00000   0.707107  0.00000      0.0
D  0.577350  0.57735  0.000000  0.00000   0.577350  0.00000      0.0
E  0.577350  0.00000  0.000000  0.57735   0.000000  0.57735      0.0
F  0.000000  0.00000  0.707107  0.00000   0.707107  0.00000      0.0
\end{sphinxVerbatim}

So our updated similarity measurement that compares user X to all of our existing users is now the following.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{normalized\PYGZus{}prefs} \PYG{o}{@} \PYG{n}{normalized\PYGZus{}X}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
A    0.408248
B    0.577350
C    0.000000
D    0.333333
E    0.666667
F    0.000000
dtype: float64
\end{sphinxVerbatim}

We now have a clearer ranking of the users than we did before.  User X is most similar to E, then B, then A, then D, without any ties.


\section{Are we done?}
\label{\detokenize{chapter-16-matrices:are-we-done}}

\subsection{We could be done now!}
\label{\detokenize{chapter-16-matrices:we-could-be-done-now}}
At this point, we could stop and build a very simple recommender system.  We could simply suggest to user X all the movies that were rated highly by perhaps the top two on the “similar existing users” list we just generated, E and B.  (We might also filter out movies that user X already indicated that they had seen and liked.)  That’s a simple algorithm we could apply.  It would take just a few lines of code.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{liked\PYGZus{}by\PYGZus{}E} \PYG{o}{=} \PYG{n}{prefs}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{E}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{p}{:}\PYG{p}{]} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{0}
\PYG{n}{liked\PYGZus{}by\PYGZus{}B} \PYG{o}{=} \PYG{n}{prefs}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{B}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{p}{:}\PYG{p}{]} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{0}
\PYG{n}{liked\PYGZus{}by\PYGZus{}X} \PYG{o}{=} \PYG{n}{X} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{0}
\PYG{c+c1}{\PYGZsh{} liked by E or by B but not by X:}
\PYG{n}{recommend} \PYG{o}{=} \PYG{p}{(} \PYG{n}{liked\PYGZus{}by\PYGZus{}E} \PYG{o}{|} \PYG{n}{liked\PYGZus{}by\PYGZus{}B} \PYG{p}{)} \PYG{o}{\PYGZam{}} \PYG{o}{\PYGZti{}}\PYG{n}{liked\PYGZus{}by\PYGZus{}X}
\PYG{n}{recommend}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Godzilla     False
Hamlet        True
Ishtar       False
JFK          False
King Kong    False
Lincoln       True
Macbeth      False
dtype: bool
\end{sphinxVerbatim}

But there’s a big missed opportunity here.


\subsection{Why we approximate}
\label{\detokenize{chapter-16-matrices:why-we-approximate}}
Sometimes hidden in the pattern of user preferences is a general shape or structure of overall movie preferences.  For instance, we can clearly see that some of the movies in our library are biographies and others are monster movies.  Shouldn’t these themes somehow influence our recommendations?

Furthermore, what if there is a theme among moviegoers’ preferences that none of us as humans would notice in the data, or maybe not even have a name for, but that matters a lot to moviegoers?  Perhaps there’s a set of movies that combines suspense, comedy, and excitement in just the right amounts, and doesn’t have a specific word in our vocabulary, but it hits home for many viewers, and could be detected by examining their preferences?  Or maybe what a certain set of moviegoers has in common is the love of a particular director, actress, or soundtrack composer.  Any of these patterns should be detectable with enough data.

Now, in the tiny 6\sphinxhyphen{}by\sphinxhyphen{}7 matrix of preferences we have here, we’re not going to create any brilliant insights of that nature.  But with a very large database (like Netflix has), maybe we could.  How would we go about it?

There are techniques for \sphinxstyleemphasis{approximating a matrix.}  This may sound a little odd, because of course we have a specific matrix already (\sphinxcode{\sphinxupquote{normalized\_prefs}}) that we can use, so why bother making an approximation of it?  The reason is because we’re actually trying to bring out the big themes and ignore the tiny details.  We’d sort of like the computer to take a step back from the data and just squint a little until the details blur and only the big\sphinxhyphen{}picture patterns remain.

We’ll see an illustration of this in the next section.


\section{The Singular Value Decomposition}
\label{\detokenize{chapter-16-matrices:the-singular-value-decomposition}}

\subsection{Matrices as actions}
\label{\detokenize{chapter-16-matrices:matrices-as-actions}}
When we speak of multiplying a matrix by a vector, as we did in \sphinxcode{\sphinxupquote{prefs @ X}} and then later with \sphinxcode{\sphinxupquote{normalized\_prefs @ normalized\_X}}, we are using the matrix not just as a piece of data, but as an action we’re using on the vector.  In fact, if we think of matrix multiplication as a binary function, and we see ourselves as binding the matrix as the first argument to that function, then the result is actually a function (an action) we can take on vectors like \sphinxcode{\sphinxupquote{X}}.

I mentioned earlier that matrix multiplication also shows up in MA307, a Bentley course on the math of computer graphics.  This is because the action that results from multiplying a matrix by a vector is one that moves points through space in a way that’s useful in two\sphinxhyphen{} and three\sphinxhyphen{}dimensional computer graphics applications.


\subsection{The SVD}
\label{\detokenize{chapter-16-matrices:the-svd}}
The Singular Value Decomposition (SVD) is a way to break the action a matrix performs into three steps, each represented by a separate matrix.  Breaking up a matrix \(M\) produces three matrices, traditionally called \(U\), \(\Sigma\), and \(V\), that have a very special relationship.  First, multiplying \(U\Sigma V\) (or \sphinxcode{\sphinxupquote{U @ Σ @ V}} in Python) produces the original matrix \(M\).  Other than that fact, the \(U\) and \(V\) matrices are not important for us to discuss here, but the \(\Sigma\) matrix is.  That matrix has zeros everywhere but along its diagonal, and the numbers on the diagonal are all positive, and in decreasing order of importance.  Here is an example of what a \(\Sigma\) matrix might look like.
\begin{equation*}
\begin{split} \left[\begin{array}{cccc} 2.31 & 0 & 0 & 0 \\ 0 & 1.19 & 0 & 0 \\ 0 & 0 & 0.33 & 0 \\ 0 & 0 & 0 & 0.0021 \end{array}\right]\end{split}
\end{equation*}
In fact, because the \(\Sigma\) matrix is always diagonal, computations that produce \(U\), \(\Sigma\), and \(V\), typically provide \(\Sigma\) just as a list of the entries along the diagonal, rather than providing the whole matrix that’s mostly zeros.

Let’s see what these three matrices look like for our preferences matrix above.  We use the built\sphinxhyphen{}in NumPy routine called \sphinxcode{\sphinxupquote{svd}} to perform the SVD.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{U}\PYG{p}{,} \PYG{n}{Σ}\PYG{p}{,} \PYG{n}{V} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linalg}\PYG{o}{.}\PYG{n}{svd}\PYG{p}{(} \PYG{n}{normalized\PYGZus{}prefs} \PYG{p}{)}
\end{sphinxVerbatim}

Let’s ask what the shape of each resulting matrix is.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{U}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{,} \PYG{n}{Σ}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{,} \PYG{n}{V}\PYG{o}{.}\PYG{n}{shape}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
((6, 6), (6,), (7, 7))
\end{sphinxVerbatim}

We see that \(U\) is \(6\times6\), \(V\) is \(7\times 7\), and \(\Sigma\) is actually just of length 6, because it contains just the diagonal entries that belong in the \(\Sigma\) matrix.  These entries are called the \sphinxstyleemphasis{singular values.}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Σ}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
array([1.71057805e+00, 1.30272528e+00, 9.26401065e\PYGZhy{}01, 6.73800315e\PYGZhy{}01,
       2.54172767e\PYGZhy{}01, 1.83716538e\PYGZhy{}17])
\end{sphinxVerbatim}

In general, if our input matrix (in this case, \sphinxcode{\sphinxupquote{normalized\_prefs}}) is \(n\times m\) in size (that is, \(n\) rows and \(m\) columns), then \(U\) will be \(n\times n\), \(V\) will be \(m\times m\), and \(\Sigma\) will be \(n\times m\), but mostly zeros.  Let’s reconstruct a \(\Sigma\) matrix of the appropriate size from the singular values, then multiply \sphinxcode{\sphinxupquote{U @ Σ @ V}} to verify that it’s the same as the original \sphinxcode{\sphinxupquote{normalized\_prefs}} matrix.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Σ\PYGZus{}matrix} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(} \PYG{p}{(}\PYG{l+m+mi}{6}\PYG{p}{,} \PYG{l+m+mi}{7}\PYG{p}{)} \PYG{p}{)}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{fill\PYGZus{}diagonal}\PYG{p}{(} \PYG{n}{Σ\PYGZus{}matrix}\PYG{p}{,} \PYG{n}{Σ} \PYG{p}{)}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(} \PYG{n}{Σ\PYGZus{}matrix}\PYG{p}{,} \PYG{l+m+mi}{2} \PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} rounding makes a simpler printout}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
array([[1.71, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],
       [0.  , 1.3 , 0.  , 0.  , 0.  , 0.  , 0.  ],
       [0.  , 0.  , 0.93, 0.  , 0.  , 0.  , 0.  ],
       [0.  , 0.  , 0.  , 0.67, 0.  , 0.  , 0.  ],
       [0.  , 0.  , 0.  , 0.  , 0.25, 0.  , 0.  ],
       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]])
\end{sphinxVerbatim}

The sixth singular value is so tiny (about \(1.84\times10^{-17}\)) that it rounds to zero in the display above.

(Note: The rounding is \sphinxstyleemphasis{not} the approximation we’re seeking!  It’s something that just makes the printout easier to read.)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(} \PYG{n}{U} \PYG{o}{@} \PYG{n}{Σ\PYGZus{}matrix} \PYG{o}{@} \PYG{n}{V}\PYG{p}{,} \PYG{l+m+mi}{2} \PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
array([[ 0.71, \PYGZhy{}0.  ,  0.  , \PYGZhy{}0.  ,  0.71, \PYGZhy{}0.  , \PYGZhy{}0.  ],
       [ 0.  ,  0.5 , \PYGZhy{}0.  ,  0.5 , \PYGZhy{}0.  ,  0.5 ,  0.5 ],
       [\PYGZhy{}0.  , \PYGZhy{}0.  ,  0.71, \PYGZhy{}0.  ,  0.71, \PYGZhy{}0.  , \PYGZhy{}0.  ],
       [ 0.58,  0.58,  0.  ,  0.  ,  0.58,  0.  ,  0.  ],
       [ 0.58,  0.  , \PYGZhy{}0.  ,  0.58, \PYGZhy{}0.  ,  0.58,  0.  ],
       [\PYGZhy{}0.  , \PYGZhy{}0.  ,  0.71, \PYGZhy{}0.  ,  0.71, \PYGZhy{}0.  , \PYGZhy{}0.  ]])
\end{sphinxVerbatim}

if we look above at our \sphinxcode{\sphinxupquote{normalized\_prefs}} matrix, we see that this is indeed a match.


\subsection{Creating an approximation}
\label{\detokenize{chapter-16-matrices:creating-an-approximation}}
Recall that the reason we embarked upon the SVD exploration was to find a way to approximate a matrix.  Because the singular values are arranged in decreasing order, you can imagine that the matrix \(U\Sigma V\) wouldn’t change very much if just the smallest of them were replaced with a zero.  After all, \(1.84\times10^{-17}\) is almost zero anyway!

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{almost\PYGZus{}Σ} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(} \PYG{n}{Σ} \PYG{p}{)}
\PYG{n}{almost\PYGZus{}Σ}\PYG{p}{[}\PYG{l+m+mi}{5}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mi}{0}
\PYG{n}{almost\PYGZus{}Σ\PYGZus{}matrix} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(} \PYG{p}{(}\PYG{l+m+mi}{6}\PYG{p}{,} \PYG{l+m+mi}{7}\PYG{p}{)} \PYG{p}{)}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{fill\PYGZus{}diagonal}\PYG{p}{(} \PYG{n}{almost\PYGZus{}Σ\PYGZus{}matrix}\PYG{p}{,} \PYG{n}{almost\PYGZus{}Σ} \PYG{p}{)}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(} \PYG{n}{U} \PYG{o}{@} \PYG{n}{almost\PYGZus{}Σ\PYGZus{}matrix} \PYG{o}{@} \PYG{n}{V}\PYG{p}{,} \PYG{l+m+mi}{2} \PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
array([[ 0.71, \PYGZhy{}0.  ,  0.  , \PYGZhy{}0.  ,  0.71, \PYGZhy{}0.  , \PYGZhy{}0.  ],
       [ 0.  ,  0.5 , \PYGZhy{}0.  ,  0.5 , \PYGZhy{}0.  ,  0.5 ,  0.5 ],
       [\PYGZhy{}0.  , \PYGZhy{}0.  ,  0.71, \PYGZhy{}0.  ,  0.71, \PYGZhy{}0.  , \PYGZhy{}0.  ],
       [ 0.58,  0.58,  0.  ,  0.  ,  0.58,  0.  ,  0.  ],
       [ 0.58,  0.  , \PYGZhy{}0.  ,  0.58, \PYGZhy{}0.  ,  0.58,  0.  ],
       [\PYGZhy{}0.  , \PYGZhy{}0.  ,  0.71, \PYGZhy{}0.  ,  0.71, \PYGZhy{}0.  , \PYGZhy{}0.  ]])
\end{sphinxVerbatim}

Indeed, this looks exactly the same when we round to two decimal places.  And yet, it is still an approximation to the original, because we did make a (tiny) change.  What if we changed even more?  Let’s replace the next\sphinxhyphen{}smallest singular value (the 0.25) with zero.  This will be a bigger change.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{almost\PYGZus{}Σ}\PYG{p}{[}\PYG{l+m+mi}{4}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mi}{0}
\PYG{n}{almost\PYGZus{}Σ\PYGZus{}matrix} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(} \PYG{p}{(}\PYG{l+m+mi}{6}\PYG{p}{,} \PYG{l+m+mi}{7}\PYG{p}{)} \PYG{p}{)}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{fill\PYGZus{}diagonal}\PYG{p}{(} \PYG{n}{almost\PYGZus{}Σ\PYGZus{}matrix}\PYG{p}{,} \PYG{n}{almost\PYGZus{}Σ} \PYG{p}{)}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(} \PYG{n}{U} \PYG{o}{@} \PYG{n}{almost\PYGZus{}Σ\PYGZus{}matrix} \PYG{o}{@} \PYG{n}{V}\PYG{p}{,} \PYG{l+m+mi}{2} \PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
array([[ 0.72,  0.08,  0.06,  0.01,  0.65,  0.01, \PYGZhy{}0.13],
       [ 0.01,  0.55,  0.03,  0.5 , \PYGZhy{}0.03,  0.5 ,  0.43],
       [\PYGZhy{}0.  , \PYGZhy{}0.01,  0.7 , \PYGZhy{}0.  ,  0.71, \PYGZhy{}0.  ,  0.01],
       [ 0.57,  0.51, \PYGZhy{}0.05, \PYGZhy{}0.  ,  0.62, \PYGZhy{}0.  ,  0.1 ],
       [ 0.57, \PYGZhy{}0.04, \PYGZhy{}0.03,  0.57,  0.03,  0.57,  0.06],
       [\PYGZhy{}0.  , \PYGZhy{}0.01,  0.7 , \PYGZhy{}0.  ,  0.71, \PYGZhy{}0.  ,  0.01]])
\end{sphinxVerbatim}

Now we can start to see some small changes.  Values that used to be zero are now approximately zero, such as 0.06 or \sphinxhyphen{}0.01.  An 0.71 became 0.72, and so on.  We have created a fuzzier (less precise) approximation to the original \sphinxcode{\sphinxupquote{normalized\_prefs}} matrix.

We could repeat this, removing more and more of the singular values in \(\Sigma\), until the resulting array were all zeros.  Obviously that final state would be a pretty bad approximation to the original matrix!  But by this method, we can choose how precise an approximation we want.  Here are our options:


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\sphinxstyletheadfamily 
Remove this many singular values
&\sphinxstyletheadfamily 
And get this kind of approximation
\\
\hline
0 (don’t remove any)
&
Original matrix (not an approximation)
\\
\hline
1
&
Identical up to at least 2 decimals
\\
\hline
2
&
Fairly close, as shown above
\\
\hline
3
&
Less faithful
\\
\hline
4
&
Even less faithful
\\
\hline
5
&
Even worse
\\
\hline
6 (remove all)
&
All zeros, terrible approximation
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}


\subsection{Measuring the quality of the approximation}
\label{\detokenize{chapter-16-matrices:measuring-the-quality-of-the-approximation}}
There is a measurement called \(\rho\) (Greek letter rho) that can let you know approximately how much of the “energy” of the original matrix is being lost with an approximation.  If \(\Sigma\) is the original vector of singular values and \(z\) is the vector of those that will be replaced by zeros, then \(\rho^2\) is computed by dividing the magnitude of \(z\) by the magnitude of \(\Sigma\).  And so \(\rho\) is the square root of that number.  You can see \(\rho\) as a measurement of the error introduced by the approximation, between 0.0 and 1.0.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{ρ} \PYG{p}{(} \PYG{n}{Σ}\PYG{p}{,} \PYG{n}{num\PYGZus{}to\PYGZus{}remove} \PYG{p}{)}\PYG{p}{:}
    \PYG{n}{z} \PYG{o}{=} \PYG{n}{Σ}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{num\PYGZus{}to\PYGZus{}remove}\PYG{p}{:}\PYG{p}{]}
    \PYG{k}{if} \PYG{n+nb}{len}\PYG{p}{(} \PYG{n}{z} \PYG{p}{)} \PYG{o}{==} \PYG{l+m+mi}{0}\PYG{p}{:}
        \PYG{k}{return} \PYG{l+m+mf}{1.0}
    \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linalg}\PYG{o}{.}\PYG{n}{norm}\PYG{p}{(} \PYG{n}{z} \PYG{p}{)} \PYG{o}{/} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linalg}\PYG{o}{.}\PYG{n}{norm}\PYG{p}{(} \PYG{n}{Σ} \PYG{p}{)} \PYG{p}{)}

\PYG{n}{ρ}\PYG{p}{(} \PYG{n}{Σ}\PYG{p}{,} \PYG{l+m+mi}{1} \PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
2.7386486204421176e\PYGZhy{}09
\end{sphinxVerbatim}

We see that \(\rho\) says the error is tiny if we replace only the last singular value with zero, because that value was almost zero anyway.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ρ}\PYG{p}{(} \PYG{n}{Σ}\PYG{p}{,} \PYG{l+m+mi}{2} \PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
0.32212667929693084
\end{sphinxVerbatim}

But the error is larger if we remove two singular values, because the second\sphinxhyphen{}lowest one was not already near to zero.


\subsection{Visualizing the approximations}
\label{\detokenize{chapter-16-matrices:visualizing-the-approximations}}
Let’s take a step back from our particular example for a moment, to consider what these SVD\sphinxhyphen{}based approximations look like in general.  I chose a \(10\times10\) matrix of random values and plotted it as a grid of colors, shown here.

\sphinxincludegraphics{{svd-step-0}.png}

What would it look like to approximate this matrix by dropping 1, 2, 3, or more of its singular values?  We can visualize all of the answers at once, and compute the \(\rho\) value for each as well.

In the picture below, we use \(i\) to denote the number of singular values we are dropping to create an approximation of the matrix.  So on the top left, when \(i=0\), we have the original matrix unchanged.  But on the top right, when \(i=10\), we’ve dropped all the singular values and our matrix is just all zeros, obviously containing little or no information.  You can see how the matrix blurs slowly from its original form to a complete lack of structure as \(i\) increases from 1 to 10.

\sphinxincludegraphics{{svd-10-steps}.png}

The bottom row shows the difference between the original matrix and the approximation.  On the bottom left, because the “approximation” equals the original, the difference is a matrix of zeros, so the the picture shown is a single color.  On the bottom right, because the approximation is all zeros, the difference is the original matrix!  As \(i\) increases and the approximations get blurrier, the error matrix grows more distinct, and you can see how \(\rho\) grows with it, measuring its importance.

\begin{sphinxadmonition}{note}{Big Picture \sphinxhyphen{} The SVD and approximation}

The singular value decomposition of a matrix lets us know which portions of the matrix are the most structurally important.  If we drop just a few of the least significant singular values, then reconstruct the matrix from what’s left, we arrive at an approximation of the original.  This has many uses, one of which is the detection of patterns within a matrix of data, as in this chapter.
\end{sphinxadmonition}


\subsection{Choosing which approximation to use}
\label{\detokenize{chapter-16-matrices:choosing-which-approximation-to-use}}
Now that we have a sense of what SVD\sphinxhyphen{}based approximations do in general, let’s return to our particular example.  What are the various values of \(\rho\) for the approximations we might choose to approximate the preferences matrix?

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{[} \PYG{n}{ρ}\PYG{p}{(} \PYG{n}{Σ}\PYG{p}{,} \PYG{n}{i} \PYG{p}{)} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{Σ}\PYG{p}{)}\PYG{o}{+}\PYG{l+m+mi}{1} \PYG{p}{)} \PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[1.0,
 2.7386486204421176e\PYGZhy{}09,
 0.32212667929693084,
 0.5422162727569352,
 0.6921213328320457,
 0.8460293400488615,
 1.0]
\end{sphinxVerbatim}

If we’re trying to keep most of the meaning of the original matrix, we’ll want to remove only 1 or 2 singular values.  For this example, let’s choose to remove three, which is close to 50\% of the “energy” of the original preferences matrix.  (In a real application, you would perform tests on past data to measure which is the best choice, but let’s keep this example simple.)

As a refresher for how to remove the lowest 3 singular values, here’s the code all in one place.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{almost\PYGZus{}Σ} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(} \PYG{n}{Σ} \PYG{p}{)}
\PYG{n}{almost\PYGZus{}Σ}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{3}\PYG{p}{:}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mi}{0}  \PYG{c+c1}{\PYGZsh{} replace the final 3 singular values with zeros}

\PYG{n}{almost\PYGZus{}Σ\PYGZus{}matrix} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(} \PYG{p}{(}\PYG{l+m+mi}{6}\PYG{p}{,} \PYG{l+m+mi}{7}\PYG{p}{)} \PYG{p}{)}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{fill\PYGZus{}diagonal}\PYG{p}{(} \PYG{n}{almost\PYGZus{}Σ\PYGZus{}matrix}\PYG{p}{,} \PYG{n}{almost\PYGZus{}Σ} \PYG{p}{)}

\PYG{n}{approx\PYGZus{}prefs} \PYG{o}{=} \PYG{n}{U} \PYG{o}{@} \PYG{n}{almost\PYGZus{}Σ\PYGZus{}matrix} \PYG{o}{@} \PYG{n}{V}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(} \PYG{n}{approx\PYGZus{}prefs}\PYG{p}{,} \PYG{l+m+mi}{2} \PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
array([[ 0.7 ,  0.16,  0.04, \PYGZhy{}0.02,  0.66, \PYGZhy{}0.02, \PYGZhy{}0.09],
       [ 0.08,  0.32,  0.09,  0.58, \PYGZhy{}0.06,  0.58,  0.34],
       [\PYGZhy{}0.02,  0.05,  0.69, \PYGZhy{}0.02,  0.72, \PYGZhy{}0.02,  0.03],
       [ 0.66,  0.21,  0.03,  0.1 ,  0.58,  0.1 , \PYGZhy{}0.02],
       [ 0.45,  0.32, \PYGZhy{}0.13,  0.45,  0.08,  0.45,  0.21],
       [\PYGZhy{}0.02,  0.05,  0.69, \PYGZhy{}0.02,  0.72, \PYGZhy{}0.02,  0.03]])
\end{sphinxVerbatim}

Note that the rounding to two decimal places is \sphinxstyleemphasis{not} part of the approximation we created.  We’re rounding it after the fact to make the display more readable.

If we’d like it to have the same table structure it had before, we can convert it into a DataFrame and assign row and column headers.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{approx\PYGZus{}prefs} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(} \PYG{n}{approx\PYGZus{}prefs} \PYG{p}{)}
\PYG{n}{approx\PYGZus{}prefs}\PYG{o}{.}\PYG{n}{index} \PYG{o}{=} \PYG{n}{prefs}\PYG{o}{.}\PYG{n}{index}
\PYG{n}{approx\PYGZus{}prefs}\PYG{o}{.}\PYG{n}{columns} \PYG{o}{=} \PYG{n}{prefs}\PYG{o}{.}\PYG{n}{columns}
\PYG{n}{approx\PYGZus{}prefs}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
   Godzilla    Hamlet    Ishtar       JFK  King Kong   Lincoln   Macbeth
A  0.696420  0.160650  0.039410 \PYGZhy{}0.021090   0.662098 \PYGZhy{}0.021090 \PYGZhy{}0.094822
B  0.080627  0.318500  0.093488  0.579816  \PYGZhy{}0.063906  0.579816  0.341795
C \PYGZhy{}0.018735  0.046549  0.687042 \PYGZhy{}0.018883   0.720243 \PYGZhy{}0.018883  0.033053
D  0.662668  0.211608  0.033058  0.097823   0.577728  0.097823 \PYGZhy{}0.020174
E  0.453101  0.324129 \PYGZhy{}0.127217  0.450933   0.081084  0.450933  0.206132
F \PYGZhy{}0.018735  0.046549  0.687042 \PYGZhy{}0.018883   0.720243 \PYGZhy{}0.018883  0.033053
\end{sphinxVerbatim}


\section{Applying our approximation}
\label{\detokenize{chapter-16-matrices:applying-our-approximation}}
Now we have an approximation to the user preference matrix.  We hope it has brought out some of the latent relationships hiding in the data, although with an example this small, who can say?  It’s unlikely, but this same technique applies much more sensibly in larger examples, one of which we’ll do in our next class meeting.

To find which users match up best, according to this approximate preference matrix, with our new user X, we do the same multiplication as before, but now with the approximate matrix.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{approx\PYGZus{}prefs} \PYG{o}{@} \PYG{n}{normalized\PYGZus{}X}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
A    0.335156
B    0.578642
C   \PYGZhy{}0.002636
D    0.427422
E    0.640955
F   \PYGZhy{}0.002636
dtype: float64
\end{sphinxVerbatim}

It seems that users E and B have retained the highest similarities to user X in this example.  But user D has a pretty high rank as well, so let’s include them also, just for variety.

Also, rather than just listing all the movies they like and that user X doesn’t, let’s try to be smarter about that as well.  Couldn’t we rank the recommendations?  Let’s take the E, B, and D rows from the approximate preferences matrix and add them together to combine an aggregate preferences vector for all movies.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{rows\PYGZus{}for\PYGZus{}similar\PYGZus{}users} \PYG{o}{=} \PYG{n}{approx\PYGZus{}prefs}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{E}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{B}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{D}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}\PYG{p}{:}\PYG{p}{]}
\PYG{n}{scores} \PYG{o}{=} \PYG{n}{rows\PYGZus{}for\PYGZus{}similar\PYGZus{}users}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{scores}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Godzilla     1.196396
Hamlet       0.854238
Ishtar      \PYGZhy{}0.000671
JFK          1.128572
King Kong    0.594907
Lincoln      1.128572
Macbeth      0.527753
dtype: float64
\end{sphinxVerbatim}

Now which of these movies has user X not yet indicated they like?

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{scores}\PYG{p}{[}\PYG{o}{\PYGZti{}}\PYG{n}{liked\PYGZus{}by\PYGZus{}X}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Hamlet       0.854238
Ishtar      \PYGZhy{}0.000671
King Kong    0.594907
Lincoln      1.128572
dtype: float64
\end{sphinxVerbatim}

All we need to do is rank them and we have our recommendation list!

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{scores}\PYG{p}{[}\PYG{o}{\PYGZti{}}\PYG{n}{liked\PYGZus{}by\PYGZus{}X}\PYG{p}{]}\PYG{o}{.}\PYG{n}{sort\PYGZus{}values}\PYG{p}{(} \PYG{n}{ascending}\PYG{o}{=}\PYG{k+kc}{False} \PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Lincoln      1.128572
Hamlet       0.854238
King Kong    0.594907
Ishtar      \PYGZhy{}0.000671
dtype: float64
\end{sphinxVerbatim}

Of course, we don’t want to recommend all of these movies; there’s even a negative score for one of them!  How many recommendations are passed on to the user is a question best determined by the designers of the user experience.  Perhaps in this case we’d recommend Lincoln and Hamlet.


\section{Conclusion}
\label{\detokenize{chapter-16-matrices:conclusion}}
In class, we will apply this same technique to an actual database of song recommendations from millions of users.  Be sure to download and prepare the data as part of \sphinxhref{course-schedule\#week-10-11-5-2020-relations-graphs-and-networks}{the homework assigned this week}.

If you want to know more about the concepts of matrix multiplication and factorization, which were covered only extremely briefly in this chapter, consider taking MA239, Linear Algebra.


\chapter{Introduction to Machine Learning}
\label{\detokenize{chapter-17-machine-learning:introduction-to-machine-learning}}\label{\detokenize{chapter-17-machine-learning::doc}}
See also the slides that summarize a portion of this content.

While Bentley University does not currently have an undergraduate course in Machine Learning, there are several related courses currently available.  MA347 (Data Mining) covers topics related to machine learning, but not exactly the same; both are advanced math and stats implemented in a programming environment, but with different focuses.

Machine learning is also closely connected to mathematical modeling, and we have statistics courses that cover modeling, especially MA252 (Regression Analysis), MA315 (Mathematical Modeling with VBA in Excel), and MA380 (Introduction to Generalized Linear Models and Survival Analysis in Business).  And if you are planning to stay at Bentley for graduate school, there is a machine learning course at the graduate level, MA707 (Introduction to Machine Learning), and a somewhat related course CS733 (AI Techniques and Applications).

Today’s notes are a small preview of the kind of material that appears in a machine learning course.


\section{Supervised and unsupervised learning}
\label{\detokenize{chapter-17-machine-learning:supervised-and-unsupervised-learning}}
\begin{sphinxadmonition}{note}{Big Picture \sphinxhyphen{} Supervised vs. unsupervised machine learning}

Machine learning is broken into two categories, supervised and unsupervised.  The definitions for these are below.
\end{sphinxadmonition}

\sphinxstylestrong{Supervised learning} provides to the computer a dataset of inputs and their corresponding outputs, and asks the computer to learn something about the relationships between the inputs and the outputs.  One of the most commonly used examples is to provide small photographs of handwritten digits as the inputs (like those shown below, \sphinxhref{https://en.wikipedia.org/wiki/MNIST\_database\#/media/File:MnistExamples.png}{from this source}) and make the corresponding outputs the integer represented.  For instance, for the top left input shown below, the output would be the integer 0.

\sphinxincludegraphics{{MnistExamples}.png}

This is called supervised learning because the data scientist is providing the outputs that the computer \sphinxstyleemphasis{should} be giving for each input.  It is as if the data scientist is looking over the computer’s shoulder, teaching it what kinds of outputs it should learn to create.  A mathematical model trained on a large enough set of inputs and outputs like those can learn to recognize handwritten digits with a high degree of accuracy.  The most common technique of doing so is with neural networks and deep learning, topics covered in MA707.

\sphinxstylestrong{Unsupervised learning} provides to the computer a dataset, but does not break it into input\sphinxhyphen{}output pairs.  Rather, the data scientist asks the computer to detect some kind of structure within the data.  One example of this is cluster analysis, covered in MA347, but you saw another example in {\hyperref[\detokenize{chapter-16-matrices::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{the Chapter 16 notes}}}}.  When we used SVD to approximate a network, thus revealing more of its latent structure than the precise data itself revealed, we were having the computer do unsupervised learning.  Another example of unsupervised learning is principal components analysis (PCA), covered in many statistics courses.

Today, we will focus on supervised learning.  For this reason, when we look at data, we will designate one column as the output that we want the computer to learn to predict from all the other columns as inputs.  The terminology for inputs and output varies:
\begin{itemize}
\item {} 
Computer science typically speaks of the \sphinxstyleemphasis{inputs} and the \sphinxstyleemphasis{output.}

\item {} 
Machine learning typically speaks of the \sphinxstyleemphasis{features} and the \sphinxstyleemphasis{target.}

\item {} 
Statistics typically speaks of the \sphinxstyleemphasis{predictors} and the \sphinxstyleemphasis{respoonse.}

\end{itemize}

I may use any of these terms in this chapter and in class; you should become familiar with the fact that they are synonyms for one another.  Although mathematics has its own terms for inputs and outputs (like domain and range) these are not used to refer to specific columns of a dataset, so I don’t include them on the list above.  \sphinxstylestrong{Most of machine learning is supervised, and we will focus on supervised learning exclusively in this chapter.}


\section{Seen and unseen data}
\label{\detokenize{chapter-17-machine-learning:seen-and-unseen-data}}
\begin{sphinxadmonition}{note}{Big Picture \sphinxhyphen{} A central issue: overfitting vs. underfitting}

Probably the most significant concern in mathematical modeling in general (and machine learning in particular) is \sphinxstyleemphasis{overfitting} vs. \sphinxstyleemphasis{underfitting} for a mathematical model, sometimes also called \sphinxstyleemphasis{bias} vs. \sphinxstyleemphasis{variance.}  We explore its meaning in this section, and we will find that it is intimately tied up with how mathematical models perform on unseen data.
\end{sphinxadmonition}


\subsection{What is a mathematical model?}
\label{\detokenize{chapter-17-machine-learning:what-is-a-mathematical-model}}
Since overfitting and underfitting are concepts that apply to mathematical models, let’s ensure that we have a common definition for a mathematical model.  Just as a model airplane is an imitation of a real airplane, and just as the \sphinxhref{https://unausa.org/model-un/}{Model UN} is an imitation of the actual United Nations, a mathematical model is an imitation of reality.  But while a model airplane is built of plastic and the Model UN is built of students, a mathematical model is built of mathematics, such as equations, algorithms, or formulas.  Like any model, a mathematical model does not perfectly represent the real thing, but we aim to make models that are good enough to be useful.

A mathematical model that you’re probably familiar with is the position of a falling object over time, introduced in every introductory physics class.  It’s written as \(s(t)=\frac12gt^2+v_0t+s_0\), where \(t\) is time, \(g\) is the acceleration due to gravity, \(v_0\) is the initial velocity, and \(s_0\) the initial position.  This is very accurate for experiments that happen in the small laboratories we encounter in physics classes, but it becomes inaccurate if we consider, for example, a skydiver.  Even before deploying a parachute, the person’s descent is significantly impacted by air resistance, and dramatically more so after deploying the parachute, but the simple model just given doesn’t include air resistance.  So it’s a good model, but not a perfect model.  All mathematical models of real world phenomena are imperfect; we just try to make good ones.


\subsection{Models built on data}
\label{\detokenize{chapter-17-machine-learning:models-built-on-data}}
Physicists, however, have it easy, in the sense that physical phenomena tend to follow simple mathematical laws.  In fact, the rule for falling objects just given in the previous paragraph is so simple that students who have completed only Algebra I can understand it; no advanced degree in mathematics required!  Such simple patterns are easy to spot in experimental data.

Data science, however, is typically applied to more complex systems, such as economies, markets, sports, medicine, and so on, where simple patterns aren’t always the rule.  In fact, when we see a dataset and try to create a mathematical model (say, a formula) that describes it well, it won’t always be obvious when we’ve done a good job.  A physicist can often go and get more data through an experiment, but a data scientist may not be able to do so; sometimes one dataset is all you have.  Is it enough data to validate when we’ve made a good model?

That raises the question:  \sphinxstyleemphasis{What is a good model?}  The answer is that a good model is one that is reliable enough to be useful, often for prediction.  For example, if we write a formula that predicts the expected increase in sales that will come from a given amount of marketing spending in a certain channel, we’ll want to use that to consider possible future scenarios when making strategic decisions.  It’s a good model if it’s reliable enough to make decent predictions about the future (with some uncertainty of course).

In that example, the formula for sales based on marketing spending would have been built from some past experience (\sphinxstyleemphasis{seen data,} that is, data we’ve actually seen, in the past).  But we’re using it to predict something that could happen in the future, asking “what if we spent this much?”  We’re hoping the model will still be good on \sphinxstyleemphasis{unseen data,} that is, inputs to the model that we haven’t yet seen happen.

Consider another example.  When researchers work on developing self\sphinxhyphen{}driving cars, they gather lots of data from cameras and sensors in actual vehicles, and train their algorithms to make the correct decisions in all of those situations.  But of course, if self\sphinxhyphen{}driving cars are ever to succeed, the models the researchers create will need to work correctly on new, unseen data as well–that is, the new camera and sensor inputs the system experiences when it’s actually driving a car around the real world.  The model will be built using known/seen data, but it has to work well also on unkown, or not\sphinxhyphen{}yet\sphinxhyphen{}seen, data.


\subsection{Overfitting and underfitting}
\label{\detokenize{chapter-17-machine-learning:overfitting-and-underfitting}}
This brings us to the big dilemma introduced above.  There are two big mistakes that a data scientist can make when fitting a model to existing data.
\begin{itemize}
\item {} 
The data scientist could make a model that tailors itself to every detail of the known data precisely.
\begin{itemize}
\item {} 
This is called \sphinxstyleemphasis{overfitting,} because the model is too much dependent on the peculiarities of that one dataset, and so it won’t behave well on new data.

\item {} 
It typically happens if the model is too complex and/or customized to the data.

\item {} 
It is also called \sphinxstyleemphasis{variance,} because the model follows too much the tiny variations of the dataset, rather than just its underlying structure.

\end{itemize}

\item {} 
The data scientist could make a model that captures only very simple characteristics of the known data and ignores some important details.
\begin{itemize}
\item {} 
This is called \sphinxstyleemphasis{underfitting,} because the model is too simple, and missed some signals that the data scientist could have learned from the known data.

\item {} 
It typically happens if the model is not complex enough.

\item {} 
It is also called \sphinxstyleemphasis{bias,} because just as a social bias may pigeonhole a complex person into a simple stereotype, making the decision to use too simple a mathematical model also pigeonholes a complex problem into a simple stereotype, and limits the values of the results.

\end{itemize}

\end{itemize}

So we have a spectrum, from simple models to complex models, and there’s some happy medium in between.  Finding that happy medium is the job of a mathematical modeler.

This issue is intimiately related to the common terms of signal and noise, so it’s worth exploring this important issue from that viewpoint as well.


\subsection{Signal and noise}
\label{\detokenize{chapter-17-machine-learning:signal-and-noise}}
Surely, we’ve all seen a movie in which something like this happens:  An astronaut is trying to radio back to earth, but the voice they’re hearing is mostly static and crackling, with only some glimpses of actual words coming through.  The words are the \sphinxstyleemphasis{signal} the astronaut is hoping to hear and the crackling static is the \sphinxstyleemphasis{noise} on the line preventing the signal from coming through clearly.  Although these are terms with roots in engineering and the hard sciences, they are common metaphors in statistics and data work as well.  One famous modern example of their use in that sphere is the title of Nate Silver’s popular and award\sphinxhyphen{}winning 2012 book, \sphinxhref{https://www.amazon.com/Signal-Noise-Many-Predictions-Fail-but/dp/0143125087}{The Signal and the Noise}.  Let’s use the pictures below to see why Silver used these terms to talk about statistics.

\sphinxincludegraphics{{signal-and-noise}.png}

Let’s imagine for a moment a simple scenario with one input variable and one output variable, such as the example earlier of marketing spend in a particular channel vs. expected sales increase.
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
If we could see with perfect clarity how the world worked, we would know exactly how customers respond to our marketing spending.  This omniscient knowledge about marketing nobody has, but we can imagine that it exists somewhere, even if no one knows it (except God).  That knowledge is the signal that we are trying to detect.  It’s shown on the left above.  (That curve doesn’t necessarily have anything to do with marketing; it’s just an example curve.)

\item {} 
Whenever we try to gather data about the phenomenon we care about, inevitably some problems mess things up.  We might make mistakes when measuring or recording data.  Some of our data might be recorded at times that are special (like a holiday weekend) that make them not representative of the whole picture.  And other variables might be influencing our data that we didn’t measure, such as the weather or other companies’ marketing campaigns.  All of this creates fluctuations we call noise, as shown in the middle.

\item {} 
What we actually measure when we gather data is the combination of these two things, as shown on the right, above.  Of course, when we get data, we see only that final graph, the signal plus the noise together, and we don’t know how to separate them.

\end{enumerate}

Not knowing that the original signal was parabolic, we might make either of two mistakes.  First, we might make a model that is too simple, an underfit model, such as a linear one.

\sphinxincludegraphics{{signal-and-noise-2}.png}

You can see that the model is a bit higher than the center of the data on each end, and a bit lower than the center of the data in the middle.  This is because the model is missing some key feature of the data, its slight downward curvature.  The model is \sphinxstyleemphasis{underfit;} it should be more fit to the unique characteristics this data is showing.

Second, we might make a model that is too complex, an overfit model, such as a high\sphinxhyphen{}degree polynomial model.

\sphinxincludegraphics{{signal-and-noise-3}.png}

This is a particularly easy mistake to make in Excel, where you can use the options in the trendline dialog box to choose any polynomial model you like, and it’s tempting to crank up the polynomial degree and watch the measurement of goodness of fit increase!  But that measure is telling you only how well the model fits the data you have, not any unseen data.  That difference is the core of what overfitting means.  The model is overfit to known data, and thus probably generalizes poorly to unseen data.

Polynomial models are especially problematic in this area, because all polynomials (other than linear ones) diverge rapidly towards \(\pm\infty\) in both directions, thus making them highly useless for prediction if given an input outside the range of the data on which the model was fit.

The happy medium between these two mistakes, in this case, is a quadratic model, because in this example, I made the signal a simple quadratic function.  Of course, in the real world, signals of many types can occur, and their exact nature is not known from the data alone.

\sphinxincludegraphics{{signal-and-noise-4}.png}

Although this quadratic model may not exactly match the quadratic function that is the signal, it is the closest we can come based on the data we observed.

Now we know what the problems are.  How do we go about avoiding underfitting or overfitting?  Machine learning (and mathematical modeling in general) have developed some best practices for doing just that.


\section{Training, validation, and testing}
\label{\detokenize{chapter-17-machine-learning:training-validation-and-testing}}
\begin{sphinxadmonition}{note}{Big Picture \sphinxhyphen{} Why we split data into train and test sets}

Recall that the key question we’re trying to answer is, “How do I make a model that works well on unseen data?”  Or more precisely, “How do I make a model that works well on data that wasn’t used to create the model?”

The solution is actually rather simple:  When given a dataset, split it into two parts, one you will use to create your model, and the other that you will use as “unseen data,” on which to test your model.  The details are actually slightly more intricate than that simple answer, but that’s the heart of it.
\end{sphinxadmonition}


\subsection{Training vs. testing}
\label{\detokenize{chapter-17-machine-learning:training-vs-testing}}
If we take the advice above into account, the process of mathematical modeling would then proceed like this:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
We get a dataset \sphinxcode{\sphinxupquote{df}} that we’re going to use to build a model.

\item {} 
We split the data into two parts, a larger part \sphinxcode{\sphinxupquote{df\_train}} that we’ll use for “training” (creating the model) and a smaller part \sphinxcode{\sphinxupquote{df\_test}} that we’ll use for testing the model after it’s been created.
\begin{itemize}
\item {} 
Since \sphinxcode{\sphinxupquote{df\_test}} isn’t used to create the model, it’s “unseen data” from the model’s point of view, and can give us a hint on how the model will perform on data that’s entirely outside of \sphinxcode{\sphinxupquote{df}}.

\item {} 
Typically, \sphinxcode{\sphinxupquote{df\_train}} is a random sample of about 70\%\sphinxhyphen{}80\% of \sphinxcode{\sphinxupquote{df}}, and \sphinxcode{\sphinxupquote{df\_test}} is the other 20\%\sphinxhyphen{}30\%.

\end{itemize}

\item {} 
We choose which model we want to use (such as linear regression, for example) and fit the model to \sphinxcode{\sphinxupquote{df\_train}} only.
\begin{itemize}
\item {} 
This is called the \sphinxstyleemphasis{training} phase; what statisticians call “fitting a model,” machine learning people call “training the model.”

\end{itemize}

\item {} 
We use the model to predict outputs for each input in \sphinxcode{\sphinxupquote{df\_test}} and compare them to the known outputs in \sphinxcode{\sphinxupquote{df\_test}}, and see how well the model does.
\begin{itemize}
\item {} 
For example, you might compute the distribution of percent errors, and see if they’re within the tolerance you can accept in your business application.

\item {} 
This is called the \sphinxstyleemphasis{testing} phase.

\end{itemize}

\item {} 
If the model seems acceptable, you would then proceed to re\sphinxhyphen{}fit the same model on the entire dataset \sphinxcode{\sphinxupquote{df}} before you use it for predictions, because more data tends to improve model quality.

\end{enumerate}

It is easy to split a DataFrame’s rows into two different sets like this with random sampling.  Use code like the following.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Create some fake data to use for demonstrating the technique:}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(} \PYG{p}{\PYGZob{}} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Totally}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Fake}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{,}\PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Data}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{l+m+mi}{5}\PYG{p}{,}\PYG{l+m+mi}{6}\PYG{p}{,}\PYG{l+m+mi}{10}\PYG{p}{)} \PYG{p}{\PYGZcb{}} \PYG{p}{)}
\PYG{n}{df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
    Totally      Fake      Data
0  1.000000  3.000000  5.000000
1  1.111111  3.111111  5.111111
2  1.222222  3.222222  5.222222
3  1.333333  3.333333  5.333333
4  1.444444  3.444444  5.444444
5  1.555556  3.555556  5.555556
6  1.666667  3.666667  5.666667
7  1.777778  3.777778  5.777778
8  1.888889  3.888889  5.888889
9  2.000000  4.000000  6.000000
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Choose an approximately 80\PYGZpc{} subset:}
\PYG{c+c1}{\PYGZsh{} (In this case, 8 is 80\PYGZpc{} of the rows.)}
\PYG{n}{rows\PYGZus{}for\PYGZus{}training} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{choice}\PYG{p}{(} \PYG{n}{df}\PYG{o}{.}\PYG{n}{index}\PYG{p}{,} \PYG{l+m+mi}{8}\PYG{p}{,} \PYG{k+kc}{False} \PYG{p}{)}
\PYG{n}{training} \PYG{o}{=} \PYG{n}{df}\PYG{o}{.}\PYG{n}{index}\PYG{o}{.}\PYG{n}{isin}\PYG{p}{(} \PYG{n}{rows\PYGZus{}for\PYGZus{}training} \PYG{p}{)}
\PYG{n}{df\PYGZus{}train} \PYG{o}{=} \PYG{n}{df}\PYG{p}{[}\PYG{n}{training}\PYG{p}{]}
\PYG{n}{df\PYGZus{}test} \PYG{o}{=} \PYG{n}{df}\PYG{p}{[}\PYG{o}{\PYGZti{}}\PYG{n}{training}\PYG{p}{]}
\end{sphinxVerbatim}

Let’s see the resulting split.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df\PYGZus{}train}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
    Totally      Fake      Data
0  1.000000  3.000000  5.000000
1  1.111111  3.111111  5.111111
2  1.222222  3.222222  5.222222
3  1.333333  3.333333  5.333333
4  1.444444  3.444444  5.444444
5  1.555556  3.555556  5.555556
7  1.777778  3.777778  5.777778
9  2.000000  4.000000  6.000000
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df\PYGZus{}test}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
    Totally      Fake      Data
6  1.666667  3.666667  5.666667
8  1.888889  3.888889  5.888889
\end{sphinxVerbatim}


\subsection{Data leakage}
\label{\detokenize{chapter-17-machine-learning:data-leakage}}
It is is essential, in the above five\sphinxhyphen{}step process, not to touch (or typically even look at) the testing data (in \sphinxcode{\sphinxupquote{df\_test}}) until the testing phase.  It is also essential not to repeat back to step 1, 2, or 3 once you’ve reached step 4.  Otherwise \sphinxcode{\sphinxupquote{df\_test}} no longer represents unseen data.  (Like that embarrassing social media post, you can’t unsee it.)  In fact, in machine learning competitions, the group running the competition will typically split the data into training and testing sets before the competition, and distribute only the training set to competitors, leaving the testing set secret, to be used for judging the winner.  It’s truly unseen data!

If a data scientist even looks at the test data or does summary statistics about it, this information can influence how they do the modeling process on training data.  This error is called \sphinxstyleemphasis{data leakage,} because some aspects of the test data have leaked out of where they’re supposed to be safely contained at the end of the whole process, and have contaminated the beginning of the process instead.  The five\sphinxhyphen{}step process given above is designed, in part, to eliminate data leakage.


\subsection{Validation}
\label{\detokenize{chapter-17-machine-learning:validation}}
But this restriction on seeing \sphinxcode{\sphinxupquote{df\_test}} only once introduces a significant drawback.  What if you have several different models you’d like to try, and you’re not sure which one will work best on unseen data?  How can we compare multiple models if we can test only one?  The question is even more complex if the model comes with parameters that a data scientist is supposed to choose (so\sphinxhyphen{}called hyperparameters), which may require some iterative experimentation.

The answer to this problem involves introducing a new phase, called \sphinxstyleemphasis{validation,} in between training and testing, and creating a three\sphinxhyphen{}way data split.  Perhaps you’ve heard of the technique of \sphinxstyleemphasis{cross\sphinxhyphen{}validation,} one particular way to do the validation step.  In this chapter, since we are just doing a quick introduction to the machine learning process, we will not dive deeply into the validation phase, but will just keep the five\sphinxhyphen{}step process shown above, which uses only a train/test split of the data.


\section{Logistic Regression}
\label{\detokenize{chapter-17-machine-learning:logistic-regression}}
Machine learning is an area of statistics and computer science that includes many types of advanced models, such as support vector machines, neural networks, decision trees, random forests, and other ensemble methods.  We will see in this small, introductory chapter just one new modeling method, logistic regression.  But we will use it as a way to see several aspects of the way machine learning is done in practice, including the train/test data split discussed above.


\subsection{Classification vs. regression}
\label{\detokenize{chapter-17-machine-learning:classification-vs-regression}}
The particular machine learning task we’ll cover as an example in this chapter uses a technique called logistic regression.  This technique is covered in detail in MA347 and MA380, but we will do just a small preview here.  The key difference between logistic regression and linear regression is one of output type:
\begin{itemize}
\item {} 
Linear regression creates a model whose output type is real numbers.

\item {} 
Logistic regression creates a model whose output type is boolean, with values 0 and 1.

\end{itemize}

(Technically, logistic regression models create outputs anywhere in the interval \((0,1)\), not including either end, and we round up/down from the center to convert them to boolean outputs.)

Machine learning tasks are often sorted broadly into two categories, \sphinxstyleemphasis{classification} and \sphinxstyleemphasis{regression}.  Models that output boolean values (or any small number of choices, not necessarily two) are called classification models, and models that output numerical values are called regression models.  (This is unfortunate, because logistic regression is used for classification.  I don’t pick the names.)  We are going to study a binary classification problem below, and so we want a model that outputs one of two values, 0 or 1.  Logistic regression is a natural choice.

If you take MA347 or MA380, you will learn the exact transformation of the inputs and outputs that make logistic regression possible.  But for our purposes here, we will just use Python code that handles them for us.  The essentials are that we provide any set of numeric variables as input and we get boolean values (zeros and ones) as model output.


\subsection{Medical example}
\label{\detokenize{chapter-17-machine-learning:medical-example}}
Let’s make this concrete with an example.  Assume we’ve measured three important health variables about ten patients in a study and then given them all an experimental drug.  We then measured whether they responded well or not to the drug (0 meaning no and 1 meaning yes).  We’d like to try to predict, from their initial three health variables, whether they will respond well to the drug, so we know which new patients might benefit.  We will use fabricated data, partly because medical data is private and partly beacuse it will be nice to have a small example from which to learn.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df\PYGZus{}drug\PYGZus{}response} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(} \PYG{p}{\PYGZob{}}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Height (in)}\PYG{l+s+s1}{\PYGZsq{}}     \PYG{p}{:} \PYG{p}{[}  \PYG{l+m+mi}{72}\PYG{p}{,}  \PYG{l+m+mi}{63}\PYG{p}{,}  \PYG{l+m+mi}{60}\PYG{p}{,}  \PYG{l+m+mi}{69}\PYG{p}{,}  \PYG{l+m+mi}{59}\PYG{p}{,}  \PYG{l+m+mi}{74}\PYG{p}{,}  \PYG{l+m+mi}{63}\PYG{p}{,}  \PYG{l+m+mi}{67}\PYG{p}{,}  \PYG{l+m+mi}{60}\PYG{p}{,}  \PYG{l+m+mi}{64} \PYG{p}{]}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Weight (lb)}\PYG{l+s+s1}{\PYGZsq{}}     \PYG{p}{:} \PYG{p}{[} \PYG{l+m+mi}{150}\PYG{p}{,} \PYG{l+m+mi}{191}\PYG{p}{,} \PYG{l+m+mi}{112}\PYG{p}{,} \PYG{l+m+mi}{205}\PYG{p}{,} \PYG{l+m+mi}{136}\PYG{p}{,} \PYG{l+m+mi}{139}\PYG{p}{,} \PYG{l+m+mi}{184}\PYG{p}{,} \PYG{l+m+mi}{230}\PYG{p}{,} \PYG{l+m+mi}{198}\PYG{p}{,} \PYG{l+m+mi}{169} \PYG{p}{]}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Systolic (mmHg)}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{:} \PYG{p}{[}  \PYG{l+m+mi}{90}\PYG{p}{,} \PYG{l+m+mi}{105}\PYG{p}{,}  \PYG{l+m+mi}{85}\PYG{p}{,} \PYG{l+m+mi}{130}\PYG{p}{,} \PYG{l+m+mi}{107}\PYG{p}{,} \PYG{l+m+mi}{117}\PYG{p}{,} \PYG{l+m+mi}{145}\PYG{p}{,}  \PYG{l+m+mi}{99}\PYG{p}{,} \PYG{l+m+mi}{109}\PYG{p}{,}  \PYG{l+m+mi}{89} \PYG{p}{]}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Response}\PYG{l+s+s1}{\PYGZsq{}}        \PYG{p}{:} \PYG{p}{[}   \PYG{l+m+mi}{0}\PYG{p}{,}   \PYG{l+m+mi}{1}\PYG{p}{,}   \PYG{l+m+mi}{0}\PYG{p}{,}   \PYG{l+m+mi}{0}\PYG{p}{,}   \PYG{l+m+mi}{1}\PYG{p}{,}   \PYG{l+m+mi}{1}\PYG{p}{,}   \PYG{l+m+mi}{1}\PYG{p}{,}   \PYG{l+m+mi}{0}\PYG{p}{,}   \PYG{l+m+mi}{1}\PYG{p}{,}   \PYG{l+m+mi}{1} \PYG{p}{]}
\PYG{p}{\PYGZcb{}} \PYG{p}{)}
\PYG{n}{df\PYGZus{}drug\PYGZus{}response}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
   Height (in)  Weight (lb)  Systolic (mmHg)  Response
0           72          150               90         0
1           63          191              105         1
2           60          112               85         0
3           69          205              130         0
4           59          136              107         1
5           74          139              117         1
6           63          184              145         1
7           67          230               99         0
8           60          198              109         1
9           64          169               89         1
\end{sphinxVerbatim}

Let us assume that this data is the training dataset, and the test dataset has already been split out and saved in a separate file, where we cannot yet see it.  We will create a model on this dataset, and then later evaluate how well it behaves on new data.

When we apply logistic regression to this dataset, we will want to make it clear which columns are to be seen as inputs (or features or predictors) and which is to be seen as the output (or target or response).  To facilitate this, let’s store them in different variables.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{predictors} \PYG{o}{=} \PYG{n}{df\PYGZus{}drug\PYGZus{}response}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Height (in)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Weight (lb)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Systolic (mmHg)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}
\PYG{n}{response}   \PYG{o}{=} \PYG{n}{df\PYGZus{}drug\PYGZus{}response}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Response}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\end{sphinxVerbatim}


\subsection{Logistic regression in scikit\sphinxhyphen{}learn}
\label{\detokenize{chapter-17-machine-learning:logistic-regression-in-scikit-learn}}
A very popular machine learning toolit is called scikit\sphinxhyphen{}learn, and is distributed as the Python module \sphinxcode{\sphinxupquote{sklearn}}.  If your Python installation came from Anaconda, you already have \sphinxcode{\sphinxupquote{sklearn}} installed.  We can use the logistic regression tools built into scikit\sphinxhyphen{}learn to create a logistic regression model that will use the first three columns above as inputs from which it should predict the final column as the output.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Import the module}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{linear\PYGZus{}model} \PYG{k+kn}{import} \PYG{n}{LogisticRegression}

\PYG{c+c1}{\PYGZsh{} Create a model and fit it to the data}
\PYG{n}{model} \PYG{o}{=} \PYG{n}{LogisticRegression}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(} \PYG{n}{predictors}\PYG{p}{,} \PYG{n}{response} \PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Let\PYGZsq{}s see how close it came.}
\PYG{n}{df\PYGZus{}drug\PYGZus{}response}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Prediction}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(} \PYG{n}{predictors} \PYG{p}{)}
\PYG{n}{df\PYGZus{}drug\PYGZus{}response}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Correct}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{df\PYGZus{}drug\PYGZus{}response}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Prediction}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{==} \PYG{n}{df\PYGZus{}drug\PYGZus{}response}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Response}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{df\PYGZus{}drug\PYGZus{}response}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
   Height (in)  Weight (lb)  Systolic (mmHg)  Response  Prediction  Correct
0           72          150               90         0           0     True
1           63          191              105         1           1     True
2           60          112               85         0           1    False
3           69          205              130         0           1    False
4           59          136              107         1           1     True
5           74          139              117         1           1     True
6           63          184              145         1           1     True
7           67          230               99         0           0     True
8           60          198              109         1           1     True
9           64          169               89         1           0    False
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df\PYGZus{}drug\PYGZus{}response}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Correct}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)} \PYG{o}{/} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{df\PYGZus{}drug\PYGZus{}response}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
0.7
\end{sphinxVerbatim}

This model achieved a 70\% correct prediction rate on this data.  Of course, this is a small dataset and is completely fabricated, so this doesn’t mean anything in the real world.  But the purpose is to show you that a relatively small amount of code can set up and use logistic regression.  We will use it for a more interesting example in class.


\subsection{Model coefficients}
\label{\detokenize{chapter-17-machine-learning:model-coefficients}}
But there is still more we can learn here.  Just like linear regression, logistic regression also computes the best coefficient for each variable as part of the model\sphinxhyphen{}fitting process.  In linear regression, the resulting model is just the combination of those coefficients with the variables, \(\beta_0+\beta_1x_1+\beta_2x_2+\cdots+\beta_nx_n\).  In logistic regression, that same formula is then composed with the logistic curve to fit the result into the interval \((0,1)\), but the coefficients can still tell us which variables were the most important.

Right now, however, our model is fit using predictors that have very different scales.  Height is a two\sphinxhyphen{}digit number, weight is a three\sphinxhyphen{}digit number, and blood pressure varies in between.  So the coefficients, which must interact with these different units, are not currently comparable.  It is therefore common to create a standardized copy of the predictors and re\sphinxhyphen{}fit the model to it, just for comparing coefficients, because then they are all on the same scale.  Standardization is the process of subtracting the mean of a sample and dividing by its standard deviation.  (Note that this makes the units in the column headers no longer accurate.)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{standardized} \PYG{o}{=} \PYG{p}{(} \PYG{n}{predictors} \PYG{o}{\PYGZhy{}} \PYG{n}{predictors}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)} \PYG{p}{)} \PYG{o}{/} \PYG{n}{predictors}\PYG{o}{.}\PYG{n}{std}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{standardized}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
   Height (in)  Weight (lb)  Systolic (mmHg)
0     1.322744    \PYGZhy{}0.583434        \PYGZhy{}0.927831
1    \PYGZhy{}0.402574     0.534360        \PYGZhy{}0.137066
2    \PYGZhy{}0.977681    \PYGZhy{}1.619438        \PYGZhy{}1.191419
3     0.747638     0.916046         1.180875
4    \PYGZhy{}1.169383    \PYGZhy{}0.965120        \PYGZhy{}0.031631
5     1.706149    \PYGZhy{}0.883330         0.495546
6    \PYGZhy{}0.402574     0.343517         1.971640
7     0.364234     1.597627        \PYGZhy{}0.453372
8    \PYGZhy{}0.977681     0.725203         0.073805
9    \PYGZhy{}0.210872    \PYGZhy{}0.065432        \PYGZhy{}0.980548
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{standardized\PYGZus{}model} \PYG{o}{=} \PYG{n}{LogisticRegression}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{standardized\PYGZus{}model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(} \PYG{n}{standardized}\PYG{p}{,} \PYG{n}{response} \PYG{p}{)}
\PYG{n}{coefficients} \PYG{o}{=} \PYG{n}{standardized\PYGZus{}model}\PYG{o}{.}\PYG{n}{coef\PYGZus{}}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
\PYG{n}{pd}\PYG{o}{.}\PYG{n}{Series}\PYG{p}{(} \PYG{n}{coefficients}\PYG{p}{,} \PYG{n}{index}\PYG{o}{=}\PYG{n}{predictors}\PYG{o}{.}\PYG{n}{columns} \PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Height (in)       \PYGZhy{}0.506490
Weight (lb)       \PYGZhy{}0.205779
Systolic (mmHg)    0.563031
dtype: float64
\end{sphinxVerbatim}

Here we can see that height and weight negatively impacted the drug response and blood pressure positively impacted it.  The magnitude of each variable (in absolute value) shows which variables are more important than others.  A higher absolute value for the variable’s coefficient means it is more important.

This is a rather simple way to compare the importance of variables, and classes like MA252 and MA347 will cover more statistically sophisticated methods.

So the good news is that fitting a logistic model to data is just a few lines of Python code!  But there are several additional details that it will be helpful to know about the context in which we apply logistic regression.  We cover each of those details in the remaining sections of this chapter.


\section{Measuring success}
\label{\detokenize{chapter-17-machine-learning:measuring-success}}
In the example above, we saw a 70\% correct prediction rate on our training dataset.  But machine learning practitioners have several ways of measuring the quality of a classification model.  For instance, in some cases, a false positive is better than a false negative.  You don’t mind if the computer system that works for your credit card company texts you after your legitimate purchase and asks, “Was this you?”  It had a false positive for a fraud prediction, checked it out with you, and you said “No problem.”  So a false positive is no big deal.  But a false negative (undetected fraud) is much worse.  So a classification model for credit card fraud should be judged more on its false negative rate than on its false positive rate.  Which measurement is best varies by application domain.

Two common measurements of binary classification accuracy are \sphinxstyleemphasis{precision} and \sphinxstyleemphasis{recall.}  Let’s make the following definitions.
\begin{itemize}
\item {} 
Use \(TP\) to stand for the number of true positives among our model’s predictions.  (In the example above, \(TP=6\), for the six rows 1, 4, 5, 6, 8, and 9.

\item {} 
Similarly, let \(TN\), \(FP\), and \(FN\) stand for true negatives, false positives, and false negatives, respectively.  (Above, we had \(TN=2\), \(FP=2\), and \(FN=0\).)

\item {} 
We define the classifier’s \sphinxstylestrong{precision} to be \(\frac{TP}{TP+FP}\).  This answers the question:  If the test said “positive,” what are the odds that it’s a true positive?  This is the measure that a patient who just got a positive diagnosis cares about.  What are the odds it’s real?

\item {} 
We define the classifier’s \sphinxstylestrong{recall} to be \(\frac{TP}{TP+FN}\).  This answers the question:  If the reality is “positive,” what are the odds the test will detect that?  This is the measure that the credit card company cares about.  What percentage of fraud does our system catch?

\end{itemize}

Let’s see how to code these measurements in Python.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} True positive means the answer and the prediction were positive.}
\PYG{n}{TP} \PYG{o}{=} \PYG{p}{(} \PYG{n}{df\PYGZus{}drug\PYGZus{}response}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Response}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{\PYGZam{}} \PYG{n}{df\PYGZus{}drug\PYGZus{}response}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Prediction}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} Similarly for the other three.}
\PYG{n}{TN} \PYG{o}{=} \PYG{p}{(} \PYG{o}{\PYGZti{}}\PYG{n}{df\PYGZus{}drug\PYGZus{}response}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Response}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{\PYGZam{}} \PYG{o}{\PYGZti{}}\PYG{n}{df\PYGZus{}drug\PYGZus{}response}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Prediction}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{FP} \PYG{o}{=} \PYG{p}{(} \PYG{o}{\PYGZti{}}\PYG{n}{df\PYGZus{}drug\PYGZus{}response}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Response}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{\PYGZam{}} \PYG{n}{df\PYGZus{}drug\PYGZus{}response}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Prediction}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{FN} \PYG{o}{=} \PYG{p}{(} \PYG{n}{df\PYGZus{}drug\PYGZus{}response}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Response}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{\PYGZam{}} \PYG{o}{\PYGZti{}}\PYG{n}{df\PYGZus{}drug\PYGZus{}response}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Prediction}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Precision and recall are defined using the formulas above.}
\PYG{n}{precision} \PYG{o}{=} \PYG{n}{TP} \PYG{o}{/} \PYG{p}{(} \PYG{n}{TP} \PYG{o}{+} \PYG{n}{FP} \PYG{p}{)}
\PYG{n}{recall} \PYG{o}{=} \PYG{n}{TP} \PYG{o}{/} \PYG{p}{(} \PYG{n}{TP} \PYG{o}{+} \PYG{n}{FN} \PYG{p}{)}

\PYG{n}{precision}\PYG{p}{,} \PYG{n}{recall}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(0.7142857142857143, 0.8333333333333334)
\end{sphinxVerbatim}

In many cases, however, both precision and recall matter.  A common way to combine them is in a score called the \(F_1\) score, which combines precision and recall using a formula called the geometric mean.
\begin{equation*}
\begin{split}F_1=\frac{2\times\text{precision}\times\text{recall}}{\text{precision}+\text{recall}}\end{split}
\end{equation*}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{F1} \PYG{o}{=} \PYG{l+m+mi}{2} \PYG{o}{*} \PYG{n}{precision} \PYG{o}{*} \PYG{n}{recall} \PYG{o}{/} \PYG{p}{(} \PYG{n}{precision} \PYG{o}{+} \PYG{n}{recall} \PYG{p}{)}
\end{sphinxVerbatim}

Just as a higher score is better for both precision and recall, a higher score is also better for \(F_1\).  We can use this measure to compare models, prioritizing a balance of both precision and recall.


\section{Categorical input variables}
\label{\detokenize{chapter-17-machine-learning:categorical-input-variables}}
Often we have input variables that are not numeric; in the medical example above, we might also have a patient’s sex or race, and want to know if those impact whether they will respond well to the drug.  Those data are not numeric; they are categorical.  But we can make them numeric using any of several common techniques.

Let’s say we had patient race, and there were several categories, including Black, White, Latino, Indian, Asian, and Other.  I will add data of this type to the original data we saw above.  Of course, this, too, is fictitious data.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df\PYGZus{}drug\PYGZus{}response}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Race}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Asian}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Black}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Black}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Latino}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{White}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{White}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Indian}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{White}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Asian}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Latino}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{df\PYGZus{}drug\PYGZus{}response}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Race}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{df\PYGZus{}drug\PYGZus{}response}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Race}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{category}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\end{sphinxVerbatim}

Suppose that the medical professionals believe, from past studies in this area, that Black and Indian patients might respond differently to the drug, but everyone else should be similar to one another.  We can therefore convert this categorical variable into two boolean variables, one answering the question, “Is the patient Black?” and the other answering the question, “Is the patient Indian?”

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df\PYGZus{}drug\PYGZus{}response}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Race=Black}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{df\PYGZus{}drug\PYGZus{}response}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Race}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Black}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{df\PYGZus{}drug\PYGZus{}response}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Race=Indian}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{df\PYGZus{}drug\PYGZus{}response}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Race}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Indian}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{df\PYGZus{}drug\PYGZus{}response}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
   Height (in)  Weight (lb)  Systolic (mmHg)  Response  Prediction  Correct  \PYGZbs{}
0           72          150               90         0           0     True   
1           63          191              105         1           1     True   
2           60          112               85         0           1    False   
3           69          205              130         0           1    False   
4           59          136              107         1           1     True   
5           74          139              117         1           1     True   
6           63          184              145         1           1     True   
7           67          230               99         0           0     True   
8           60          198              109         1           1     True   
9           64          169               89         1           0    False   

     Race  Race=Black  Race=Indian  
0   Asian       False        False  
1   Black        True        False  
2   Black        True        False  
3  Latino       False        False  
4   White       False        False  
5   White       False        False  
6  Indian       False         True  
7   White       False        False  
8   Asian       False        False  
9  Latino       False        False  
\end{sphinxVerbatim}

The value of having done this is that boolean inputs can be represented using numerical values 0 and 1, just as boolean outputs can.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df\PYGZus{}drug\PYGZus{}response}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Race=Black}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{df\PYGZus{}drug\PYGZus{}response}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Race=Black}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(} \PYG{n+nb}{int} \PYG{p}{)}
\PYG{n}{df\PYGZus{}drug\PYGZus{}response}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Race=Indian}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{df\PYGZus{}drug\PYGZus{}response}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Race=Indian}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(} \PYG{n+nb}{int} \PYG{p}{)}
\PYG{n}{df\PYGZus{}drug\PYGZus{}response}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
   Height (in)  Weight (lb)  Systolic (mmHg)  Response  Prediction  Correct  \PYGZbs{}
0           72          150               90         0           0     True   
1           63          191              105         1           1     True   
2           60          112               85         0           1    False   
3           69          205              130         0           1    False   
4           59          136              107         1           1     True   
5           74          139              117         1           1     True   
6           63          184              145         1           1     True   
7           67          230               99         0           0     True   
8           60          198              109         1           1     True   
9           64          169               89         1           0    False   

     Race  Race=Black  Race=Indian  
0   Asian           0            0  
1   Black           1            0  
2   Black           1            0  
3  Latino           0            0  
4   White           0            0  
5   White           0            0  
6  Indian           0            1  
7   White           0            0  
8   Asian           0            0  
9  Latino           0            0  
\end{sphinxVerbatim}

These variables could then be added to our \sphinxcode{\sphinxupquote{predictors}} DataFrame and used as numerical inputs to our model.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{predictors} \PYG{o}{=} \PYG{n}{predictors}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} handle warnings about slices}
\PYG{n}{predictors}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Race=Black}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{df\PYGZus{}drug\PYGZus{}response}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Race=Black}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{predictors}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Race=Indian}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{df\PYGZus{}drug\PYGZus{}response}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Race=Indian}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\end{sphinxVerbatim}

If no medical opinion had been present to suggest which races the model should focus on, we could create a boolean variable for each possible race.  There are some disadvantages to adding too many columns to your data, which we won’t cover here, but this is a common practice.  If all categories are converted into boolean variables, the result is called a \sphinxstyleemphasis{one\sphinxhyphen{}hot encoding,} because each row will have just one of the race columns equal to 1 and all others equal to 0.


\section{Overfitting and underfitting in this example}
\label{\detokenize{chapter-17-machine-learning:overfitting-and-underfitting-in-this-example}}
Let’s return to the major theme introduced at the start of this chapter.  One way to overfit a regression or classification model is to throw in every variable you have access to as inputs to the model.  This is very similar to the example of polynomial regression used earlier, because polynomial regression essentially adds new columns \(x^2,x^3,x^4,\ldots\) as inputs.  A simple model will use just the most important variables, not necessarily every possible variable.

Statistics has many methods for evaluating which variables should be included or excluded from a model, and MA252 (Regression Analysis) covers such techniques.  But we have seen one way to discern which variables are the most impactful in logistic regression–examining the coefficients on those variables.  Variables whose coefficients are closer to zero are less likely to be indicative of signal and more likely to be indicative of noise.  Variables whose coefficients are larger (in absolute value, that is, farther from zero) are more likely to be indicative of the actual underlying structure of the problem.

Our in\sphinxhyphen{}class exercise on mortgage data will assess variable relevance using logistic regression coefficients.


\chapter{Detailed Course Schedule}
\label{\detokenize{course-schedule:detailed-course-schedule}}\label{\detokenize{course-schedule::doc}}
This include all topics covered and all assignments given and when they are due.


\section{Week 1 \sphinxhyphen{} 9/3/2020 \sphinxhyphen{} Introduction and mathematical foundations}
\label{\detokenize{course-schedule:week-1-9-3-2020-introduction-and-mathematical-foundations}}

\subsection{Content}
\label{\detokenize{course-schedule:content}}\begin{itemize}
\item {} 
Chapter 1: Introduction to data science \sphinxhyphen{} {\hyperref[\detokenize{chapter-1-intro-to-data-science::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{reading}}}} and slides

\item {} 
Chapter 2: Mathematical foundations \sphinxhyphen{} {\hyperref[\detokenize{chapter-2-mathematical-foundations::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{reading}}}} and slides

\end{itemize}


\subsection{Due before next class}
\label{\detokenize{course-schedule:due-before-next-class}}\begin{itemize}
\item {} 
\sphinxstylestrong{DataCamp}
\begin{itemize}
\item {} 
Optional, basic review:
\begin{itemize}
\item {} 
\sphinxhref{https://www.datacamp.com/courses/intro-to-python-for-data-science}{Introduction to Python}

\item {} 
\sphinxhref{https://www.datacamp.com/courses/python-data-science-toolbox-part-1}{Python Data Science Toolbox, Part 1}

\end{itemize}

\item {} 
Required (though it may still be review):
\begin{itemize}
\item {} 
\sphinxhref{https://www.datacamp.com/courses/intermediate-python-for-data-science}{Intermediate Python}, chapters 1\sphinxhyphen{}4

\item {} 
\sphinxhref{https://www.datacamp.com/courses/pandas-foundations}{pandas Foundations}, just chapter 1

\item {} 
\sphinxhref{https://www.datacamp.com/courses/manipulating-dataframes-with-pandas}{Manipulating DataFrames with pandas}, just chapter 1

\end{itemize}

\item {} 
\sphinxhref{big-cheat-sheet.html\#before-week-2-review-of-cs230}{See here for a cheat sheet} of all the content of the above DataCamp lessons.

\end{itemize}

\item {} 
\sphinxstylestrong{Reading}
\begin{itemize}
\item {} 
Each week, you are expected to read the appropriate chapters from the course notes \sphinxstyleemphasis{before} class.  Since this is the first day for the course, I did not expect you to have read Chapters 1\sphinxhyphen{}2 in advance.  But that means that you must now read them together with Chapters 3\sphinxhyphen{}4 before next week.

\item {} 
{\hyperref[\detokenize{chapter-1-intro-to-data-science::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Chapter 1: Introduction to data science}}}} (adds details to today’s class content)

\item {} 
{\hyperref[\detokenize{chapter-2-mathematical-foundations::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Chapter 2: Mathematical foundations}}}} (adds details to today’s class content)

\item {} 
{\hyperref[\detokenize{chapter-3-jupyter::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Chapter 3: Computational notebooks (Jupyter)}}}} (prepares for next week)

\item {} 
{\hyperref[\detokenize{chapter-4-review-of-python-and-pandas::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Chapter 4: Python review focusing on pandas and mathematical foundations}}}} (prepares for next week)

\end{itemize}

\item {} 
\sphinxstylestrong{Other}
\begin{itemize}
\item {} 
If you don’t already have a Python environment installed on your computer, {\hyperref[\detokenize{anaconda-installation::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{see these instructions for installing one}}}}.  As part of that process, ensure that you can open both Jupyter Lab and VS Code.

\item {} 
Optional: There are many LOYO opportunities from this week’s course notes (chapters 1 and 2).  See the syllabus for a definition of LOYO and consider forming a team and siezing one of the opportunities.

\end{itemize}

\end{itemize}


\bigskip\hrule\bigskip



\section{Week 2 \sphinxhyphen{} 9/10/2020 \sphinxhyphen{} Jupyter and a review of Python and pandas}
\label{\detokenize{course-schedule:week-2-9-10-2020-jupyter-and-a-review-of-python-and-pandas}}

\subsection{Content}
\label{\detokenize{course-schedule:id1}}\begin{itemize}
\item {} 
Chapter 3: Computational notebooks (Jupyter) \sphinxhyphen{} {\hyperref[\detokenize{chapter-3-jupyter::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{reading}}}} and slides

\item {} 
Chapter 4: Review of Python and pandas \sphinxhyphen{} {\hyperref[\detokenize{chapter-4-review-of-python-and-pandas::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{reading}}}}, but no slides

\end{itemize}


\subsection{Due before next class}
\label{\detokenize{course-schedule:id2}}\begin{itemize}
\item {} 
\sphinxstylestrong{DataCamp}
\begin{itemize}
\item {} 
\sphinxhref{https://www.datacamp.com/courses/manipulating-dataframes-with-pandas}{Manipulating DataFrames with pandas}, chapters 2\sphinxhyphen{}4

\item {} 
\sphinxhref{big-cheat-sheet.html\#before-week-3}{See here for a cheat sheet} of all the content of the above DataCamp lessons.

\end{itemize}

\item {} 
\sphinxstylestrong{Reading}
\begin{itemize}
\item {} 
{\hyperref[\detokenize{chapter-5-before-and-after::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Chapter 5: Before and after, in mathematics and communication}}}}

\item {} 
{\hyperref[\detokenize{chapter-6-single-table-verbs::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Chapter 6: Pandas single\sphinxhyphen{}table verbs}}}}

\end{itemize}

\end{itemize}


\bigskip\hrule\bigskip



\section{Week 3 \sphinxhyphen{} 9/17/2020 \sphinxhyphen{} Before and after, single\sphinxhyphen{}table verbs}
\label{\detokenize{course-schedule:week-3-9-17-2020-before-and-after-single-table-verbs}}

\subsection{Content}
\label{\detokenize{course-schedule:id3}}\begin{itemize}
\item {} 
Chapter 5: Before and after, in mathematics and communication \sphinxhyphen{} {\hyperref[\detokenize{chapter-5-before-and-after::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{reading}}}} and slides

\item {} 
Chapter 6: Pandas single\sphinxhyphen{}table verbs \sphinxhyphen{} {\hyperref[\detokenize{chapter-6-single-table-verbs::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{reading}}}} and slides

\end{itemize}


\subsection{Due before next class}
\label{\detokenize{course-schedule:id4}}\begin{itemize}
\item {} 
\sphinxstylestrong{Communication exercise}
\begin{itemize}
\item {} 
Download this Jupyter notebook.

\item {} 
Download this CSV file into the same folder.

\item {} 
The first half of the notebook has plenty of comments and explanations, but the second half does not.  Use the principles discussed in class today (and covered in {\hyperref[\detokenize{chapter-5-before-and-after::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Chapter 5 of the course notes}}}}) to comment/document/explain the second half of that file.

\item {} 
Upload your work to a new Deepnote project.  (Don’t forget the data file!)

\item {} 
Email the URL for that project to your instructor.

\end{itemize}

\item {} 
\sphinxstylestrong{DataCamp}
\begin{itemize}
\item {} 
\sphinxhref{https://www.datacamp.com/courses/pandas-foundations}{pandas Foundations}, just chapter 2

\item {} 
\sphinxhref{big-cheat-sheet.html\#before-week-4-review-of-visualization-in-cs230}{See here for a cheat sheet} of all the content of the above DataCamp lessons.

\end{itemize}

\item {} 
\sphinxstylestrong{Reading}
\begin{itemize}
\item {} 
{\hyperref[\detokenize{chapter-7-abstraction::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Chapter 7: Abstraction in mathematics and computing}}}}

\item {} 
{\hyperref[\detokenize{chapter-8-version-control::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Chapter 8: Version control and GitHub}}}}

\end{itemize}

\end{itemize}


\bigskip\hrule\bigskip



\section{Week 4 \sphinxhyphen{} 9/24/2020 \sphinxhyphen{} Abstraction and version control}
\label{\detokenize{course-schedule:week-4-9-24-2020-abstraction-and-version-control}}

\subsection{Content}
\label{\detokenize{course-schedule:id5}}\begin{itemize}
\item {} 
Chapter 7: Abstraction in mathematics and computing \sphinxhyphen{} {\hyperref[\detokenize{chapter-7-abstraction::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{reading}}}} and slides

\item {} 
Chapter 8: Version control and GitHub \sphinxhyphen{} {\hyperref[\detokenize{chapter-8-version-control::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{reading}}}} and slides

\end{itemize}


\subsection{Due before next class}
\label{\detokenize{course-schedule:id6}}\begin{itemize}
\item {} 
\sphinxstylestrong{Version control exercise}
\begin{itemize}
\item {} 
This assignment is described in the final slide for Chapter 8, linked to above.

\end{itemize}

\item {} 
\sphinxstylestrong{DataCamp}
\begin{itemize}
\item {} 
\sphinxhref{https://www.datacamp.com/courses/intermediate-python-for-data-science}{Intermediate Python}, chapter 5

\item {} 
\sphinxhref{https://www.datacamp.com/courses/statistical-thinking-in-python-part-1}{Statistical Thinking in Python, Part 1}, all chapters

\item {} 
\sphinxhref{https://www.datacamp.com/courses/introduction-to-data-visualization-in-python}{Introduction to Data Visualization with Python}, chapters 1 and 3 only

\item {} 
\sphinxhref{big-cheat-sheet.html\#before-week-5}{See here for a cheat sheet} of all the content of the above DataCamp lessons.

\end{itemize}

\item {} 
\sphinxstylestrong{Reading}
\begin{itemize}
\item {} 
{\hyperref[\detokenize{chapter-9-math-and-stats::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Chapter 9: Math and stats in Python}}}}

\item {} 
{\hyperref[\detokenize{chapter-10-visualization::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Chapter 10: New visualization tools}}}}

\end{itemize}

\end{itemize}


\bigskip\hrule\bigskip



\section{Week 5 \sphinxhyphen{} 10/1/2020 \sphinxhyphen{} Math and stats in Python, plus Visualization}
\label{\detokenize{course-schedule:week-5-10-1-2020-math-and-stats-in-python-plus-visualization}}

\subsection{Content}
\label{\detokenize{course-schedule:id7}}\begin{itemize}
\item {} 
Chapter 9: Math and stats in Python \sphinxhyphen{} {\hyperref[\detokenize{chapter-9-math-and-stats::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{reading}}}} and slides

\item {} 
Chapter 10: New visualization tools \sphinxhyphen{} {\hyperref[\detokenize{chapter-10-visualization::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{reading}}}} and slides

\end{itemize}


\subsection{Due before next class}
\label{\detokenize{course-schedule:id8}}\begin{itemize}
\item {} 
\sphinxstylestrong{Data preparation exercise}
\begin{itemize}
\item {} 
(Some steps of this you have probably already completed.  What’s new for everyone is making a project that can easily load the file into Jupyter, so we’re ready to experiment with it next week in class.)

\item {} 
Look at the 2016 election data \sphinxhref{https://www.npr.org/2016/11/08/500927768/2016-presidential-election-results-for-each-state}{on this page of NPR’s website}.

\item {} 
Extract the table from that page into a CSV file (for example, by copying and pasting into Excel, then touching it up as needed).

\item {} 
Write a Jupyter notebook that imports the CSV file.

\item {} 
Ensure that you remove all rows that are not for entire states (which you can do in Excel or Jupyter, whichever you prefer).

\item {} 
Publish the notebook and the dataset together to a Deepnote or Colab project.

\item {} 
Share the project URL with your instructor by email.

\item {} 
We will use this dataset in class next week.

\end{itemize}

\item {} 
\sphinxstylestrong{DataCamp}
\begin{itemize}
\item {} 
\sphinxhref{https://learn.datacamp.com/courses/merging-dataframes-with-pandas}{Merging DataFrames with pandas}, chapters 1\sphinxhyphen{}3
\begin{itemize}
\item {} 
\sphinxstylestrong{NOTE:} We will not cover this content in class next week.  We will cover it the subsequent week instead.  But I’m assigning you to do it this week because then you won’t have any homework next week, when the project is due, and you’ll be able to focus on that instead.

\end{itemize}

\item {} 
\sphinxhref{big-cheat-sheet.html\#before-week-6}{See here for a cheat sheet} of all the content of the above DataCamp lessons.

\end{itemize}

\item {} 
\sphinxstylestrong{Reading}
\begin{itemize}
\item {} 
{\hyperref[\detokenize{chapter-11-processing-rows::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Chapter 11: Processing the rows of a \sphinxcode{\sphinxupquote{DataFrame}}}}}}

\end{itemize}

\item {} 
\sphinxstylestrong{Other}
\begin{itemize}
\item {} 
Optional: There are several LOYO opportunities from this week’s course notes (chapters 9 and 10).  Consider forming a team and siezing one of the opportunities.

\end{itemize}

\end{itemize}


\bigskip\hrule\bigskip



\section{Week 6 \sphinxhyphen{} 10/8/2020 \sphinxhyphen{} Processing the Rows of a \sphinxstyleliteralintitle{\sphinxupquote{DataFrame}}}
\label{\detokenize{course-schedule:week-6-10-8-2020-processing-the-rows-of-a-dataframe}}

\subsection{Content}
\label{\detokenize{course-schedule:id9}}\begin{itemize}
\item {} 
Chapter 11: Processing the rows of a \sphinxcode{\sphinxupquote{DataFrame}} \sphinxhyphen{} {\hyperref[\detokenize{chapter-11-processing-rows::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{reading}}}} and slides

\end{itemize}


\subsection{Due before next class}
\label{\detokenize{course-schedule:id10}}\begin{itemize}
\item {} 
\sphinxstylestrong{No DataCamp this week, so that you can focus on the project.}

\item {} 
\sphinxstylestrong{Reading}
\begin{itemize}
\item {} 
{\hyperref[\detokenize{chapter-12-concat-and-merge::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Chapter 12: Concatenation and Merging}}}}

\end{itemize}

\item {} 
\sphinxstylestrong{Other}
\begin{itemize}
\item {} 
Optional: There are a few LOYO opportunities from this week’s course notes (chapter 11).  Consider forming a team and siezing one of the opportunities.

\end{itemize}

\end{itemize}


\bigskip\hrule\bigskip



\section{Week 7 \sphinxhyphen{} 10/15/2020 \sphinxhyphen{} Concatenation and Merging}
\label{\detokenize{course-schedule:week-7-10-15-2020-concatenation-and-merging}}

\subsection{Content}
\label{\detokenize{course-schedule:id11}}\begin{itemize}
\item {} 
Chapter 12: Concatenation and Merging \sphinxhyphen{} {\hyperref[\detokenize{chapter-12-concat-and-merge::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{reading}}}} and slides

\end{itemize}


\subsection{Due before next class}
\label{\detokenize{course-schedule:id12}}
It’s a light week, because you just did Project 1 and deserve a little time to rest.
\begin{itemize}
\item {} 
\sphinxstylestrong{DataCamp}
\begin{itemize}
\item {} 
\sphinxhref{https://learn.datacamp.com/courses/streamlined-data-ingestion-with-pandas}{Streamlined Data Ingestion with pandas}

\item {} 
\sphinxhref{big-cheat-sheet.html\#before-week-8}{See here for a cheat sheet} of all the content of the above DataCamp lessons.

\end{itemize}

\item {} 
\sphinxstylestrong{Reading}
\begin{itemize}
\item {} 
{\hyperref[\detokenize{chapter-13-etl::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Chapter 13: Miscellaneous Munging Methods (ETL)}}}}

\end{itemize}

\end{itemize}


\bigskip\hrule\bigskip



\section{Week 8 \sphinxhyphen{} 10/22/2020 \sphinxhyphen{} Miscellaneous Munging Methods (ETL)}
\label{\detokenize{course-schedule:week-8-10-22-2020-miscellaneous-munging-methods-etl}}

\subsection{Content}
\label{\detokenize{course-schedule:id13}}\begin{itemize}
\item {} 
Chapter 13: Miscellaneous Munging Methods (ETL) \sphinxhyphen{} {\hyperref[\detokenize{chapter-13-etl::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{reading}}}} and slides

\end{itemize}


\subsection{Due before next class}
\label{\detokenize{course-schedule:id14}}\begin{itemize}
\item {} 
\sphinxstylestrong{DataCamp} (last one for the whole semester!)
\begin{itemize}
\item {} 
\sphinxhref{https://learn.datacamp.com/courses/introduction-to-sql}{Introduction to SQL for Data Science}
\begin{itemize}
\item {} 
\sphinxstylestrong{NOTE:} Bentley’s CS350 course goes into this content in far greater detail.  You can see this lesson as a small preview or taste of that course.

\end{itemize}

\item {} 
\sphinxhref{big-cheat-sheet.html\#before-week-9}{See here for a cheat sheet} of all the content of the above DataCamp lessons.

\end{itemize}

\item {} 
\sphinxstylestrong{Reading}
\begin{itemize}
\item {} 
{\hyperref[\detokenize{chapter-14-dashboards::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Chapter 14: Dashboards}}}}

\end{itemize}

\item {} 
\sphinxstylestrong{Other}
\begin{itemize}
\item {} 
Install \sphinxhref{https://www.streamlit.io/}{Streamlit} (\sphinxcode{\sphinxupquote{pip install streamlit}}).  Take a screenshot to prove you did so.

\item {} 
\sphinxhref{https://signup.heroku.com/}{Create a Heroku account}.  Then \sphinxhref{https://devcenter.heroku.com/articles/getting-started-with-python\#set-up}{install the Heroku command\sphinxhyphen{}line tools}.  Ensure that after doing so, you can get to a terminal and run \sphinxcode{\sphinxupquote{heroku login}} successfully.  Take a screenshot to prove you did so.

\item {} 
Email both screenshots in one email to your instructor.

\end{itemize}

\item {} 
\sphinxstylestrong{Project Planning}
\begin{itemize}
\item {} 
Optional: If you want to get ahead on the final project in a way that’s rather easy and fun, start hunting for datasets that cover a topic you’re interested in and might want to analyze.  Try to find a dataset that’s pretty comprehensive, so that there are plenty of options for ways to analyze, visualize, and manipulate it.

\end{itemize}

\end{itemize}


\bigskip\hrule\bigskip



\section{Week 9 \sphinxhyphen{} 10/29/2020 \sphinxhyphen{} Dashboards}
\label{\detokenize{course-schedule:week-9-10-29-2020-dashboards}}

\subsection{Content}
\label{\detokenize{course-schedule:id15}}\begin{itemize}
\item {} 
Chapter 14: Dashboards \sphinxhyphen{} {\hyperref[\detokenize{chapter-14-dashboards::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{reading}}}} and slides

\end{itemize}


\subsection{Due before next class}
\label{\detokenize{course-schedule:id16}}\begin{itemize}
\item {} 
\sphinxstylestrong{Network data exercise}
\begin{itemize}
\item {} 
The purpose of this exercise is to familiarize you with some network data, since next week we will be studying just that.  It also gives you another chance to practice \sphinxcode{\sphinxupquote{pd.merge()}}.

\item {} 
Download this Excel workbook of shipping data among U.S. states in 1997.

\item {} 
Look over all the sheets in the workbook to familiarize yourself with their meaning.

\item {} 
Create a Jupyter notebook that reads all the sheets from the workbook.

\item {} 
Add code that creates a DataFrame just like the shipping sheet, but with each state abbreviation replaced by its full name.

\item {} 
The “adjacent” column in the distances DataFrame should be boolean type; convert it.

\item {} 
Add two columns to the shipping table, one containing the distance between the two states, and the other containing the boolean of whether the two states are adjacent, both taken from the distance table.

\item {} 
Publish the dataset and your notebook with either Deepnote or Colab, your choice.  \sphinxstylestrong{Note:} Reading Excel files requires installing the \sphinxcode{\sphinxupquote{xlrd}} module, which is not present by default in some cloud computing environments.  You may need to run \sphinxcode{\sphinxupquote{pip install xlrd}} in the terminal, or at the top of the notebook, or place it in a \sphinxcode{\sphinxupquote{requirements.txt}} file.

\item {} 
Send the link to your project to your instructor.

\end{itemize}

\item {} 
\sphinxstylestrong{Reading}
\begin{itemize}
\item {} 
{\hyperref[\detokenize{chapter-15-networks::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Chapter 15: Relations as graphs and network analysis}}}}

\end{itemize}

\end{itemize}


\bigskip\hrule\bigskip



\section{Week 10 \sphinxhyphen{} 11/5/2020 \sphinxhyphen{} Relations, graphs, and networks}
\label{\detokenize{course-schedule:week-10-11-5-2020-relations-graphs-and-networks}}

\subsection{Content}
\label{\detokenize{course-schedule:id17}}\begin{itemize}
\item {} 
Chapter 15: Relations as graphs and network analysis \sphinxhyphen{} {\hyperref[\detokenize{chapter-15-networks::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{reading}}}} and slides

\end{itemize}


\subsection{Due before next class}
\label{\detokenize{course-schedule:id18}}\begin{itemize}
\item {} 
\sphinxstylestrong{Data prep exercise for a music recommendation system}
\begin{itemize}
\item {} 
In class next time we will build a recommender system for songs (that is, given your preferences and a big database of other people’s preferences, it will try to match you with new songs you might like).

\item {} 
Visit \sphinxhref{https://archive.org/details/thisismyjam-datadump}{this page} and read about the data archive, then download it from there in ZIP format.  It is almost 1GB in size, so leave some time for this download!

\item {} 
Unzip the download and find within it three files; we care only about \sphinxcode{\sphinxupquote{jams.tsv}}.  Place this file in a folder where you can access it with Python and pandas.  It contains every user’s “jams” from 2011\sphinxhyphen{}2015.

\item {} 
Write some code to load into a pandas Series the full set of \sphinxstyleemphasis{unique} user IDs in that file.  That is, do not include any user more than once in the series.  (This code may be slow to run, because the file is large.)  This step is asking for \sphinxstyleemphasis{just the user IDs,} not any jam or song data.

\item {} 
Use the \sphinxcode{\sphinxupquote{sample()}} method in pandas Series objects to select a random subset of the users to work with, so that we don’t have to deal with the entire jams file, which would take a long time to do computations with.  Include at least 1000 in your sample, to get a sufficient representation of the full dataset.  I chose 2000 in my own work, but later computations will get much slower if you go beyond about 2000.

\item {} 
Write some code to load from the \sphinxcode{\sphinxupquote{jams.tsv}} DataFrame every jam by all the users in your sample.  There are roughly 15 jams per user on average, so you should end up with 15 times as many results as the number of users you chose (about 15,000 to 30,000).

\item {} 
We need only three columns of the result: user ID, artist, and song title.  Discard all other columns.

\item {} 
To give a song a unique name string, let’s combine the artist and song title into a single column.  That is, rather than a column with “Don’t Stop Believin’” for song title and “Journey” as artist, create a new column called “song” that contains text like “Don’t Stop Believin’, by Journey”.

\item {} 
Drop the original title and artist columns so that your final jams DataFrame contains just two columns, user and song.

\item {} 
Export that DataFrame to a new CSV file that we will analyze in class.  Call it \sphinxcode{\sphinxupquote{jam\sphinxhyphen{}sample.csv}}.

\item {} 
It should be less than 3MB, so you can email it to your instructor to demonstrate that you have done this prep work.

\end{itemize}

\item {} 
\sphinxstylestrong{Reading}
\begin{itemize}
\item {} 
{\hyperref[\detokenize{chapter-16-matrices::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Chapter 16: Relations as matrices}}}}

\end{itemize}

\end{itemize}


\bigskip\hrule\bigskip



\section{Week 11 \sphinxhyphen{} 11/12/2020 \sphinxhyphen{} Relations as matrices}
\label{\detokenize{course-schedule:week-11-11-12-2020-relations-as-matrices}}

\subsection{Content}
\label{\detokenize{course-schedule:id19}}\begin{itemize}
\item {} 
Chapter 15: Relations as matrices \sphinxhyphen{} {\hyperref[\detokenize{chapter-16-matrices::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{reading}}}} and slides

\end{itemize}


\subsection{Due before next class}
\label{\detokenize{course-schedule:id20}}\begin{itemize}
\item {} 
\sphinxstylestrong{Data preparation exercise}
\begin{itemize}
\item {} 
In class next time we will do an introductory machine learning exercise about predicting mortgage approval/denial.

\item {} 
Download the training dataset here.  It is a sample from the same mortgage dataset we’ve used many times.  Recall that its data dictionary is available \sphinxhref{https://ffiec.cfpb.gov/documentation/2018/lar-data-fields/}{online here}.

\item {} 
Load it into pandas and check the data types of the columns.

\item {} 
To make all the data numeric, we will be replacing categorical columns with boolean columns in which false is represented by 0 and true is represented by 1.  This will make it possible to use that data in a numerical model.

\item {} 
Replace the \sphinxcode{\sphinxupquote{conforming\_loan\_limit}} column with two boolean columns, one that means “conforming loan limit is C (conforming)” and one that means “conforming loan limit is NC (not conforming).”  Don’t forget to use 0/1 instead of False/True.  (There are other values that column may take on, but we will analyze just those two.)

\item {} 
Replace the \sphinxcode{\sphinxupquote{derived\_sex}} column with two boolean columns, one that means “derived sex is Male” and one that means “derived sex is Female.”  Don’t forget to use 0/1 instead of False/True.  (There are other values that column may take on, but we will analyze just those two.)

\item {} 
The \sphinxcode{\sphinxupquote{action\_taken}} column contains only 1s and 3s.  This is because this dataset was filtered to include only accepted or rejected mortgages (no withdrawals, pre\sphinxhyphen{}approvals, etc.).  Replace this column with another boolean column, still using 0/1 for False/True, meaning “application accepted.”

\item {} 
The debt\sphinxhyphen{}to\sphinxhyphen{}income ratio column is categorical instead of numeric.  Make it numeric by replacing each category with a central value in that category.  For instance, the category “20\%\sphinxhyphen{}<30\%” can be replaced with the number 25, the category “43” can be just the number 43, etc.  Let’s use 70 for “>60\%.”

\item {} 
Your newly cleaned data should have all numeric columns.  Export it as a CSV file and bring it with you to class for an in\sphinxhyphen{}class activity in Week 12.

\item {} 
To receive credit for having done this preparatory homework, also email the file to your instructor before class on Week 12.

\end{itemize}

\item {} 
\sphinxstylestrong{Reading}
\begin{itemize}
\item {} 
{\hyperref[\detokenize{chapter-17-machine-learning::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Chapter 17: Introduction to machine learning}}}}

\end{itemize}

\end{itemize}

(Perhaps more assignments are coming; this section is still incomplete.)


\bigskip\hrule\bigskip



\section{Week 12 \sphinxhyphen{} 11/19/2020 \sphinxhyphen{} Introduction to machine learning}
\label{\detokenize{course-schedule:week-12-11-19-2020-introduction-to-machine-learning}}

\subsection{Content}
\label{\detokenize{course-schedule:id21}}\begin{itemize}
\item {} 
Chapter 17: Introduction to machine learning \sphinxhyphen{} {\hyperref[\detokenize{chapter-17-machine-learning::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{reading}}}} and slides

\end{itemize}

No more homework this semester!  Use the remaining time to do a great final project!


\bigskip\hrule\bigskip



\section{Week 13 \sphinxhyphen{} 11/26/2020 \sphinxhyphen{} Thanksgiving break, no class}
\label{\detokenize{course-schedule:week-13-11-26-2020-thanksgiving-break-no-class}}
No assignments over break, but it would be wise to continue to make progress on the Final Project.


\bigskip\hrule\bigskip



\section{Week 14 \sphinxhyphen{} 12/3/2020 \sphinxhyphen{} Final Exam Review and Final Project Workshop}
\label{\detokenize{course-schedule:week-14-12-3-2020-final-exam-review-and-final-project-workshop}}

\subsection{Final Exam}
\label{\detokenize{course-schedule:final-exam}}\begin{itemize}
\item {} 
Based on the review we do in class today, study for the Final Exam.

\end{itemize}


\subsection{Final Project}
\label{\detokenize{course-schedule:final-project}}\begin{itemize}
\item {} 
Come to class today ready to use half of class to work on your final project in class, and ask questions of the instructor if/when you get stuck on anything.

\end{itemize}


\chapter{Big Cheat Sheet}
\label{\detokenize{big-cheat-sheet:big-cheat-sheet}}\label{\detokenize{big-cheat-sheet::doc}}
This file summarizes all the coding concepts learned from DataCamp in MA346, as well as those learned in CS230 that remain important in MA346.  It is broken into sections in the order in which we encounter the topics in the course, and the course schedule on {\hyperref[\detokenize{intro::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{the main page}}}} links to each section from the day on which it’s learned.


\bigskip\hrule\bigskip



\section{Before Week 2: Review of CS230}
\label{\detokenize{big-cheat-sheet:before-week-2-review-of-cs230}}

\subsection{Introduction to Python (optional, basic review)}
\label{\detokenize{big-cheat-sheet:introduction-to-python-optional-basic-review}}

\subsubsection{Chapter 1: Python Basics}
\label{\detokenize{big-cheat-sheet:chapter-1-python-basics}}
Comments, which are not executed:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Start with a hash, then explain your code.}
\end{sphinxVerbatim}

Print simple data:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(} \PYG{l+m+mi}{1} \PYG{o}{+} \PYG{l+m+mi}{5} \PYG{p}{)}
\end{sphinxVerbatim}

Storing data in a variable:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{num\PYGZus{}friends} \PYG{o}{=} \PYG{l+m+mi}{1000}
\end{sphinxVerbatim}

Integers and real numbers (“floating point”):

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{20}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{3192}\PYG{p}{,} \PYG{l+m+mf}{16.51309}\PYG{p}{,} \PYG{l+m+mf}{0.003}
\end{sphinxVerbatim}

Strings:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{You can use double quotes.}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{You can use single quotes.}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Don}\PYG{l+s+se}{\PYGZbs{}\PYGZsq{}}\PYG{l+s+s1}{t forget backslashes when needed.}\PYG{l+s+s1}{\PYGZsq{}}
\end{sphinxVerbatim}

Booleans:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kc}{True}\PYG{p}{,} \PYG{k+kc}{False}
\end{sphinxVerbatim}

Asking Python for the type of a piece of data:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{type}\PYG{p}{(} \PYG{l+m+mi}{5} \PYG{p}{)}\PYG{p}{,} \PYG{n+nb}{type}\PYG{p}{(} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{example}\PYG{l+s+s2}{\PYGZdq{}} \PYG{p}{)}\PYG{p}{,} \PYG{n+nb}{type}\PYG{p}{(} \PYG{n}{my\PYGZus{}data} \PYG{p}{)}
\end{sphinxVerbatim}

Converting among data types:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{str}\PYG{p}{(} \PYG{l+m+mi}{5} \PYG{p}{)}\PYG{p}{,} \PYG{n+nb}{int}\PYG{p}{(} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZhy{}120}\PYG{l+s+s2}{\PYGZdq{}} \PYG{p}{)}\PYG{p}{,} \PYG{n+nb}{float}\PYG{p}{(} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{0.5629}\PYG{l+s+s2}{\PYGZdq{}} \PYG{p}{)}
\end{sphinxVerbatim}

Basic arithmetic (\(+\), \(-\), \(\times\), \(\div\)):

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{l+m+mi}{1} \PYG{o}{+} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{1} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{1} \PYG{o}{*} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{1} \PYG{o}{/} \PYG{l+m+mi}{2}
\end{sphinxVerbatim}

Exponents, integer division, and remainders:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{l+m+mi}{1} \PYG{o}{*}\PYG{o}{*} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{1} \PYG{o}{/}\PYG{o}{/} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{1} \PYG{o}{\PYGZpc{}} \PYG{l+m+mi}{2}
\end{sphinxVerbatim}


\subsubsection{Chapter 2: Python Lists}
\label{\detokenize{big-cheat-sheet:chapter-2-python-lists}}
Create a list with square brackets:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{small\PYGZus{}primes} \PYG{o}{=} \PYG{p}{[} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{7}\PYG{p}{,} \PYG{l+m+mi}{11}\PYG{p}{,} \PYG{l+m+mi}{13}\PYG{p}{,} \PYG{l+m+mi}{17}\PYG{p}{,} \PYG{l+m+mi}{19}\PYG{p}{,} \PYG{l+m+mi}{23} \PYG{p}{]}
\end{sphinxVerbatim}

Lists can mix data of any type, even other lists:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Sublists are name, age, height (in m)}
\PYG{n}{heroes} \PYG{o}{=} \PYG{p}{[} \PYG{p}{[} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Harry Potter}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+m+mi}{11}\PYG{p}{,} \PYG{l+m+mf}{1.3} \PYG{p}{]}\PYG{p}{,}
           \PYG{p}{[} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Ron Weasley}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+m+mi}{11}\PYG{p}{,} \PYG{l+m+mf}{1.5} \PYG{p}{]}\PYG{p}{,}
           \PYG{p}{[} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Hermione Granger}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+m+mi}{11}\PYG{p}{,} \PYG{l+m+mf}{1.4} \PYG{p}{]} \PYG{p}{]}
\end{sphinxVerbatim}

Accessing elements from the list is zero\sphinxhyphen{}based:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{small\PYGZus{}primes}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}   \PYG{c+c1}{\PYGZsh{} == 2}
\PYG{n}{small\PYGZus{}primes}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}  \PYG{c+c1}{\PYGZsh{} == 23}
\end{sphinxVerbatim}

Slicing lists is left\sphinxhyphen{}inclusive, right\sphinxhyphen{}exclusive:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{small\PYGZus{}primes}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{:}\PYG{l+m+mi}{4}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} == [5,7]}
\PYG{n}{small\PYGZus{}primes}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{4}\PYG{p}{]}  \PYG{c+c1}{\PYGZsh{} == [2,3,5,7]}
\PYG{n}{small\PYGZus{}primes}\PYG{p}{[}\PYG{l+m+mi}{4}\PYG{p}{:}\PYG{p}{]}  \PYG{c+c1}{\PYGZsh{} == [11,13,17,19,23]}
\end{sphinxVerbatim}

It can even use a “stride” to count by something other than one:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{small\PYGZus{}primes}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{:}\PYG{l+m+mi}{7}\PYG{p}{:}\PYG{l+m+mi}{2}\PYG{p}{]}     \PYG{c+c1}{\PYGZsh{} selects items 0,2,4,6}
\PYG{n}{small\PYGZus{}primes}\PYG{p}{[}\PYG{p}{:}\PYG{p}{:}\PYG{l+m+mi}{3}\PYG{p}{]}       \PYG{c+c1}{\PYGZsh{} selects items 0,3,6}
\PYG{n}{small\PYGZus{}primes}\PYG{p}{[}\PYG{p}{:}\PYG{p}{:}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}      \PYG{c+c1}{\PYGZsh{} selects all, but in reverse}
\end{sphinxVerbatim}

If indexing gives you a list, you can index again:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{heroes}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}        \PYG{c+c1}{\PYGZsh{} == \PYGZsq{}Ron Weasley\PYGZsq{}}
\end{sphinxVerbatim}

Modify an item in a list, or a slice all at once:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{some\PYGZus{}list}\PYG{p}{[}\PYG{l+m+mi}{5}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mi}{10}
\PYG{n}{some\PYGZus{}list}\PYG{p}{[}\PYG{l+m+mi}{5}\PYG{p}{:}\PYG{l+m+mi}{10}\PYG{p}{]} \PYG{o}{=} \PYG{p}{[} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{my}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{new}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{entries}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{]}
\end{sphinxVerbatim}

Adding or removing entries from a list:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{small\PYGZus{}primes} \PYG{o}{+}\PYG{o}{=} \PYG{p}{[} \PYG{l+m+mi}{27}\PYG{p}{,} \PYG{l+m+mi}{29}\PYG{p}{,} \PYG{l+m+mi}{31} \PYG{p}{]}
\PYG{n}{small\PYGZus{}primes} \PYG{o}{=} \PYG{n}{small\PYGZus{}primes} \PYG{o}{+} \PYG{p}{[} \PYG{l+m+mi}{37}\PYG{p}{,} \PYG{l+m+mi}{41} \PYG{p}{]}
\PYG{n}{small\PYGZus{}primes}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(} \PYG{l+m+mi}{43} \PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} to add just one entry}
\PYG{k}{del}\PYG{p}{(} \PYG{n}{heroes}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]} \PYG{p}{)}    \PYG{c+c1}{\PYGZsh{} Voldemort\PYGZsq{}s goal}
\PYG{k}{del}\PYG{p}{(} \PYG{n}{heroes}\PYG{p}{[}\PYG{p}{:}\PYG{p}{]} \PYG{p}{)}    \PYG{c+c1}{\PYGZsh{} or, even better, this}
\end{sphinxVerbatim}

Copying or not copying lists:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} L will refer to the same list in memory as heroes:}
\PYG{n}{L} \PYG{o}{=} \PYG{n}{heroes}
\PYG{c+c1}{\PYGZsh{} M will refer to a full copy of the heroes array:}
\PYG{n}{M} \PYG{o}{=} \PYG{n}{heroes}\PYG{p}{[}\PYG{p}{:}\PYG{p}{]}
\end{sphinxVerbatim}


\subsubsection{Chapter 3: Functions and Packages}
\label{\detokenize{big-cheat-sheet:chapter-3-functions-and-packages}}
Calling a function and saving the result:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{lastSmallPrime} \PYG{o}{=} \PYG{n+nb}{max}\PYG{p}{(} \PYG{n}{small\PYGZus{}primes} \PYG{p}{)}
\end{sphinxVerbatim}

Getting help on a function:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{help}\PYG{p}{(} \PYG{n+nb}{max} \PYG{p}{)}
\end{sphinxVerbatim}

Methods are functions that belong to an object.  (In Python, every piece of data is an object.)

Examples:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{name} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{jerry}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{name}\PYG{o}{.}\PYG{n}{capitalize}\PYG{p}{(}\PYG{p}{)}             \PYG{c+c1}{\PYGZsh{} == \PYGZsq{}Jerry\PYGZsq{}}
\PYG{n}{name}\PYG{o}{.}\PYG{n}{count}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{r}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}             \PYG{c+c1}{\PYGZsh{} == 2}
\PYG{n}{flavors} \PYG{o}{=} \PYG{p}{[} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{vanilla}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{chocolate}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{strawberry}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{]}
\PYG{n}{flavors}\PYG{o}{.}\PYG{n}{index}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{chocolate}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} == 1}
\end{sphinxVerbatim}

Installing a package from conda:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
conda install package\PYGZus{}name
\end{sphinxVerbatim}

Ensuring conda forge packages are available:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
conda config \PYGZhy{}\PYGZhy{}add channels conda\PYGZhy{}forge
\end{sphinxVerbatim}

Installing a package from pip:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
pip3 install package\PYGZus{}name
\end{sphinxVerbatim}

Importing a package and using its contents:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{math}
\PYG{n+nb}{print}\PYG{p}{(} \PYG{n}{math}\PYG{o}{.}\PYG{n}{pi} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} or if you\PYGZsq{}ll use it a lot and want to be brief:}
\PYG{k+kn}{import} \PYG{n+nn}{math} \PYG{k}{as} \PYG{n+nn}{M}
\PYG{n+nb}{print}\PYG{p}{(} \PYG{n}{M}\PYG{o}{.}\PYG{n}{pi} \PYG{p}{)}
\end{sphinxVerbatim}

Importing just some functions from a package:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{math} \PYG{k+kn}{import} \PYG{n}{pi}\PYG{p}{,} \PYG{n}{degrees}
\PYG{n+nb}{print}\PYG{p}{(} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{The value of pi in degrees is:}\PYG{l+s+s2}{\PYGZdq{}} \PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(} \PYG{n}{degrees}\PYG{p}{(} \PYG{n}{pi} \PYG{p}{)} \PYG{p}{)}        \PYG{c+c1}{\PYGZsh{} == 180.0}
\end{sphinxVerbatim}


\subsubsection{Chapter 4: NumPy}
\label{\detokenize{big-cheat-sheet:chapter-4-numpy}}
Creating NumPy arrays from Python lists:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{n}{a} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(} \PYG{p}{[} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{9} \PYG{p}{]} \PYG{p}{)}
\end{sphinxVerbatim}

Elementise computations are supported:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{a} \PYG{o}{*} \PYG{l+m+mi}{2}       \PYG{c+c1}{\PYGZsh{} == [ 10, 20, 12, 6, 18 ]}
\PYG{n}{a} \PYG{o}{\PYGZlt{}} \PYG{l+m+mi}{10}      \PYG{c+c1}{\PYGZsh{} == [ True, False, True, True, True ]}
\end{sphinxVerbatim}

Use comparisons to subset/select:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{a}\PYG{p}{[}\PYG{n}{a} \PYG{o}{\PYGZlt{}} \PYG{l+m+mi}{10}\PYG{p}{]}   \PYG{c+c1}{\PYGZsh{} == [ 5, 6, 3, 9 ]}
\end{sphinxVerbatim}

Note: NumPy arrays don’t permit mixing data types:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(} \PYG{p}{[} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{hi}\PYG{l+s+s2}{\PYGZdq{}} \PYG{p}{]} \PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} converts all to strings}
\end{sphinxVerbatim}

NumPy arrays can be 2d, 3d, etc.:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{a} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(} \PYG{p}{[} \PYG{p}{[} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{4} \PYG{p}{]}\PYG{p}{,}
                \PYG{p}{[} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{,} \PYG{l+m+mi}{7}\PYG{p}{,} \PYG{l+m+mi}{8} \PYG{p}{]} \PYG{p}{]} \PYG{p}{)}
\PYG{n}{a}\PYG{o}{.}\PYG{n}{shape}     \PYG{c+c1}{\PYGZsh{} == (2,4)}
\end{sphinxVerbatim}

You can index/select with comma notation:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{]}      \PYG{c+c1}{\PYGZsh{} == 8}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{:}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{:}\PYG{l+m+mi}{2}\PYG{p}{]}  \PYG{c+c1}{\PYGZsh{} == [[1,2],[5,6]]}
\PYG{n}{a}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}      \PYG{c+c1}{\PYGZsh{} == [3,7]}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{p}{:}\PYG{p}{]}      \PYG{c+c1}{\PYGZsh{} == [1,2,3,4]}
\end{sphinxVerbatim}

Fast NumPy versions of Python functions, and some new ones:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(} \PYG{n}{a} \PYG{p}{)}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{sort}\PYG{p}{(} \PYG{n}{a} \PYG{p}{)}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(} \PYG{n}{a} \PYG{p}{)}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{median}\PYG{p}{(} \PYG{n}{a} \PYG{p}{)}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{std}\PYG{p}{(} \PYG{n}{a} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} and others}
\end{sphinxVerbatim}


\subsection{Python Data Science Toolbox, Part 1 (optional, basic review)}
\label{\detokenize{big-cheat-sheet:python-data-science-toolbox-part-1-optional-basic-review}}

\subsubsection{Chapter 1: Writing your own functions}
\label{\detokenize{big-cheat-sheet:chapter-1-writing-your-own-functions}}
Tuples are like lists, but use parentheses, and are immutable.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{t} \PYG{o}{=} \PYG{p}{(} \PYG{l+m+mi}{6}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{7} \PYG{p}{)}        \PYG{c+c1}{\PYGZsh{} create a tuple}
\PYG{n}{t}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}                   \PYG{c+c1}{\PYGZsh{} == 6}
\PYG{n}{a}\PYG{p}{,} \PYG{n}{b}\PYG{p}{,} \PYG{n}{c} \PYG{o}{=} \PYG{n}{t}            \PYG{c+c1}{\PYGZsh{} a==6, b==1, c==7}
\end{sphinxVerbatim}

Syntax for defining a function:

(A function that modifies any global variables needs the Python \sphinxcode{\sphinxupquote{global}} keyword inside to identify those variables.)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{function\PYGZus{}name} \PYG{p}{(} \PYG{n}{arguments} \PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}Write a docstring describing the function.\PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{c+c1}{\PYGZsh{} do some things here.}
    \PYG{c+c1}{\PYGZsh{} note the indentation!}
    \PYG{c+c1}{\PYGZsh{} and optionally:}
    \PYG{k}{return} \PYG{n}{some\PYGZus{}value}
    \PYG{c+c1}{\PYGZsh{} to return multiple values: return v1, v2}
\end{sphinxVerbatim}

Syntax for calling a function:

(Note the distinction between “arguments”  and “parameters.”)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} if you do not care about a return value:}
\PYG{n}{function\PYGZus{}name}\PYG{p}{(} \PYG{n}{parameters} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} if you wish to store the return value:}
\PYG{n}{my\PYGZus{}variable} \PYG{o}{=} \PYG{n}{function\PYGZus{}name}\PYG{p}{(} \PYG{n}{parameters} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} if the function returns multiple values:}
\PYG{n}{var1}\PYG{p}{,} \PYG{n}{var2} \PYG{o}{=} \PYG{n}{function\PYGZus{}name}\PYG{p}{(} \PYG{n}{parameters} \PYG{p}{)}
\end{sphinxVerbatim}


\subsubsection{Chapter 2: Default arguments, variable\sphinxhyphen{}length arguments, and scope}
\label{\detokenize{big-cheat-sheet:chapter-2-default-arguments-variable-length-arguments-and-scope}}
Defining nested functions:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{multiply\PYGZus{}by} \PYG{p}{(} \PYG{n}{x} \PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}Creates a function that multiplies by x\PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{def} \PYG{n+nf}{result} \PYG{p}{(} \PYG{n}{y} \PYG{p}{)}\PYG{p}{:}
        \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}Multiplies x by y\PYGZdq{}\PYGZdq{}\PYGZdq{}}
        \PYG{k}{return} \PYG{n}{x} \PYG{o}{*} \PYG{n}{y}
    \PYG{k}{return} \PYG{n}{result}
\PYG{c+c1}{\PYGZsh{} example usage:}
\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{height\PYGZus{}in\PYGZus{}inches}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{apply}\PYG{p}{(}
    \PYG{n}{multiply\PYGZus{}by}\PYG{p}{(} \PYG{l+m+mf}{2.54} \PYG{p}{)} \PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} result is now in cm}
\end{sphinxVerbatim}

Providing default values for arguments:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{rand\PYGZus{}between} \PYG{p}{(} \PYG{n}{a}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{b}\PYG{o}{=}\PYG{l+m+mi}{1} \PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}Gives a random float between a and b\PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(}\PYG{p}{)} \PYG{o}{*} \PYG{p}{(} \PYG{n}{b} \PYG{o}{\PYGZhy{}} \PYG{n}{a} \PYG{p}{)} \PYG{o}{+} \PYG{n}{a}
\end{sphinxVerbatim}

Accepting any number of arguments:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{commas\PYGZus{}between} \PYG{p}{(} \PYG{o}{*}\PYG{n}{args} \PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}Returns the args as a string with commas\PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{result} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}
    \PYG{k}{for} \PYG{n}{item} \PYG{o+ow}{in} \PYG{n}{args}\PYG{p}{:}
        \PYG{n}{result} \PYG{o}{+}\PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{, }\PYG{l+s+s2}{\PYGZdq{}} \PYG{o}{+} \PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{item}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{result}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{:}\PYG{p}{]}
\PYG{n}{commas\PYGZus{}between}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{hi}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+m+mi}{7}\PYG{p}{)}    \PYG{c+c1}{\PYGZsh{} == \PYGZdq{}1,hi,7\PYGZdq{}}
\end{sphinxVerbatim}

Accepting a dictionary of arguments:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{inverted} \PYG{p}{(} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs} \PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}Interchanges keys and values in a dict\PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{result} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}
    \PYG{k}{for} \PYG{n}{key}\PYG{p}{,} \PYG{n}{value} \PYG{o+ow}{in} \PYG{n}{kwargs}\PYG{o}{.}\PYG{n}{items}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{result}\PYG{p}{[}\PYG{n}{value}\PYG{p}{]} \PYG{o}{=} \PYG{n}{key}
    \PYG{k}{return} \PYG{n}{result}
\PYG{n}{inverted}\PYG{p}{(} \PYG{n}{jim}\PYG{o}{=}\PYG{l+m+mi}{42}\PYG{p}{,} \PYG{n}{angie}\PYG{o}{=}\PYG{l+m+mi}{9} \PYG{p}{)}
        \PYG{c+c1}{\PYGZsh{} == \PYGZob{} 42 : \PYGZsq{}jim\PYGZsq{}, 9 : \PYGZsq{}angie\PYGZsq{} \PYGZcb{}}
\end{sphinxVerbatim}


\subsubsection{Chapter 3: Lambda functions and error handling}
\label{\detokenize{big-cheat-sheet:chapter-3-lambda-functions-and-error-handling}}
Anonymous functions:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{lambda} \PYG{n}{arg1}\PYG{p}{,} \PYG{n}{arg2}\PYG{p}{:} \PYG{n}{return\PYGZus{}value\PYGZus{}here}
\PYG{c+c1}{\PYGZsh{} example:}
\PYG{k}{lambda} \PYG{n}{k}\PYG{p}{:} \PYG{n}{k} \PYG{o}{\PYGZpc{}} \PYG{l+m+mi}{2} \PYG{o}{==} \PYG{l+m+mi}{0}    \PYG{c+c1}{\PYGZsh{} detects whether k is even}
\end{sphinxVerbatim}

Some examples in which anonymous functions are useful:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{list}\PYG{p}{(} \PYG{n+nb}{map}\PYG{p}{(} \PYG{k}{lambda} \PYG{n}{k}\PYG{p}{:} \PYG{n}{k}\PYG{o}{\PYGZpc{}}\PYG{l+m+mi}{2}\PYG{o}{==}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{]} \PYG{p}{)} \PYG{p}{)}
                   \PYG{c+c1}{\PYGZsh{} == [False,True,False,True,False]}
\PYG{n+nb}{list}\PYG{p}{(} \PYG{n+nb}{filter}\PYG{p}{(} \PYG{k}{lambda} \PYG{n}{k}\PYG{p}{:} \PYG{n}{k}\PYG{o}{\PYGZpc{}}\PYG{l+m+mi}{2}\PYG{o}{==}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{]} \PYG{p}{)} \PYG{p}{)}
                   \PYG{c+c1}{\PYGZsh{} == [2,4]}
\PYG{n}{reduce}\PYG{p}{(} \PYG{k}{lambda} \PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{:} \PYG{n}{x}\PYG{o}{*}\PYG{n}{y}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{]} \PYG{p}{)}
                   \PYG{c+c1}{\PYGZsh{} == 120 (1*2*3*4*5)}
\end{sphinxVerbatim}

Raising errors if users call your functions incorrectly:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} You can detect problems in advance:}
\PYG{k}{def} \PYG{n+nf}{factorial} \PYG{p}{(} \PYG{n}{n} \PYG{p}{)}\PYG{p}{:}
    \PYG{k}{if} \PYG{n+nb}{type}\PYG{p}{(} \PYG{n}{n} \PYG{p}{)} \PYG{o}{!=} \PYG{n+nb}{int}\PYG{p}{:}
        \PYG{k}{raise} \PYG{n+ne}{TypeError}\PYG{p}{(} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{n must be an int}\PYG{l+s+s2}{\PYGZdq{}} \PYG{p}{)}
    \PYG{k}{if} \PYG{n}{n} \PYG{o}{\PYGZlt{}} \PYG{l+m+mi}{0}\PYG{p}{:}
        \PYG{k}{raise} \PYG{n+ne}{ValueError}\PYG{p}{(} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{n must be nonnegative}\PYG{l+s+s2}{\PYGZdq{}} \PYG{p}{)}
    \PYG{k}{return} \PYG{n}{reduce}\PYG{p}{(} \PYG{k}{lambda} \PYG{n}{x}\PYG{p}{,}\PYG{n}{y}\PYG{p}{:} \PYG{n}{x}\PYG{o}{*}\PYG{n}{y}\PYG{p}{,} \PYG{n+nb}{range}\PYG{p}{(} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{n}\PYG{o}{+}\PYG{l+m+mi}{1} \PYG{p}{)} \PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Or you can let Python detect them:}
\PYG{k}{def} \PYG{n+nf}{solve\PYGZus{}equation} \PYG{p}{(} \PYG{n}{a}\PYG{p}{,} \PYG{n}{b} \PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}Solves a*x+b=0 for x\PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{try}\PYG{p}{:}
        \PYG{k}{return} \PYG{o}{\PYGZhy{}}\PYG{n}{b} \PYG{o}{/} \PYG{n}{a}
    \PYG{k}{except}\PYG{p}{:}
        \PYG{k}{return} \PYG{k+kc}{None}
\PYG{n}{solve\PYGZus{}equation}\PYG{p}{(} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1} \PYG{p}{)}    \PYG{c+c1}{\PYGZsh{} == 0.5}
\PYG{n}{solve\PYGZus{}equation}\PYG{p}{(} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{5} \PYG{p}{)}     \PYG{c+c1}{\PYGZsh{} == None}
\end{sphinxVerbatim}


\subsection{Intermediate Python (required review)}
\label{\detokenize{big-cheat-sheet:intermediate-python-required-review}}

\subsubsection{Chapter 1: Matplotlib}
\label{\detokenize{big-cheat-sheet:chapter-1-matplotlib}}
Conventional way to import matplotlib:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\end{sphinxVerbatim}

Creating a line plot:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(} \PYG{n}{x\PYGZus{}data}\PYG{p}{,} \PYG{n}{y\PYGZus{}data} \PYG{p}{)}     \PYG{c+c1}{\PYGZsh{} create plot}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}                     \PYG{c+c1}{\PYGZsh{} display plot}
\end{sphinxVerbatim}

Creating a scatter plot:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(} \PYG{n}{x\PYGZus{}data}\PYG{p}{,} \PYG{n}{y\PYGZus{}data} \PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} create plot}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}                     \PYG{c+c1}{\PYGZsh{} display plot}
\PYG{c+c1}{\PYGZsh{} or this alternative form:}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(} \PYG{n}{x\PYGZus{}data}\PYG{p}{,} \PYG{n}{y\PYGZus{}data}\PYG{p}{,} \PYG{n}{kind}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{scatter}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

Labeling axes and adding title:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x axis label here}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y axis label here}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Title of Plot}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\end{sphinxVerbatim}


\subsubsection{Chapter 2: Dictionaries \& Pandas}
\label{\detokenize{big-cheat-sheet:chapter-2-dictionaries-pandas}}
Creating a dictionary directly:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{days\PYGZus{}in\PYGZus{}month} \PYG{o}{=} \PYG{p}{\PYGZob{}}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{january}\PYG{l+s+s2}{\PYGZdq{}}  \PYG{p}{:} \PYG{l+m+mi}{31}\PYG{p}{,}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{february}\PYG{l+s+s2}{\PYGZdq{}} \PYG{p}{:} \PYG{l+m+mi}{28}\PYG{p}{,}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{march}\PYG{l+s+s2}{\PYGZdq{}}    \PYG{p}{:} \PYG{l+m+mi}{31}\PYG{p}{,}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{april}\PYG{l+s+s2}{\PYGZdq{}}    \PYG{p}{:} \PYG{l+m+mi}{30}\PYG{p}{,}
    \PYG{c+c1}{\PYGZsh{} and so on, until...}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{december}\PYG{l+s+s2}{\PYGZdq{}} \PYG{p}{:} \PYG{l+m+mi}{31}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

Getting and using keys:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{days\PYGZus{}in\PYGZus{}month}\PYG{o}{.}\PYG{n}{keys}\PYG{p}{(}\PYG{p}{)}    \PYG{c+c1}{\PYGZsh{} == [\PYGZdq{}january\PYGZdq{},}
                        \PYG{c+c1}{\PYGZsh{}     \PYGZdq{}february\PYGZdq{},...]}
\PYG{n}{days\PYGZus{}in\PYGZus{}month}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{april}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}  \PYG{c+c1}{\PYGZsh{} == 30}
\end{sphinxVerbatim}

Updating dictionary and checking membership:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{days\PYGZus{}in\PYGZus{}month}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{february}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mi}{29}   \PYG{c+c1}{\PYGZsh{} update for 2020}
\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{tuesday}\PYG{l+s+s2}{\PYGZdq{}} \PYG{o+ow}{in} \PYG{n}{days\PYGZus{}in\PYGZus{}month}       \PYG{c+c1}{\PYGZsh{} == False}
\PYG{n}{days\PYGZus{}in\PYGZus{}month}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{tuesday}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mi}{9}     \PYG{c+c1}{\PYGZsh{} a mistake}
\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{tuesday}\PYG{l+s+s2}{\PYGZdq{}} \PYG{o+ow}{in} \PYG{n}{days\PYGZus{}in\PYGZus{}month}       \PYG{c+c1}{\PYGZsh{} == True}
\PYG{k}{del}\PYG{p}{(} \PYG{n}{days\PYGZus{}in\PYGZus{}month}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{tuesday}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]} \PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} delete mistake}
\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{tuesday}\PYG{l+s+s2}{\PYGZdq{}} \PYG{o+ow}{in} \PYG{n}{days\PYGZus{}in\PYGZus{}month}       \PYG{c+c1}{\PYGZsh{} == False}
\end{sphinxVerbatim}

Build manually from dictionary:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(} \PYG{p}{\PYGZob{}}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{column label 1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{[}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{this example uses...}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{string data here.}\PYG{l+s+s2}{\PYGZdq{}}
    \PYG{p}{]}\PYG{p}{,}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{column label 2}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{[}
        \PYG{l+m+mf}{100.65}\PYG{p}{,}  \PYG{c+c1}{\PYGZsh{} and numerical data}
        \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{92.04}   \PYG{c+c1}{\PYGZsh{} here, for example}
    \PYG{p}{]}
    \PYG{c+c1}{\PYGZsh{} and more columns if needed}
\PYG{p}{\PYGZcb{}} \PYG{p}{)}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{index} \PYG{o}{=} \PYG{p}{[}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{put your...}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{row labels here.}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{p}{]}
\end{sphinxVerbatim}

Import from CSV file:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} if row and column headers are in first row/column:}
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{/path/to/file.csv}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                  \PYG{n}{index\PYGZus{}col} \PYG{o}{=} \PYG{l+m+mi}{0} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} if no row headers:}
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{/path/to/file.csv}\PYG{l+s+s2}{\PYGZdq{}} \PYG{p}{)}
\end{sphinxVerbatim}

Indexing and selecting data:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{column name}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}    \PYG{c+c1}{\PYGZsh{} is a \PYGZdq{}Series\PYGZdq{} (labeled column)}
\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{column name}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{values}\PYG{p}{(}\PYG{p}{)}
                     \PYG{c+c1}{\PYGZsh{} extract just its values}
\PYG{n}{df}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{column name}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{]}  \PYG{c+c1}{\PYGZsh{} is a 1\PYGZhy{}column dataframe}
\PYG{n}{df}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{col1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{col2}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{]}  \PYG{c+c1}{\PYGZsh{} is a 2\PYGZhy{}column dataframe}
\PYG{n}{df}\PYG{p}{[}\PYG{n}{n}\PYG{p}{:}\PYG{n}{m}\PYG{p}{]}              \PYG{c+c1}{\PYGZsh{} slice of rows, a dataframe}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{row name}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}   \PYG{c+c1}{\PYGZsh{} is a \PYGZdq{}Series\PYGZdq{} (labeled column)}
                     \PYG{c+c1}{\PYGZsh{} yes, the row becomes a column}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{row name}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} 1\PYGZhy{}row dataframe}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{r1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{r2}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{r3}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{]}
                     \PYG{c+c1}{\PYGZsh{} 3\PYGZhy{}row dataframe}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{r1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{r2}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{r3}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}\PYG{p}{:}\PYG{p}{]}
                     \PYG{c+c1}{\PYGZsh{} same as previous}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{c1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{c2}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{c3}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{]}
                     \PYG{c+c1}{\PYGZsh{} 3\PYGZhy{}column dataframe}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{r1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{r2}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{r3}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{c1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{c2}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{]}
                     \PYG{c+c1}{\PYGZsh{} 3x2 slice of the dataframe}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{5}\PYG{p}{]}\PYG{p}{]}         \PYG{c+c1}{\PYGZsh{} is a \PYGZdq{}Series\PYGZdq{} (labeled column)}
                     \PYG{c+c1}{\PYGZsh{} contains the 6th row\PYGZsq{}s data}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{5}\PYG{p}{,}\PYG{l+m+mi}{6}\PYG{p}{,}\PYG{l+m+mi}{7}\PYG{p}{]}\PYG{p}{]}     \PYG{c+c1}{\PYGZsh{} 3\PYGZhy{}row dataframe (6th\PYGZhy{}8th)}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{5}\PYG{p}{,}\PYG{l+m+mi}{6}\PYG{p}{,}\PYG{l+m+mi}{7}\PYG{p}{]}\PYG{p}{,}\PYG{p}{:}\PYG{p}{]}   \PYG{c+c1}{\PYGZsh{} same as previous}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{]}\PYG{p}{]}     \PYG{c+c1}{\PYGZsh{} 2\PYGZhy{}column dataframe}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{5}\PYG{p}{,}\PYG{l+m+mi}{6}\PYG{p}{,}\PYG{l+m+mi}{7}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{]}\PYG{p}{]}
                     \PYG{c+c1}{\PYGZsh{} 3x2 slice of the dataframe}
\end{sphinxVerbatim}


\subsubsection{Chapter 3: Logic, Control Flow, and Filtering}
\label{\detokenize{big-cheat-sheet:chapter-3-logic-control-flow-and-filtering}}
Python relations work on NumPy arrays and Pandas Series:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZlt{}}\PYG{p}{,} \PYG{o}{\PYGZlt{}}\PYG{o}{=}\PYG{p}{,} \PYG{o}{\PYGZgt{}}\PYG{p}{,} \PYG{o}{\PYGZgt{}}\PYG{o}{=}\PYG{p}{,} \PYG{o}{==}\PYG{p}{,} \PYG{o}{!=}
\end{sphinxVerbatim}

Logical operators can combine the above relations:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o+ow}{and}\PYG{p}{,} \PYG{o+ow}{or}\PYG{p}{,} \PYG{o+ow}{not}          \PYG{c+c1}{\PYGZsh{} use these on booleans}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{logical\PYGZus{}and}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,}\PYG{n}{y}\PYG{p}{)}   \PYG{c+c1}{\PYGZsh{} use these on numpy arrays}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{logical\PYGZus{}or}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,}\PYG{n}{y}\PYG{p}{)}    \PYG{c+c1}{\PYGZsh{} (assuming you have imported}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{logical\PYGZus{}not}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}     \PYG{c+c1}{\PYGZsh{} numpy as np)}
\end{sphinxVerbatim}

Filtering Pandas DataFrames:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{series} \PYG{o}{=} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{column}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
\PYG{n+nb}{filter} \PYG{o}{=} \PYG{n}{series} \PYG{o}{\PYGZgt{}} \PYG{n}{some\PYGZus{}number}
\PYG{n}{df}\PYG{p}{[}\PYG{n+nb}{filter}\PYG{p}{]}  \PYG{c+c1}{\PYGZsh{} new dataframe, a subset of the rows}
\PYG{c+c1}{\PYGZsh{} or all at once:}
\PYG{n}{df}\PYG{p}{[}\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{column}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]} \PYG{o}{\PYGZgt{}} \PYG{n}{some\PYGZus{}number}\PYG{p}{]}
\PYG{c+c1}{\PYGZsh{} combining multiple conditions:}
\PYG{n}{df}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{logical\PYGZus{}and}\PYG{p}{(} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{population}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{5000}\PYG{p}{,}
                   \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{area}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]} \PYG{o}{\PYGZlt{}} \PYG{l+m+mi}{1250} \PYG{p}{)}\PYG{p}{]}
\end{sphinxVerbatim}

Conditional statements:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Take an action if a condition is true:}
\PYG{k}{if} \PYG{n}{put\PYGZus{}condition\PYGZus{}here}\PYG{p}{:}
    \PYG{n}{take\PYGZus{}an\PYGZus{}action}\PYG{p}{(}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} Take a different action if the condition is false:}
\PYG{k}{if} \PYG{n}{put\PYGZus{}condition\PYGZus{}here}\PYG{p}{:}
    \PYG{n}{take\PYGZus{}an\PYGZus{}action}\PYG{p}{(}\PYG{p}{)}
\PYG{k}{else}\PYG{p}{:}
    \PYG{n}{do\PYGZus{}this\PYGZus{}instead}\PYG{p}{(}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} Consider multiple conditions:}
\PYG{k}{if} \PYG{n}{put\PYGZus{}condition\PYGZus{}here}\PYG{p}{:}
    \PYG{n}{take\PYGZus{}an\PYGZus{}action}\PYG{p}{(}\PYG{p}{)}
\PYG{k}{elif} \PYG{n}{other\PYGZus{}condition\PYGZus{}here}\PYG{p}{:}
    \PYG{n}{do\PYGZus{}this\PYGZus{}instead}\PYG{p}{(}\PYG{p}{)}
\PYG{k}{elif} \PYG{n}{yet\PYGZus{}another\PYGZus{}condition}\PYG{p}{:}
    \PYG{n}{do\PYGZus{}this\PYGZus{}instead2}\PYG{p}{(}\PYG{p}{)}
\PYG{k}{else}\PYG{p}{:}
    \PYG{n}{finally\PYGZus{}this}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}


\subsubsection{Chapter 4: Loops}
\label{\detokenize{big-cheat-sheet:chapter-4-loops}}
Looping constructs:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{while} \PYG{n}{some\PYGZus{}condition}\PYG{p}{:}
    \PYG{n}{do\PYGZus{}this\PYGZus{}repeatedly}\PYG{p}{(}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{} as many lines of code here as you like.}
    \PYG{c+c1}{\PYGZsh{} note that indentation is crucial!}
    \PYG{c+c1}{\PYGZsh{} be sure to work towards some\PYGZus{}condition}
    \PYG{c+c1}{\PYGZsh{} becoming false eventually!}

\PYG{k}{for} \PYG{n}{item} \PYG{o+ow}{in} \PYG{n}{my\PYGZus{}list}\PYG{p}{:}
    \PYG{n}{do\PYGZus{}something\PYGZus{}with}\PYG{p}{(} \PYG{n}{item} \PYG{p}{)}

\PYG{k}{for} \PYG{n}{index}\PYG{p}{,} \PYG{n}{item} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(} \PYG{n}{my\PYGZus{}list} \PYG{p}{)}\PYG{p}{:}
    \PYG{n+nb}{print}\PYG{p}{(} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{item }\PYG{l+s+s2}{\PYGZdq{}} \PYG{o}{+} \PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{index}\PYG{p}{)} \PYG{o}{+}
           \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ is }\PYG{l+s+s2}{\PYGZdq{}} \PYG{o}{+} \PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{item}\PYG{p}{)} \PYG{p}{)}

\PYG{k}{for} \PYG{n}{key}\PYG{p}{,} \PYG{n}{value} \PYG{o+ow}{in} \PYG{n}{my\PYGZus{}dict}\PYG{o}{.}\PYG{n}{items}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{n+nb}{print}\PYG{p}{(} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{key }\PYG{l+s+s2}{\PYGZdq{}} \PYG{o}{+} \PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{key}\PYG{p}{)} \PYG{o}{+}
           \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ has value }\PYG{l+s+s2}{\PYGZdq{}} \PYG{o}{+} \PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{value}\PYG{p}{)} \PYG{p}{)}

\PYG{k}{for} \PYG{n}{item} \PYG{o+ow}{in} \PYG{n}{my\PYGZus{}numpy\PYGZus{}array}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} works if the array is one\PYGZhy{}dimensional}
    \PYG{n+nb}{print}\PYG{p}{(} \PYG{n}{item} \PYG{p}{)}

\PYG{k}{for} \PYG{n}{item} \PYG{o+ow}{in} \PYG{n}{np}\PYG{o}{.}\PYG{n}{nditer}\PYG{p}{(} \PYG{n}{my\PYGZus{}numpy\PYGZus{}array} \PYG{p}{)}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} if it is 2d, 3d, or more}
    \PYG{n+nb}{print}\PYG{p}{(} \PYG{n}{item} \PYG{p}{)}

\PYG{k}{for} \PYG{n}{column\PYGZus{}name} \PYG{o+ow}{in} \PYG{n}{my\PYGZus{}dataframe}\PYG{p}{:}
    \PYG{n}{work\PYGZus{}with}\PYG{p}{(} \PYG{n}{my\PYGZus{}dataframe}\PYG{p}{[}\PYG{n}{column\PYGZus{}name}\PYG{p}{]} \PYG{p}{)}

\PYG{k}{for} \PYG{n}{row\PYGZus{}name}\PYG{p}{,} \PYG{n}{row} \PYG{o+ow}{in} \PYG{n}{my\PYGZus{}dataframe}\PYG{o}{.}\PYG{n}{iterrows}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{n+nb}{print}\PYG{p}{(} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{row }\PYG{l+s+s2}{\PYGZdq{}} \PYG{o}{+} \PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{row\PYGZus{}name}\PYG{p}{)} \PYG{o}{+}
           \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ has these entries: }\PYG{l+s+s2}{\PYGZdq{}} \PYG{o}{+} \PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{row}\PYG{p}{)} \PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} in dataframes, sometimes you can skip the for loop:}
\PYG{n}{my\PYGZus{}dataframe}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{column}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{apply}\PYG{p}{(} \PYG{n}{function} \PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} a Series}
\end{sphinxVerbatim}


\subsection{pandas Foundations (required review)}
\label{\detokenize{big-cheat-sheet:pandas-foundations-required-review}}

\subsubsection{Chapter 1: Data ingestion \& inspection}
\label{\detokenize{big-cheat-sheet:chapter-1-data-ingestion-inspection}}
Basic DataFrame/Series tools:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{l+m+mi}{5}\PYG{p}{)}           \PYG{c+c1}{\PYGZsh{} first five rows}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{tail}\PYG{p}{(}\PYG{l+m+mi}{5}\PYG{p}{)}           \PYG{c+c1}{\PYGZsh{} last five rows}
\PYG{n}{series}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{l+m+mi}{5}\PYG{p}{)}       \PYG{c+c1}{\PYGZsh{} head, tail also work on series}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{p}{)}            \PYG{c+c1}{\PYGZsh{} summary of the data types used}
\end{sphinxVerbatim}

Adding details to reading DataFrames from CSV files:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} if no column headers:}
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{/path/to/file.csv}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                  \PYG{n}{index\PYGZus{}col} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{header} \PYG{o}{=} \PYG{k+kc}{None}\PYG{p}{,}
                  \PYG{n}{names} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{column}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{names}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{here}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} if any missing data you want to mark as NaN:}
\PYG{c+c1}{\PYGZsh{} (na\PYGZus{}values can be a list of patterns,}
\PYG{c+c1}{\PYGZsh{} or a dict mapping column names to patterns/lists)}
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{/path/to/file.csv}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                  \PYG{n}{na\PYGZus{}values} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{pattern to replace}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} and many other options!  (see the documentation)}
\end{sphinxVerbatim}

To get a DataFrame with a date/time index:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} read as dates any columns that pandas can:}
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{/path/to/file.csv}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                  \PYG{n}{parse\PYGZus{}dates} \PYG{o}{=} \PYG{k+kc}{True} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} read as dates just the columns you specify:}
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{/path/to/file.csv}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                  \PYG{n}{parse\PYGZus{}dates} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{column}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{names}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} to use one of those columns as a date/time index:}
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{/path/to/file.csv}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                  \PYG{n}{parse\PYGZus{}dates} \PYG{o}{=} \PYG{k+kc}{True}\PYG{p}{,}
                  \PYG{n}{index\PYGZus{}col} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Date}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} combine multiple columns to form a date:}
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{/path/to/file.csv}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                  \PYG{n}{parse\PYGZus{}dates} \PYG{o}{=} \PYG{p}{[}\PYG{p}{[}\PYG{n}{column}\PYG{p}{,}\PYG{n}{indices}\PYG{p}{]}\PYG{p}{]} \PYG{p}{)}
\end{sphinxVerbatim}

Export to CSV or XLSX file:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{o}{.}\PYG{n}{to\PYGZus{}csv}\PYG{p}{(} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{/path/to/output\PYGZus{}file.csv}\PYG{l+s+s2}{\PYGZdq{}} \PYG{p}{)}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{to\PYGZus{}excel}\PYG{p}{(} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{/path/to/output\PYGZus{}file.xlsx}\PYG{l+s+s2}{\PYGZdq{}} \PYG{p}{)}
\end{sphinxVerbatim}

You can also create a plot from a \sphinxcode{\sphinxupquote{Series}} or dataframe:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{)}            \PYG{c+c1}{\PYGZsh{} or series.plot()}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} or to show each column in a subplot:}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(} \PYG{n}{subplots} \PYG{o}{=} \PYG{k+kc}{True} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} or to plot certain columns:}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{col name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{other col name}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

A few small ways to customize plots:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xscale}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{log}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{yticks}\PYG{p}{(} \PYG{p}{[} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{20} \PYG{p}{]} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{grid}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

To create a histogram:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{hist}\PYG{p}{(} \PYG{n}{data}\PYG{p}{,} \PYG{n}{bins}\PYG{o}{=}\PYG{l+m+mi}{10} \PYG{p}{)}      \PYG{c+c1}{\PYGZsh{} 10 is the default}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

To “clean up” so you can start a new plot:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{clf}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

Write text onto a plot:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{text}\PYG{p}{(} \PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Text to write}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\end{sphinxVerbatim}

To save a plot to a file:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} before plt.show(), call:}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{savefig}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{filename.png}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} or .jpg or .pdf}
\end{sphinxVerbatim}


\subsection{Manipulating DataFrames with pandas (required review)}
\label{\detokenize{big-cheat-sheet:manipulating-dataframes-with-pandas-required-review}}

\subsubsection{Chapter 1: Extracting and transforming data}
\label{\detokenize{big-cheat-sheet:chapter-1-extracting-and-transforming-data}}
(This builds on the DataCamp Intermediate Python section.)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{l+m+mi}{5}\PYG{p}{:}\PYG{l+m+mi}{7}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{:}\PYG{l+m+mi}{4}\PYG{p}{]}     \PYG{c+c1}{\PYGZsh{} select ranges of rows/columns}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{:}\PYG{l+m+mi}{4}\PYG{p}{]}       \PYG{c+c1}{\PYGZsh{} select a range, all rows}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{5}\PYG{p}{,}\PYG{l+m+mi}{6}\PYG{p}{]}\PYG{p}{,}\PYG{p}{:}\PYG{p}{]}     \PYG{c+c1}{\PYGZsh{} select a range, all columns}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{l+m+mi}{5}\PYG{p}{:}\PYG{p}{,}\PYG{p}{:}\PYG{p}{]}        \PYG{c+c1}{\PYGZsh{} all but the first five rows}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{A}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{B}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{p}{:}\PYG{p}{]}    \PYG{c+c1}{\PYGZsh{} colons can take row names too}
                     \PYG{c+c1}{\PYGZsh{} (but include both endpoints)}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{C}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{D}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}    \PYG{c+c1}{\PYGZsh{} ...also column names}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{D}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{A}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}   \PYG{c+c1}{\PYGZsh{} rows by name, reverse order}
\end{sphinxVerbatim}

(This builds on the DataCamp Intermediate Python section.)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} avoid using np.logical\PYGZus{}and with \PYGZam{} instead:}
\PYG{n}{df}\PYG{p}{[}\PYG{p}{(}\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{population}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{5000}\PYG{p}{)}
 \PYG{o}{\PYGZam{}} \PYG{p}{(}\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{area}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]} \PYG{o}{\PYGZlt{}} \PYG{l+m+mi}{1250} \PYG{p}{)}\PYG{p}{]}
\PYG{c+c1}{\PYGZsh{} avoid using np.logical\PYGZus{}or with | instead:}
\PYG{n}{df}\PYG{p}{[}\PYG{p}{(}\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{population}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{5000}\PYG{p}{)}
 \PYG{o}{|} \PYG{p}{(}\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{area}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]} \PYG{o}{\PYGZlt{}} \PYG{l+m+mi}{1250} \PYG{p}{)}\PYG{p}{]}
\PYG{c+c1}{\PYGZsh{} filtering for missing values:}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{n}{df}\PYG{o}{.}\PYG{n}{all}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}  \PYG{c+c1}{\PYGZsh{} only columns with no zeroes}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{n}{df}\PYG{o}{.}\PYG{n}{any}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}  \PYG{c+c1}{\PYGZsh{} only columns with some nonzero}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{n}{df}\PYG{o}{.}\PYG{n}{isnull}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{any}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}
                    \PYG{c+c1}{\PYGZsh{} only columns with a NaN entry}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{n}{df}\PYG{o}{.}\PYG{n}{notnull}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{all}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}
                    \PYG{c+c1}{\PYGZsh{} only columns with no NaNs}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{dropna}\PYG{p}{(} \PYG{n}{how}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{any}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
                    \PYG{c+c1}{\PYGZsh{} remove rows with any NaNs}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{dropna}\PYG{p}{(} \PYG{n}{how}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{all}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
                    \PYG{c+c1}{\PYGZsh{} remove rows with all NaNs}
\end{sphinxVerbatim}

You can filter one column based on another using these tools.

Apply a function to each value, returning a new DataFrame:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{example} \PYG{p}{(} \PYG{n}{x} \PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{n}{x} \PYG{o}{+} \PYG{l+m+mi}{1}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{apply}\PYG{p}{(} \PYG{n}{example} \PYG{p}{)}   \PYG{c+c1}{\PYGZsh{} adds 1 to everything}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{apply}\PYG{p}{(} \PYG{k}{lambda} \PYG{n}{x}\PYG{p}{:} \PYG{n}{x} \PYG{o}{+} \PYG{l+m+mi}{1} \PYG{p}{)}    \PYG{c+c1}{\PYGZsh{} same}
\PYG{c+c1}{\PYGZsh{} some functions are built\PYGZhy{}in:}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{floordiv}\PYG{p}{(} \PYG{l+m+mi}{10} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} many operators automatically repeat:}
\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{total pay}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{salary}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{+} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{bonus}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{c+c1}{\PYGZsh{} to extend a dataframe with a new column:}
\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{new col}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{old col}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{apply}\PYG{p}{(} \PYG{n}{f} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} slightly different syntax for the index:}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{index} \PYG{o}{=} \PYG{n}{df}\PYG{o}{.}\PYG{n}{index}\PYG{o}{.}\PYG{n}{map}\PYG{p}{(} \PYG{n}{f} \PYG{p}{)}
\end{sphinxVerbatim}

You can also map columns through \sphinxcode{\sphinxupquote{dict}}s, not just functions.


\bigskip\hrule\bigskip



\section{Before Week 3}
\label{\detokenize{big-cheat-sheet:before-week-3}}

\subsection{Manipulating DataFrames with pandas}
\label{\detokenize{big-cheat-sheet:manipulating-dataframes-with-pandas}}

\subsubsection{Chapter 2: Advanced indexing}
\label{\detokenize{big-cheat-sheet:chapter-2-advanced-indexing}}
Creating a Series:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{s} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{Series}\PYG{p}{(} \PYG{p}{[} \PYG{l+m+mf}{5.0}\PYG{p}{,} \PYG{l+m+mf}{3.2}\PYG{p}{,} \PYG{l+m+mf}{1.9} \PYG{p}{]} \PYG{p}{)}   \PYG{c+c1}{\PYGZsh{} just data}
\PYG{n}{s} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{Series}\PYG{p}{(} \PYG{p}{[} \PYG{l+m+mf}{5.0}\PYG{p}{,} \PYG{l+m+mf}{3.2}\PYG{p}{,} \PYG{l+m+mf}{1.9} \PYG{p}{]}\PYG{p}{,}    \PYG{c+c1}{\PYGZsh{} data with...}
  \PYG{n}{index} \PYG{o}{=} \PYG{p}{[} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Mon}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Tue}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Wed}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{]} \PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} ...an index}
\PYG{n}{s}\PYG{o}{.}\PYG{n}{index}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{:}\PYG{p}{]}                          \PYG{c+c1}{\PYGZsh{} sliceable}
\PYG{n}{s}\PYG{o}{.}\PYG{n}{index}\PYG{o}{.}\PYG{n}{name} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Day of Week}\PYG{l+s+s1}{\PYGZsq{}}         \PYG{c+c1}{\PYGZsh{} index name}
\end{sphinxVerbatim}

Column headings are also a series:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{o}{.}\PYG{n}{columns}                    \PYG{c+c1}{\PYGZsh{} is a pd.Series}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{columns}\PYG{o}{.}\PYG{n}{name}               \PYG{c+c1}{\PYGZsh{} usually a string}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{columns}\PYG{o}{.}\PYG{n}{values}             \PYG{c+c1}{\PYGZsh{} column names array}
\end{sphinxVerbatim}

Using an existing column as the index:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{o}{.}\PYG{n}{index} \PYG{o}{=} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{column name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}  \PYG{c+c1}{\PYGZsh{} once it\PYGZsq{}s the index,}
\PYG{k}{del} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{column name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}         \PYG{c+c1}{\PYGZsh{} it can be deleted}
\end{sphinxVerbatim}

Making an index from multiple columns that, when taken together, uniquely identify rows:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df} \PYG{o}{=} \PYG{n}{df}\PYG{o}{.}\PYG{n}{set\PYGZus{}index}\PYG{p}{(} \PYG{p}{[} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{last\PYGZus{}name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{first\PYGZus{}name}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{]} \PYG{p}{)}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{index}\PYG{o}{.}\PYG{n}{name}                 \PYG{c+c1}{\PYGZsh{} will be None}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{index}\PYG{o}{.}\PYG{n}{names}                \PYG{c+c1}{\PYGZsh{} list of strings}
\PYG{n}{df} \PYG{o}{=} \PYG{n}{df}\PYG{o}{.}\PYG{n}{sort\PYGZus{}index}\PYG{p}{(}\PYG{p}{)}          \PYG{c+c1}{\PYGZsh{} hierarchical sort}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Jones}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Heide}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{]}    \PYG{c+c1}{\PYGZsh{} index rows by tuples}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Jones}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Heide}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}    \PYG{c+c1}{\PYGZsh{} and you can fetch an}
       \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{birth\PYGZus{}date}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}          \PYG{c+c1}{\PYGZsh{} entry that way, too}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Jones}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}               \PYG{c+c1}{\PYGZsh{} all rows of Joneses}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Jones}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Menendez}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}    \PYG{c+c1}{\PYGZsh{} many last names}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Jones}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Wu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Heide}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}
      \PYG{c+c1}{\PYGZsh{} get both rows: Heide Jones and Heide Wu}
      \PYG{c+c1}{\PYGZsh{} (yes, the colon is necessary for rows)}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Jones}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Wu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Heide}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{birth\PYGZus{}date}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
      \PYG{c+c1}{\PYGZsh{} get Heide Jones\PYGZsq{}s and Heide Wu\PYGZsq{}s birth dates}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Jones}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Heide}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Henry}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}\PYG{p}{:}\PYG{p}{]}
      \PYG{c+c1}{\PYGZsh{} get full rows for Heide and Henry Jones}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Jones}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n+nb}{slice}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Heide}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Henry}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,}\PYG{p}{:}\PYG{p}{]}
      \PYG{c+c1}{\PYGZsh{} \PYGZsq{}Heide\PYGZsq{}:\PYGZsq{}Henry\PYGZsq{} doesn\PYGZsq{}t work inside tuples}
\end{sphinxVerbatim}


\subsubsection{Chapter 3: Rearranging and reshaping data}
\label{\detokenize{big-cheat-sheet:chapter-3-rearranging-and-reshaping-data}}
If columns A and B together uniquely identify entries in column C, you can create a new DataFrame showing this:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{new\PYGZus{}df} \PYG{o}{=} \PYG{n}{df}\PYG{o}{.}\PYG{n}{pivot}\PYG{p}{(} \PYG{n}{index}   \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{A}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                   \PYG{n}{columns} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{B}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                   \PYG{n}{values}  \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{C}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} or do this for all columns at once,}
\PYG{c+c1}{\PYGZsh{} creating a hierarchical column index:}
\PYG{n}{new\PYGZus{}df} \PYG{o}{=} \PYG{n}{df}\PYG{o}{.}\PYG{n}{pivot}\PYG{p}{(} \PYG{n}{index}   \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{A}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                   \PYG{n}{columns} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{B}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\end{sphinxVerbatim}

You can also invert pivoting, which is called “melting:”

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{old\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{melt}\PYG{p}{(} \PYG{n}{new\PYGZus{}df}\PYG{p}{,}
    \PYG{n}{id\PYGZus{}vars} \PYG{o}{=} \PYG{p}{[} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{A}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{]}\PYG{p}{,}          \PYG{c+c1}{\PYGZsh{} old index}
    \PYG{n}{value\PYGZus{}vars} \PYG{o}{=} \PYG{p}{[} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{values}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{of}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{column}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{B}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{]}\PYG{p}{,}
        \PYG{c+c1}{\PYGZsh{} optional...pandas can often infer it}
    \PYG{n}{var\PYGZus{}name} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{B}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}       \PYG{c+c1}{\PYGZsh{} these two lines just}
    \PYG{n}{value\PYGZus{}name} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{C}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}    \PYG{c+c1}{\PYGZsh{} restore column names}
\end{sphinxVerbatim}

Convert hierarchical row index to a hierarchical column index:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} assume df.index.names is [\PYGZsq{}A\PYGZsq{},\PYGZsq{}B\PYGZsq{},\PYGZsq{}C\PYGZsq{}]}
\PYG{n}{df} \PYG{o}{=} \PYG{n}{df}\PYG{o}{.}\PYG{n}{unstack}\PYG{p}{(} \PYG{n}{level} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{B}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} or A or C}
\PYG{c+c1}{\PYGZsh{} equivalently:}
\PYG{n}{df} \PYG{o}{=} \PYG{n}{df}\PYG{o}{.}\PYG{n}{unstack}\PYG{p}{(} \PYG{n}{level} \PYG{o}{=} \PYG{l+m+mi}{1} \PYG{p}{)}    \PYG{c+c1}{\PYGZsh{} or 0 or 2}
\PYG{c+c1}{\PYGZsh{} and this can be inverted:}
\PYG{n}{df} \PYG{o}{=} \PYG{n}{df}\PYG{o}{.}\PYG{n}{stack}\PYG{p}{(} \PYG{n}{level} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{B}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}    \PYG{c+c1}{\PYGZsh{} for example}
\end{sphinxVerbatim}

To change the nesting order of a hierarchical index:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df} \PYG{o}{=} \PYG{n}{df}\PYG{o}{.}\PYG{n}{swaplevel}\PYG{p}{(} \PYG{n}{levelindex1}\PYG{p}{,} \PYG{n}{levelindex2} \PYG{p}{)}
\PYG{n}{df} \PYG{o}{=} \PYG{n}{sort\PYGZus{}index}\PYG{p}{(}\PYG{p}{)}               \PYG{c+c1}{\PYGZsh{} necessary now}
\end{sphinxVerbatim}

If the pivot column(s) aren’t a unique index, use \sphinxcode{\sphinxupquote{pivot\_table}} instead, often with an aggregation function:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{new\PYGZus{}df} \PYG{o}{=} \PYG{n}{df}\PYG{o}{.}\PYG{n}{pivot\PYGZus{}table}\PYG{p}{(}        \PYG{c+c1}{\PYGZsh{} this pivot table}
    \PYG{n}{index}   \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{A}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}              \PYG{c+c1}{\PYGZsh{} is a frequency}
    \PYG{n}{columns} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{B}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}              \PYG{c+c1}{\PYGZsh{} table, because}
    \PYG{n}{values}  \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{C}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}              \PYG{c+c1}{\PYGZsh{} aggfunc is count}
    \PYG{n}{aggfunc} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{count}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}         \PYG{c+c1}{\PYGZsh{} (default: mean)}
\PYG{c+c1}{\PYGZsh{} other aggfuncs: \PYGZsq{}sum\PYGZsq{}, plus many functions in}
\PYG{c+c1}{\PYGZsh{} numpy, such as np.min, np.max, np.median, etc.}
\PYG{c+c1}{\PYGZsh{} You can also add column totals at the bottom:}
\PYG{n}{new\PYGZus{}df} \PYG{o}{=} \PYG{n}{df}\PYG{o}{.}\PYG{n}{pivot\PYGZus{}table}\PYG{p}{(}
    \PYG{n}{index}   \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{A}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
    \PYG{n}{columns} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{B}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
    \PYG{n}{values}  \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{C}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
    \PYG{n}{margins} \PYG{o}{=} \PYG{k+kc}{True} \PYG{p}{)}            \PYG{c+c1}{\PYGZsh{} add column sums}
\end{sphinxVerbatim}


\subsubsection{Chapter 4: Grouping data}
\label{\detokenize{big-cheat-sheet:chapter-4-grouping-data}}
Group all columns except column A by the unique values in column A, then apply some aggregation method to each group:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} example: total number of rows for each weekday}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{weekday}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}\PYG{o}{.}\PYG{n}{count}\PYG{p}{(}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} example: total sales in each city}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{city}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sales}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} multiple column names gives a multi\PYGZhy{}level index}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(} \PYG{p}{[} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{city}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{state}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{]} \PYG{p}{)}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} you can group by any series with the same index;}
\PYG{c+c1}{\PYGZsh{} here is an example:}
\PYG{n}{series} \PYG{o}{=} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{column A}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{apply}\PYG{p}{(} \PYG{n}{np}\PYG{o}{.}\PYG{n}{round} \PYG{p}{)}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(} \PYG{n}{series} \PYG{p}{)}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{column B}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

The \sphinxcode{\sphinxupquote{agg}} method lets us do even more:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} you can do multiple aggregations at once;}
\PYG{c+c1}{\PYGZsh{} this, too, gives a multi\PYGZhy{}level index:}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{weekday}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}\PYG{o}{.}\PYG{n}{agg}\PYG{p}{(} \PYG{p}{[} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{max}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sum}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{]} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} or you can pass a user\PYGZhy{}defined function:}
\PYG{k}{def} \PYG{n+nf}{sum\PYGZus{}of\PYGZus{}squares} \PYG{p}{(} \PYG{n}{series} \PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{p}{(} \PYG{n}{series} \PYG{o}{*} \PYG{n}{series} \PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{weekday}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{column name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
  \PYG{o}{.}\PYG{n}{agg}\PYG{p}{(} \PYG{n}{sum\PYGZus{}of\PYGZus{}squares} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} or dictionaries can let us apply different}
\PYG{c+c1}{\PYGZsh{} aggregations to different columns:}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{weekday}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Quantity Ordered}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                         \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Total Cost}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}
  \PYG{o}{.}\PYG{n}{agg}\PYG{p}{(} \PYG{p}{\PYGZob{}} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Quantity Ordered}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{median}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
          \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Total Cost}\PYG{l+s+s1}{\PYGZsq{}}       \PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sum}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{\PYGZcb{}} \PYG{p}{)}
\end{sphinxVerbatim}

\sphinxcode{\sphinxupquote{transform}} is just like \sphinxcode{\sphinxupquote{apply}}, except that it must convert each value into exactly one other, thus preserving shape.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} example: convert values to zscores}
\PYG{k+kn}{from} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{stats} \PYG{k+kn}{import} \PYG{n}{zscore}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{region}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gdp}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{transform}\PYG{p}{(} \PYG{n}{zscore} \PYG{p}{)}
  \PYG{o}{.}\PYG{n}{agg}\PYG{p}{(} \PYG{p}{[} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{min}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{max}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{]} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} example: impute missing values as medians}
\PYG{k}{def} \PYG{n+nf}{impute\PYGZus{}median}\PYG{p}{(}\PYG{n}{series}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{n}{series}\PYG{o}{.}\PYG{n}{fillna}\PYG{p}{(}\PYG{n}{series}\PYG{o}{.}\PYG{n}{median}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{grouped} \PYG{o}{=} \PYG{n}{df}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(} \PYG{p}{[} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{col B}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{col C}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{]} \PYG{p}{)}
\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{col A}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{grouped}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{col A}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
              \PYG{o}{.}\PYG{n}{transform}\PYG{p}{(} \PYG{n}{impute\PYGZus{}median} \PYG{p}{)}
\end{sphinxVerbatim}


\bigskip\hrule\bigskip



\section{Before Week 4: Review of Visualization in CS230}
\label{\detokenize{big-cheat-sheet:before-week-4-review-of-visualization-in-cs230}}

\subsection{pandas Foundations}
\label{\detokenize{big-cheat-sheet:pandas-foundations}}

\subsubsection{Chapter 2: Exploratory data analysis}
\label{\detokenize{big-cheat-sheet:chapter-2-exploratory-data-analysis}}
Plots from DataFrames:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} any of these can be followed with plt.title(),}
\PYG{c+c1}{\PYGZsh{} plt.xlabel(), etc., then plt.show() at the end:}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{col name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{col name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{kind}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{scatter}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{col name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{kind}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{box}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{col name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{kind}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{hist}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(} \PYG{n}{kind}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{box}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)} \PYG{c+c1}{\PYGZsh{} all columns side\PYGZhy{}by\PYGZhy{}side}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(} \PYG{n}{kind}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{hist}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)} \PYG{c+c1}{\PYGZsh{} all columns on same axes}
\end{sphinxVerbatim}

Histogram options: \sphinxcode{\sphinxupquote{bins}}, \sphinxcode{\sphinxupquote{range}}, \sphinxcode{\sphinxupquote{normed}}, \sphinxcode{\sphinxupquote{cumulative}}, and more.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{o}{.}\PYG{n}{describe}\PYG{p}{(}\PYG{p}{)}        \PYG{c+c1}{\PYGZsh{} summary statistics}
\PYG{c+c1}{\PYGZsh{} df.describe() makes calls to df.mean(), df.std(),}
\PYG{c+c1}{\PYGZsh{} df.median(), df.quantile(), etc...}
\end{sphinxVerbatim}


\bigskip\hrule\bigskip



\section{Before Week 5}
\label{\detokenize{big-cheat-sheet:before-week-5}}

\subsection{Intermediate Python}
\label{\detokenize{big-cheat-sheet:intermediate-python}}

\subsubsection{Chapter 5: Case Study: Hacker Statistics}
\label{\detokenize{big-cheat-sheet:chapter-5-case-study-hacker-statistics}}
Uniform random numbers from NumPy:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(} \PYG{n}{my\PYGZus{}int} \PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} choose a random sequence}
\PYG{c+c1}{\PYGZsh{} (seeds are optional, but ensure reproducibility)}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(}\PYG{p}{)}          \PYG{c+c1}{\PYGZsh{} uniform random in [0,1)}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randint}\PYG{p}{(}\PYG{n}{a}\PYG{p}{,}\PYG{n}{b}\PYG{p}{)}    \PYG{c+c1}{\PYGZsh{} uniform random in a:b}
\end{sphinxVerbatim}


\subsection{Statistical Thinking in Python, Part 1}
\label{\detokenize{big-cheat-sheet:statistical-thinking-in-python-part-1}}

\subsubsection{Chapter 1: Graphical Exploratory Data Analysis}
\label{\detokenize{big-cheat-sheet:chapter-1-graphical-exploratory-data-analysis}}
Plotting a histogram of your data:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{hist}\PYG{p}{(} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{column of interest}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{column name (units)}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{number of [fill in]}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

To change the \(y\) axis to probabilities:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{hist}\PYG{p}{(} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{column of interest}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{normed}\PYG{o}{=}\PYG{k+kc}{True} \PYG{p}{)}
\end{sphinxVerbatim}

Sometimes there is a sensible choice of where to place bin boundaries, based on the meaning of the \(x\) axis.  Example:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{hist}\PYG{p}{(} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{column of percentages}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}
          \PYG{n}{bins}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{10}\PYG{p}{,}\PYG{l+m+mi}{20}\PYG{p}{,}\PYG{l+m+mi}{30}\PYG{p}{,}\PYG{l+m+mi}{40}\PYG{p}{,}\PYG{l+m+mi}{50}\PYG{p}{,}\PYG{l+m+mi}{60}\PYG{p}{,}\PYG{l+m+mi}{70}\PYG{p}{,}\PYG{l+m+mi}{80}\PYG{p}{,}\PYG{l+m+mi}{90}\PYG{p}{,}\PYG{l+m+mi}{100}\PYG{p}{]} \PYG{p}{)}
\end{sphinxVerbatim}

Change default plot styling to Seaborn:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{set}\PYG{p}{(}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} then do plotting afterwards}
\end{sphinxVerbatim}

If your data has observations as rows and features as columns, with two features of interest in columns A and B, you can create a “bee swarm plot” as follows.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} assuming your dataframe is called df:}
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{swarmplot}\PYG{p}{(} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{A}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{B}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{df} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{explain column A}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{explain column B}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

To show a data’s distribution as an Empirical Cumulative Distribution Function plot:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} the data must be sorted from lowest to highest:}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sort}\PYG{p}{(} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{column of interest}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} the y values must count evenly from 0\PYGZpc{} to 100\PYGZpc{}:}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{o}{+}\PYG{l+m+mi}{1} \PYG{p}{)} \PYG{o}{/} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} then create and show the plot:}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(} \PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{marker}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{.}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linestyle}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{none}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{explain column of interest}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ECDF}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{margins}\PYG{p}{(} \PYG{l+m+mf}{0.02} \PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} 2\PYGZpc{} margin all around}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

Multiple ECDFs on one plot:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} prepare the data as before, but now repeatedly:}
\PYG{c+c1}{\PYGZsh{} (this could be abstracted into a function)}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sort}\PYG{p}{(} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{column 1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{p}{)}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{o}{+}\PYG{l+m+mi}{1} \PYG{p}{)} \PYG{o}{/} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(} \PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{marker}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{.}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linestyle}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{none}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sort}\PYG{p}{(} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{column 2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{p}{)}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{o}{+}\PYG{l+m+mi}{1} \PYG{p}{)} \PYG{o}{/} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} and so on, if there were other columns to plot}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(} \PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{marker}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{.}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linestyle}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{none}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} and so on if there are more data series}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(} \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{explain x1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{explain x2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}
            \PYG{n}{loc}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{lower right}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} then label axes and show plot as usual (not shown)}
\end{sphinxVerbatim}


\subsubsection{Chapter 2: Quantitative Exploratory Data Analysis}
\label{\detokenize{big-cheat-sheet:chapter-2-quantitative-exploratory-data-analysis}}
The mean is the center of mass of the data:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{column name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{p}{)}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(} \PYG{n}{series} \PYG{p}{)}
\end{sphinxVerbatim}

The median is the 50th percentile, or midpoint of the data:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{np}\PYG{o}{.}\PYG{n}{median}\PYG{p}{(} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{column name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{p}{)}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{median}\PYG{p}{(} \PYG{n}{series} \PYG{p}{)}
\end{sphinxVerbatim}

Or you can compute any percentile:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{quartiles} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{percentile}\PYG{p}{(}
    \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{column name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[} \PYG{l+m+mi}{25}\PYG{p}{,} \PYG{l+m+mi}{50}\PYG{p}{,} \PYG{l+m+mi}{75} \PYG{p}{]} \PYG{p}{)}
\PYG{n}{iqr} \PYG{o}{=} \PYG{n}{quartiles}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]} \PYG{o}{\PYGZhy{}} \PYG{n}{quartiles}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
\end{sphinxVerbatim}

Box plots show the quartiles, the IQR, and the outliers:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{boxplot}\PYG{p}{(} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{A}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{B}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{df} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} then label axes and show plot as above}
\end{sphinxVerbatim}

Variance measures the spread of the data, the average squared distance from the mean.
Standard deviation is its square root.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{np}\PYG{o}{.}\PYG{n}{var}\PYG{p}{(} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{column name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} or any series}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{std}\PYG{p}{(} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{column name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} or any series}
\end{sphinxVerbatim}

Covariance measures correlation between two data series.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} get a covariance matrix on of these ways:}
\PYG{n}{M} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{cov}\PYG{p}{(} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{column 1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{column 2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{p}{)}
\PYG{n}{M} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{cov}\PYG{p}{(} \PYG{n}{series1}\PYG{p}{,} \PYG{n}{series2} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} extract the value you care about, for example:}
\PYG{n}{covariance} \PYG{o}{=} \PYG{n}{M}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}
\end{sphinxVerbatim}

The Pearson correlation coefficient normalizes this to \([-1,1]\):

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} same as covariance, but using np.corrcoef instead:}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{corrcoef}\PYG{p}{(} \PYG{n}{series1}\PYG{p}{,} \PYG{n}{series2} \PYG{p}{)}
\end{sphinxVerbatim}


\subsubsection{Chapter 3: Thinking probabalistically–Discrete variables}
\label{\detokenize{big-cheat-sheet:chapter-3-thinking-probabalistically-discrete-variables}}
Recall these random number generation basics:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(} \PYG{n}{my\PYGZus{}int} \PYG{p}{)}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}        \PYG{c+c1}{\PYGZsh{} uniform random in [0,1)}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randint}\PYG{p}{(}\PYG{n}{a}\PYG{p}{,}\PYG{n}{b}\PYG{p}{)}    \PYG{c+c1}{\PYGZsh{} uniform random in a:b}
\end{sphinxVerbatim}

Sampling many times from some distribution:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} if the distribution is built into numpy:}
\PYG{n}{results} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(} \PYG{n}{size}\PYG{o}{=}\PYG{l+m+mi}{1000} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} if the distribution is not built into numpy:}
\PYG{n}{simulation\PYGZus{}size} \PYG{o}{=} \PYG{l+m+mi}{1000}    \PYG{c+c1}{\PYGZsh{} or any number}
\PYG{n}{results} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{empty}\PYG{p}{(} \PYG{n}{simulation\PYGZus{}size} \PYG{p}{)}
\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(} \PYG{n}{simulation\PYGZus{}size} \PYG{p}{)}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} generate a random number here, however you}
    \PYG{c+c1}{\PYGZsh{} need to; here is a random example:}
    \PYG{n}{value} \PYG{o}{=} \PYG{l+m+mi}{1} \PYG{o}{\PYGZhy{}} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)} \PYG{o}{*}\PYG{o}{*} \PYG{l+m+mi}{2}
    \PYG{c+c1}{\PYGZsh{} store it in the list of results:}
    \PYG{n}{results}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{=} \PYG{n}{value}
\end{sphinxVerbatim}

Bernoulli trials with probability \(p\):

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{success} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)} \PYG{o}{\PYGZlt{}} \PYG{n}{p}    \PYG{c+c1}{\PYGZsh{} one trial}
\PYG{n}{num\PYGZus{}successes} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{binomial}\PYG{p}{(}
    \PYG{n}{num\PYGZus{}trials}\PYG{p}{,} \PYG{n}{p} \PYG{p}{)}                 \PYG{c+c1}{\PYGZsh{} many trials}
\PYG{c+c1}{\PYGZsh{} 1000 experiments, each containing 20 trials:}
\PYG{n}{results} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{binomial}\PYG{p}{(} \PYG{l+m+mi}{20}\PYG{p}{,} \PYG{n}{p}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{l+m+mi}{1000} \PYG{p}{)}
\end{sphinxVerbatim}

Poisson distribution (size parameter optional):

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{samples} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{poisson}\PYG{p}{(}
    \PYG{n}{mean\PYGZus{}arrival\PYGZus{}rate}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{l+m+mi}{1000} \PYG{p}{)}
\end{sphinxVerbatim}


\subsubsection{Chapter 4: Thinking probabalistically–Continuous variables}
\label{\detokenize{big-cheat-sheet:chapter-4-thinking-probabalistically-continuous-variables}}
Normal (Gaussian) distribution (size parameter optional):

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{samples} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(} \PYG{n}{mean}\PYG{p}{,} \PYG{n}{std}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{l+m+mi}{1000} \PYG{p}{)}
\end{sphinxVerbatim}

Exponential distribution (time between events in a Poisson distribution,
size parameter optional again):

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{samples} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{exponential}\PYG{p}{(} \PYG{n}{mean\PYGZus{}wait}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{l+m+mi}{10} \PYG{p}{)}
\end{sphinxVerbatim}

You can take an array of numbers generated by simulation and plot it as an ECDF, as covered in the Graphical EDA chapter, earlier in this week.


\subsection{Introduction to Data Visualization with Python}
\label{\detokenize{big-cheat-sheet:introduction-to-data-visualization-with-python}}
\sphinxstyleemphasis{NOTE: Only Chapters 1 and 3 are required here.}


\subsubsection{Chapter 1: Customizing Plots}
\label{\detokenize{big-cheat-sheet:chapter-1-customizing-plots}}
Break a plot into an \(n\times m\) grid of subplots as follows:

(This is preferable to \sphinxcode{\sphinxupquote{plt.axes}}, not covered here.)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} create the grid and begin working on subplot \PYGZsh{}1:}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplot}\PYG{p}{(} \PYG{n}{n}\PYG{p}{,} \PYG{n}{m}\PYG{p}{,} \PYG{l+m+mi}{1} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(} \PYG{n}{x}\PYG{p}{,} \PYG{n}{y} \PYG{p}{)}          \PYG{c+c1}{\PYGZsh{} this will create plot \PYGZsh{}1}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{...}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}        \PYG{c+c1}{\PYGZsh{} title for plot \PYGZsh{}1}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{...}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}       \PYG{c+c1}{\PYGZsh{} ...and any other options}
\PYG{c+c1}{\PYGZsh{} keep the same grid and now work on subplot \PYGZsh{}2:}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplot}\PYG{p}{(} \PYG{n}{n}\PYG{p}{,} \PYG{n}{m}\PYG{p}{,} \PYG{l+m+mi}{2} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} any plot commands here for plot 2,}
\PYG{c+c1}{\PYGZsh{} continuing for any further subplots, ending with:}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

Tweak the limits on the axes as follows:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlim}\PYG{p}{(} \PYG{p}{[} \PYG{n+nb}{min}\PYG{p}{,} \PYG{n+nb}{max} \PYG{p}{]} \PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} set x axis limits}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylim}\PYG{p}{(} \PYG{p}{[} \PYG{n+nb}{min}\PYG{p}{,} \PYG{n+nb}{max} \PYG{p}{]} \PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} set y axis limits}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{axis}\PYG{p}{(} \PYG{p}{[} \PYG{n}{xmin}\PYG{p}{,} \PYG{n}{xmax}\PYG{p}{,} \PYG{n}{ymin}\PYG{p}{,} \PYG{n}{ymax} \PYG{p}{]} \PYG{p}{)} \PYG{c+c1}{\PYGZsh{} both}
\end{sphinxVerbatim}

To add a legend to a plot:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} when plotting series, give each a label,}
\PYG{c+c1}{\PYGZsh{} which will identify it in the legend:}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(} \PYG{n}{x1}\PYG{p}{,} \PYG{n}{y1}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{first series}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(} \PYG{n}{x2}\PYG{p}{,} \PYG{n}{y2}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{second series}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(} \PYG{n}{x3}\PYG{p}{,} \PYG{n}{y3}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{third series}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} then add the legend:}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(} \PYG{n}{loc}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{upper right}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} then show the plot as usual}
\end{sphinxVerbatim}

To annotate a figure:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} add text at some point (here, (10,15)):}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{annotate}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{text}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{xy}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{,}\PYG{l+m+mi}{15}\PYG{p}{)} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} add text at (10,15) with an arrow to (5,15):}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{annotate}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{text}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{xytext}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{,}\PYG{l+m+mi}{15}\PYG{p}{)}\PYG{p}{,} \PYG{n}{xy}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{5}\PYG{p}{,}\PYG{l+m+mi}{15}\PYG{p}{)}\PYG{p}{,}
              \PYG{n}{arrowprops}\PYG{o}{=}\PYG{p}{\PYGZob{}} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{color}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{\PYGZcb{}} \PYG{p}{)}
\end{sphinxVerbatim}

Change plot styles globally:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{style}\PYG{o}{.}\PYG{n}{available}       \PYG{c+c1}{\PYGZsh{} see list of styles}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{style}\PYG{o}{.}\PYG{n}{use}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{style}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} choose one}
\end{sphinxVerbatim}


\subsubsection{Chapter 3: Statistical plots with Seaborn}
\label{\detokenize{big-cheat-sheet:chapter-3-statistical-plots-with-seaborn}}
Plotting a linear regression line:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{lmplot}\PYG{p}{(} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{col 1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{col 2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{df} \PYG{p}{)}
\end{sphinxVerbatim}

Plotting a linear regression line:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{lmplot}\PYG{p}{(} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{col 1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{col 2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{df} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} and the corresponding residual plot:}
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{residplot}\PYG{p}{(} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{col 1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{col 2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{df}\PYG{p}{,}
               \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} color optional}
\end{sphinxVerbatim}

Plotting a polynomial regression curve of order \(n\):

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{regplot}\PYG{p}{(} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{col 1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{col 2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{df}\PYG{p}{,}
             \PYG{n}{order}\PYG{o}{=}\PYG{n}{n} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} this will include a scatter plot, but if you\PYGZsq{}ve}
\PYG{c+c1}{\PYGZsh{} already done one, you can omit redoing it:}
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{regplot}\PYG{p}{(} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{col 1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{col 2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{df}\PYG{p}{,}
             \PYG{n}{order}\PYG{o}{=}\PYG{n}{n}\PYG{p}{,} \PYG{n}{scatter}\PYG{o}{=}\PYG{k+kc}{None} \PYG{p}{)}
\end{sphinxVerbatim}

To do multiple regression plots for each value of a categorical variable in column X, distinguished by color:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{lmplot}\PYG{p}{(} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{col 1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{col 2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{df}\PYG{p}{,}
            \PYG{n}{hue}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{column X}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{palette}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Set1}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} (many other options exist for palette)}
\end{sphinxVerbatim}

Now separate plots into columns, rather than all on one plot:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{lmplot}\PYG{p}{(} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{col 1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{col 2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{df}\PYG{p}{,}
            \PYG{n}{row}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{column X}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{lmplot}\PYG{p}{(} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{col 1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{col 2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{df}\PYG{p}{,}
            \PYG{n}{col}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{column X}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\end{sphinxVerbatim}

Strip plots can visualize univariate distributions, especially useful when broken into categories:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{stripplot}\PYG{p}{(} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{data column}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{category column}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
               \PYG{n}{data}\PYG{o}{=}\PYG{n}{df} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} to add jitter to spread data out a bit in x:}
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{stripplot}\PYG{p}{(} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{data column}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{category column}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
               \PYG{n}{data}\PYG{o}{=}\PYG{n}{df}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{n}{jitter}\PYG{o}{=}\PYG{k+kc}{True} \PYG{p}{)}
\end{sphinxVerbatim}

Swarm plots, covered earlier, are very similar, but can also have colors in them to distinguish categorical variables:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{swarmplot}\PYG{p}{(} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{data column}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{category 1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
               \PYG{n}{hue}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{category 2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{df} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} and you can also change the orientation:}
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{swarmplot}\PYG{p}{(} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{category 1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{data column}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
               \PYG{n}{hue}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{category 2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{df}\PYG{p}{,}
               \PYG{n}{orient}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{h}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\end{sphinxVerbatim}

Violin plots make curves using kernel density estimation:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{violinplot}\PYG{p}{(} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{data column}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{category 1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                \PYG{n}{hue}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{category 2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{df} \PYG{p}{)}
\end{sphinxVerbatim}

Joint plots for visualizing a relationship between two variables:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{jointplot}\PYG{p}{(} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{col 1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{col 2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{df} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} and to add smoothing using KDE:}
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{jointplot}\PYG{p}{(} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{col 1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{col 2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{df}\PYG{p}{,}
               \PYG{n}{kind}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{kde}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} other kind options: reg, resid, hex}
\end{sphinxVerbatim}

Scatter plots and histograms for all numerical columns in \sphinxcode{\sphinxupquote{df}}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{pairplot}\PYG{p}{(} \PYG{n}{df} \PYG{p}{)}           \PYG{c+c1}{\PYGZsh{} no grouping/coloring}
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{pairplot}\PYG{p}{(} \PYG{n}{df}\PYG{p}{,} \PYG{n}{hue}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{A}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} color by column A}
\end{sphinxVerbatim}

Visualize a covariance matrix with a heatmap:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{M} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{cov}\PYG{p}{(} \PYG{n}{df}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{col 1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{col 2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{col3}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{p}{,} \PYG{c+c1}{\PYGZsh{} or more}
            \PYG{n}{rowvar}\PYG{o}{=}\PYG{k+kc}{False} \PYG{p}{)}   \PYG{c+c1}{\PYGZsh{} vars are in columns}
\PYG{c+c1}{\PYGZsh{} (or you can use np.corrcoef to normalize np.cov)}
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{heatmap}\PYG{p}{(} \PYG{n}{M} \PYG{p}{)}
\end{sphinxVerbatim}


\bigskip\hrule\bigskip



\section{Before Week 6}
\label{\detokenize{big-cheat-sheet:before-week-6}}

\subsection{Merging DataFrames with pandas}
\label{\detokenize{big-cheat-sheet:merging-dataframes-with-pandas}}

\subsubsection{Chapter 1: Preparing data}
\label{\detokenize{big-cheat-sheet:chapter-1-preparing-data}}
The \sphinxcode{\sphinxupquote{glob}} module is useful:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{glob} \PYG{k+kn}{import} \PYG{n}{glob}            \PYG{c+c1}{\PYGZsh{} built\PYGZhy{}in module}
\PYG{n}{filenames} \PYG{o}{=} \PYG{n}{glob}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{*.csv}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}      \PYG{c+c1}{\PYGZsh{} filename list}
\PYG{n}{data\PYGZus{}frames} \PYG{o}{=} \PYG{p}{[} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{n}{f}\PYG{p}{)}
    \PYG{k}{for} \PYG{n}{f} \PYG{o+ow}{in} \PYG{n}{filenames} \PYG{p}{]}         \PYG{c+c1}{\PYGZsh{} import all files}
\end{sphinxVerbatim}

You can reorder the rows in a DataFrame with \sphinxcode{\sphinxupquote{reindex}}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} example: if an index of month or day names were}
\PYG{c+c1}{\PYGZsh{} sorted alphabetically as strings}
\PYG{c+c1}{\PYGZsh{} rather than chronologically:}
\PYG{n}{ordered\PYGZus{}days} \PYG{o}{=} \PYG{p}{[} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Mon}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Tue}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Wed}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Thu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                 \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Fri}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sat}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sun}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{]}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{reindex}\PYG{p}{(} \PYG{n}{ordered\PYGZus{}days} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} use this to make two dataframes with a common}
\PYG{c+c1}{\PYGZsh{} index agree on their ordering:}
\PYG{n}{df1}\PYG{o}{.}\PYG{n}{reindex}\PYG{p}{(} \PYG{n}{df2}\PYG{o}{.}\PYG{n}{index} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} in case the indices don\PYGZsq{}t perfectly match,}
\PYG{c+c1}{\PYGZsh{} NaN values will be inserted, which you can drop:}
\PYG{n}{df1}\PYG{o}{.}\PYG{n}{reindex}\PYG{p}{(} \PYG{n}{df2}\PYG{o}{.}\PYG{n}{index} \PYG{p}{)}\PYG{o}{.}\PYG{n}{dropna}\PYG{p}{(}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} or for missing rows, fill with earlier ones:}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{reindex}\PYG{p}{(} \PYG{n}{some\PYGZus{}series}\PYG{p}{,} \PYG{n}{method}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ffill}\PYG{l+s+s2}{\PYGZdq{}} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} (there is also a bfill, for back\PYGZhy{}fill)}
\end{sphinxVerbatim}

You can reorder a DataFrame in preparation for reindexing:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} sort by index, ascending or descending:}
\PYG{n}{df} \PYG{o}{=} \PYG{n}{df}\PYG{o}{.}\PYG{n}{sort\PYGZus{}index}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{df} \PYG{o}{=} \PYG{n}{df}\PYG{o}{.}\PYG{n}{sort\PYGZus{}index}\PYG{p}{(} \PYG{n}{ascending}\PYG{o}{=}\PYG{k+kc}{False} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} sort by a column, ascending or descending:}
\PYG{n}{df} \PYG{o}{=} \PYG{n}{df}\PYG{o}{.}\PYG{n}{sort\PYGZus{}values}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{column name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}      \PYG{c+c1}{\PYGZsh{} required}
                     \PYG{n}{ascending}\PYG{o}{=}\PYG{k+kc}{False} \PYG{p}{)}   \PYG{c+c1}{\PYGZsh{} optional}
\end{sphinxVerbatim}


\subsubsection{Chapter 2: Concatenating data}
\label{\detokenize{big-cheat-sheet:chapter-2-concatenating-data}}
To add one DataFrame onto the end of another:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{big\PYGZus{}df} \PYG{o}{=} \PYG{n}{df1}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(} \PYG{n}{df2} \PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} top: df1, bottom: df2}
\PYG{n}{big\PYGZus{}s} \PYG{o}{=} \PYG{n}{s1}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(} \PYG{n}{s2} \PYG{p}{)}     \PYG{c+c1}{\PYGZsh{} works for Series, too}
\PYG{c+c1}{\PYGZsh{} This also stacks indices, so you usually want to:}
\PYG{n}{big\PYGZus{}df} \PYG{o}{=} \PYG{n}{big\PYGZus{}df}\PYG{o}{.}\PYG{n}{reset\PYGZus{}index}\PYG{p}{(} \PYG{n}{drop}\PYG{o}{=}\PYG{k+kc}{True} \PYG{p}{)}
\end{sphinxVerbatim}

To add many DataFrames or series on top of one another:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{big\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{concat}\PYG{p}{(} \PYG{p}{[} \PYG{n}{df1}\PYG{p}{,} \PYG{n}{df2}\PYG{p}{,} \PYG{n}{df3} \PYG{p}{]} \PYG{p}{)}
           \PYG{o}{.}\PYG{n}{reset\PYGZus{}index}\PYG{p}{(} \PYG{n}{drop}\PYG{o}{=}\PYG{k+kc}{True} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} equivalently:}
\PYG{n}{big\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{concat}\PYG{p}{(} \PYG{p}{[} \PYG{n}{df1}\PYG{p}{,} \PYG{n}{df2}\PYG{p}{,} \PYG{n}{df3} \PYG{p}{]}\PYG{p}{,}
                    \PYG{n}{ignore\PYGZus{}index}\PYG{o}{=}\PYG{k+kc}{True} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} or add a hierarchical index to disambiguate:}
\PYG{n}{big\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{concat}\PYG{p}{(} \PYG{p}{[} \PYG{n}{df1}\PYG{p}{,} \PYG{n}{df2}\PYG{p}{,} \PYG{n}{df3} \PYG{p}{]}\PYG{p}{,}
                    \PYG{n}{keys}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{key1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{key2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{key3}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} equivalently:}
\PYG{n}{big\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{concat}\PYG{p}{(} \PYG{p}{\PYGZob{}} \PYG{n}{key1} \PYG{p}{:} \PYG{n}{df1}\PYG{p}{,}
                      \PYG{n}{key2} \PYG{p}{:} \PYG{n}{df2}\PYG{p}{,}
                      \PYG{n}{key3} \PYG{p}{:} \PYG{n}{df3} \PYG{p}{\PYGZcb{}} \PYG{p}{)}
\end{sphinxVerbatim}

If \sphinxcode{\sphinxupquote{df2}} introduces new columns, and you want to form rows based on common indices, concat by columns:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{big\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{concat}\PYG{p}{(} \PYG{p}{[} \PYG{n}{df1}\PYG{p}{,} \PYG{n}{df2} \PYG{p}{]}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} equivalently:}
\PYG{n}{big\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{concat}\PYG{p}{(} \PYG{p}{[} \PYG{n}{df1}\PYG{p}{,} \PYG{n}{df2} \PYG{p}{]}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{columns}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} these accept keys=[...] also, or a dict to concat}
\end{sphinxVerbatim}

By default, \sphinxcode{\sphinxupquote{concat}} performs an “outer join,” that is, index sets are unioned.  To intersect them (“inner join”) do this:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{big\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{concat}\PYG{p}{(} \PYG{p}{[} \PYG{n}{df1}\PYG{p}{,} \PYG{n}{df2} \PYG{p}{]}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,}
                    \PYG{n}{join}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{inner}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} equivalently:}
\PYG{n}{big\PYGZus{}df} \PYG{o}{=} \PYG{n}{df1}\PYG{o}{.}\PYG{n}{join}\PYG{p}{(} \PYG{n}{df2}\PYG{p}{,} \PYG{n}{how}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{inner}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\end{sphinxVerbatim}


\subsection{Chapter 3: Merging data}
\label{\detokenize{big-cheat-sheet:chapter-3-merging-data}}
Inner joins on non\sphinxhyphen{}index columns are done with \sphinxcode{\sphinxupquote{merge}}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} default merges on all columns present}
\PYG{c+c1}{\PYGZsh{} in both dataframes:}
\PYG{n}{merged} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{merge}\PYG{p}{(} \PYG{n}{df1}\PYG{p}{,} \PYG{n}{df2} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} or you can choose your column:}
\PYG{n}{merged} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{merge}\PYG{p}{(} \PYG{n}{df1}\PYG{p}{,} \PYG{n}{df2}\PYG{p}{,} \PYG{n}{on}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{colname}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} or multiple columns:}
\PYG{n}{merged} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{merge}\PYG{p}{(} \PYG{n}{df1}\PYG{p}{,} \PYG{n}{df2}\PYG{p}{,} \PYG{n}{on}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{col1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{col2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} if the columns have different names in each df:}
\PYG{n}{merged} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{merge}\PYG{p}{(} \PYG{n}{df1}\PYG{p}{,} \PYG{n}{df2}\PYG{p}{,}
    \PYG{n}{left\PYGZus{}on}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{col1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{right\PYGZus{}on}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{col2}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} to specify meaningful suffixes to replace the}
\PYG{c+c1}{\PYGZsh{} default suffixes \PYGZus{}x and \PYGZus{}y:}
\PYG{n}{merged} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{merge}\PYG{p}{(} \PYG{n}{df1}\PYG{p}{,} \PYG{n}{df2}\PYG{p}{,}
    \PYG{n}{suffixes}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZus{}from\PYGZus{}2011}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZus{}from\PYGZus{}2012}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} you can also specify left, right, or outer joins:}
\PYG{n}{merged} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{merge}\PYG{p}{(} \PYG{n}{df1}\PYG{p}{,} \PYG{n}{df2}\PYG{p}{,} \PYG{n}{how}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{outer}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\end{sphinxVerbatim}

We often have to sort after merging (maybe by a date index), for which there is \sphinxcode{\sphinxupquote{merge\_ordered}}.  It most often goes with an outer join, so that’s its default.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} instead of this:}
\PYG{n}{merged} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{merge}\PYG{p}{(} \PYG{n}{df1}\PYG{p}{,} \PYG{n}{df2}\PYG{p}{,} \PYG{n}{how}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{outer}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
           \PYG{o}{.}\PYG{n}{sorted\PYGZus{}values}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{colname}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} do this, which is shorter and faster:}
\PYG{n}{merged} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{merge\PYGZus{}ordered}\PYG{p}{(} \PYG{n}{df1}\PYG{p}{,} \PYG{n}{df2} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} it accepts same keyword arguments as merge,}
\PYG{c+c1}{\PYGZsh{} plus fill\PYGZus{}method, like so:}
\PYG{n}{merged} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{merge\PYGZus{}ordered}\PYG{p}{(} \PYG{n}{df1}\PYG{p}{,} \PYG{n}{df2}\PYG{p}{,}
           \PYG{n}{fill\PYGZus{}method}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ffill}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\end{sphinxVerbatim}

When dates don’t fully match, you can round dates in the right DataFrame up to the nearest date in the left DataFrame:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{merged} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{merge\PYGZus{}asof}\PYG{p}{(} \PYG{n}{df1}\PYG{p}{,} \PYG{n}{df2} \PYG{p}{)}
\end{sphinxVerbatim}


\bigskip\hrule\bigskip



\section{Before Week 8}
\label{\detokenize{big-cheat-sheet:before-week-8}}

\subsection{Streamlined Data Ingestion with pandas}
\label{\detokenize{big-cheat-sheet:streamlined-data-ingestion-with-pandas}}

\subsubsection{Chapter 1: Importing Data from Flat Files}
\label{\detokenize{big-cheat-sheet:chapter-1-importing-data-from-flat-files}}
Any file whose rows are on separate lines and whose entries are separated by some delimiter can be read with the same \sphinxcode{\sphinxupquote{read\_csv}} function we’ve already seen.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{my\PYGZus{}csv\PYGZus{}file.csv}\PYG{l+s+s2}{\PYGZdq{}} \PYG{p}{)}   \PYG{c+c1}{\PYGZsh{} commas}
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{my\PYGZus{}tabbed\PYGZus{}file.tsv}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                  \PYG{n}{sep}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+s2}{\PYGZdq{}} \PYG{p}{)}            \PYG{c+c1}{\PYGZsh{} tabs}
\end{sphinxVerbatim}

If you only need some of the data, you can save space:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} choose just some columns:}
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{my\PYGZus{}csv\PYGZus{}file.csv}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{usecols}\PYG{o}{=}\PYG{p}{[}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{use}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{only}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{these}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{columns}\PYG{l+s+s2}{\PYGZdq{}} \PYG{p}{]} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} can also give a list of column indices,}
\PYG{c+c1}{\PYGZsh{} or a function that filters column names}

\PYG{c+c1}{\PYGZsh{} choose just the first 100 rows:}
\PYG{n}{df1} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{my\PYGZus{}csv\PYGZus{}file.csv}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{nrows}\PYG{o}{=}\PYG{l+m+mi}{100} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} choose just rows 1001 to 1100,}
\PYG{c+c1}{\PYGZsh{} re\PYGZhy{}using the column header from df1:}
\PYG{n}{df2} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{my\PYGZus{}csv\PYGZus{}file.csv}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                   \PYG{n}{nrows}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{,} \PYG{n}{skiprows}\PYG{o}{=}\PYG{l+m+mi}{1000}\PYG{p}{,}
                   \PYG{n}{header}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{,}       \PYG{c+c1}{\PYGZsh{} skipped it}
                   \PYG{n}{names}\PYG{o}{=}\PYG{n+nb}{list}\PYG{p}{(}\PYG{n}{df1}\PYG{p}{)} \PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} re\PYGZhy{}use}
\end{sphinxVerbatim}

If pandas is guessing a column’s data type incorrectly, you can specify it manually:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{my\PYGZus{}geographic\PYGZus{}data.csv}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                  \PYG{n}{dtype}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{zipcode}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{n+nb}{str}\PYG{p}{,}
                         \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{isemployed}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{n+nb}{bool}\PYG{p}{\PYGZcb{}} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} to correctly handle bool types:}
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{my\PYGZus{}geographic\PYGZus{}data.csv}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                  \PYG{n}{dtype}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{zipcode}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{n+nb}{str}\PYG{p}{,}
                         \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{isemployed}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{n+nb}{bool}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
                  \PYG{n}{true\PYGZus{}values}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Yes}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
                  \PYG{n}{no\PYGZus{}values}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{No}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} note: missing values get coded as True!}
\PYG{c+c1}{\PYGZsh{} (pandas understands True, False, 0, and 1)}
\end{sphinxVerbatim}

If some lines in a file are corrupt, you can ask \sphinxcode{\sphinxupquote{read\_csv}} to skip them and just warn you, importing everything else:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{maybe\PYGZus{}corrupt\PYGZus{}lines.csv}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                  \PYG{n}{error\PYGZus{}bad\PYGZus{}lines}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,}
                  \PYG{n}{warn\PYGZus{}bad\PYGZus{}lines}\PYG{o}{=}\PYG{k+kc}{True} \PYG{p}{)}
\end{sphinxVerbatim}


\subsubsection{Chapter 2: Importing Data from Excel Files}
\label{\detokenize{big-cheat-sheet:chapter-2-importing-data-from-excel-files}}
If the spreadsheet is a single table of data without formatting:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}excel}\PYG{p}{(} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{my\PYGZus{}table.xlsx}\PYG{l+s+s2}{\PYGZdq{}} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} nrows, skiprows, usecols, work as before, plus:}
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}excel}\PYG{p}{(} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{my\PYGZus{}table.xlsx}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                    \PYG{n}{usecols}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{C:J,L}\PYG{l+s+s2}{\PYGZdq{}} \PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} excel style}
\end{sphinxVerbatim}

If a file contains multiple sheets, choose one by name or index:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}excel}\PYG{p}{(} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{my\PYGZus{}workbook.xlsx}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                    \PYG{n}{sheet\PYGZus{}name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{budget}\PYG{l+s+s2}{\PYGZdq{}} \PYG{p}{)}
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}excel}\PYG{p}{(} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{my\PYGZus{}workbook.xlsx}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                    \PYG{n}{sheet\PYGZus{}name}\PYG{o}{=}\PYG{l+m+mi}{3} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} (the default is the first sheet, index 0)}
\end{sphinxVerbatim}

Or load all sheets into an ordered dictionary mapping sheet names to DataFrames:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{dfs} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}excel}\PYG{p}{(} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{my\PYGZus{}workbook.xlsx}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                     \PYG{n}{sheet\PYGZus{}name}\PYG{o}{=}\PYG{k+kc}{None} \PYG{p}{)}
\end{sphinxVerbatim}

Advanced methods of date/time parsing:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} standard, as seen before:}
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}excel}\PYG{p}{(} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{file.xlsx}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                    \PYG{n}{parse\PYGZus{}dates}\PYG{o}{=}\PYG{k+kc}{True} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} just some cols, in standard date/time format:}
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}excel}\PYG{p}{(} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{file.xlsx}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                    \PYG{n}{parse\PYGZus{}dates}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{col1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{col2}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} what if a date/time pair is split over 2 cols?}
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}excel}\PYG{p}{(} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{file.xlsx}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                    \PYG{n}{parse\PYGZus{}dates}\PYG{o}{=}\PYG{p}{[}
                        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{datetime1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                        \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{date2}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{time2}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
                    \PYG{p}{]} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} what if we want to control column names?}
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}excel}\PYG{p}{(} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{file.xlsx}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                    \PYG{n}{parse\PYGZus{}dates}\PYG{o}{=}\PYG{p}{\PYGZob{}}
                        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{name1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{datetime1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{name2}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{date2}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{time2}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
                    \PYG{p}{\PYGZcb{}} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} for nonstandard formats, do post\PYGZhy{}processing,}
\PYG{c+c1}{\PYGZsh{} using a strftime format string, like this example:}
\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{col}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{to\PYGZus{}datetime}\PYG{p}{(} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{col}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
    \PYG{n+nb}{format}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZpc{}}\PYG{l+s+s2}{m}\PYG{l+s+si}{\PYGZpc{}d}\PYG{l+s+s2}{\PYGZpc{}}\PYG{l+s+s2}{Y }\PYG{l+s+s2}{\PYGZpc{}}\PYG{l+s+s2}{H:}\PYG{l+s+s2}{\PYGZpc{}}\PYG{l+s+s2}{M:}\PYG{l+s+s2}{\PYGZpc{}}\PYG{l+s+s2}{S}\PYG{l+s+s2}{\PYGZdq{}} \PYG{p}{)}
\end{sphinxVerbatim}


\subsubsection{Chapter 3: Importing Data from Databases}
\label{\detokenize{big-cheat-sheet:chapter-3-importing-data-from-databases}}
In SQLite, databases are \sphinxcode{\sphinxupquote{.db}} files:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} prepare to connect to the database:}
\PYG{k+kn}{from} \PYG{n+nn}{sqlalchemy} \PYG{k+kn}{import} \PYG{n}{create\PYGZus{}engine}
\PYG{n}{engine} \PYG{o}{=} \PYG{n}{create\PYGZus{}engine}\PYG{p}{(} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{sqlite:///filename.db}\PYG{l+s+s2}{\PYGZdq{}} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} fetch a table:}
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}sql}\PYG{p}{(} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{table name}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{engine} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} or run any kind of SQL query:}
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}sql}\PYG{p}{(} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{PUT QUERY CODE HERE}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{engine} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} if the query code is big:}
\PYG{n}{query} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}\PYG{l+s+s2}{PUT YOUR SQL CODE}
\PYG{l+s+s2}{           HERE ON AS MANY LINES}
\PYG{l+s+s2}{           AS YOU LIKE;}\PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}sql}\PYG{p}{(} \PYG{n}{query}\PYG{p}{,} \PYG{n}{engine} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} or get a list of tables:}
\PYG{n+nb}{print}\PYG{p}{(} \PYG{n}{engine}\PYG{o}{.}\PYG{n}{table\PYGZus{}names}\PYG{p}{(}\PYG{p}{)} \PYG{p}{)}
\end{sphinxVerbatim}


\subsubsection{Chapter 4: Importing JSON Data and Working with APIs}
\label{\detokenize{big-cheat-sheet:chapter-4-importing-json-data-and-working-with-apis}}
From a file or string:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} from a file:}
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}json}\PYG{p}{(} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{filename.json}\PYG{l+s+s2}{\PYGZdq{}} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} from a string:}
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}json}\PYG{p}{(} \PYG{n}{string\PYGZus{}containing\PYGZus{}json} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} can specify dtype, as with read\PYGZus{}csv:}
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}json}\PYG{p}{(} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{filename.json}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                   \PYG{n}{dtype}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{zipcode}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{n+nb}{str}\PYG{p}{\PYGZcb{}} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} also see pandas documentation for JSON \PYGZdq{}orient\PYGZdq{}:}
\PYG{c+c1}{\PYGZsh{} records, columns, index, values, or split}
\end{sphinxVerbatim}

From the web with an API:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{requests}
\PYG{n}{response} \PYG{o}{=} \PYG{n}{requests}\PYG{o}{.}\PYG{n}{get}\PYG{p}{(}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{http://your.api.com/goes/here}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
    \PYG{n}{headers} \PYG{o}{=} \PYG{p}{\PYGZob{}}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{dictionary}\PYG{l+s+s2}{\PYGZdq{}} \PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{with things like}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{username}\PYG{l+s+s2}{\PYGZdq{}} \PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{or API key}\PYG{l+s+s2}{\PYGZdq{}}
    \PYG{p}{\PYGZcb{}}\PYG{p}{,}
    \PYG{n}{params} \PYG{o}{=} \PYG{p}{\PYGZob{}}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{dictionary}\PYG{l+s+s2}{\PYGZdq{}} \PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{with options as}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{required by}\PYG{l+s+s2}{\PYGZdq{}} \PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{the API docs}\PYG{l+s+s2}{\PYGZdq{}}
    \PYG{p}{\PYGZcb{}} \PYG{p}{)}
\PYG{n}{data} \PYG{o}{=} \PYG{n}{response}\PYG{o}{.}\PYG{n}{json}\PYG{p}{(}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} ignore metadata}
\PYG{n}{result} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(} \PYG{n}{data} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} or possibly some part of the data, like:}
\PYG{n}{result} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(} \PYG{n}{data}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{some key}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} (you must inspect it to know)}
\end{sphinxVerbatim}

If the JSON has nested objects, you can flatten:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{pandas}\PYG{n+nn}{.}\PYG{n+nn}{io}\PYG{n+nn}{.}\PYG{n+nn}{json} \PYG{k+kn}{import} \PYG{n}{json\PYGZus{}normalize}
\PYG{c+c1}{\PYGZsh{} instead of this line:}
\PYG{n}{result} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(} \PYG{n}{data}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{maybe a column}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} do this:}
\PYG{n}{result} \PYG{o}{=} \PYG{n}{json\PYGZus{}normalize}\PYG{p}{(} \PYG{n}{data}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{maybe a column}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
                         \PYG{n}{sep}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZus{}}\PYG{l+s+s2}{\PYGZdq{}} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} (if there is deep nesting, see the record\PYGZus{}path,}
\PYG{c+c1}{\PYGZsh{} meta, and meta\PYGZus{}prefix options)}
\end{sphinxVerbatim}


\bigskip\hrule\bigskip



\section{Before Week 9}
\label{\detokenize{big-cheat-sheet:before-week-9}}

\subsection{Introduction to SQL}
\label{\detokenize{big-cheat-sheet:introduction-to-sql}}

\subsubsection{Chapter 1: Selecting columns}
\label{\detokenize{big-cheat-sheet:chapter-1-selecting-columns}}
SQL (“sequel”) means Structured Query Language.  A SQL database contains tables, each of which is like a DataFrame.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZhy{}\PYGZhy{} A single\PYGZhy{}line SQL comment}
\PYG{c+cm}{/*}
\PYG{c+cm}{A multi\PYGZhy{}line}
\PYG{c+cm}{SQL comment}
\PYG{c+cm}{*/}
\end{sphinxVerbatim}

To fetch one column from a table:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{SELECT} \PYG{k}{column\PYGZus{}name} \PYG{k}{FROM} \PYG{k}{table\PYGZus{}name}\PYG{p}{;}
\end{sphinxVerbatim}

To fetch multiple columns from a table:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{SELECT} \PYG{n}{column1}\PYG{p}{,} \PYG{n}{column2} \PYG{k}{FROM} \PYG{k}{table\PYGZus{}name}\PYG{p}{;}
\PYG{k}{SELECT} \PYG{o}{*} \PYG{k}{FROM} \PYG{k}{table\PYGZus{}name}\PYG{p}{;}   \PYG{c+c1}{\PYGZhy{}\PYGZhy{} all columns}
\end{sphinxVerbatim}

To remove duplicates:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{SELECT} \PYG{k}{DISTINCT} \PYG{k}{column\PYGZus{}name}
\PYG{k}{FROM} \PYG{k}{table\PYGZus{}name}\PYG{p}{;}
\end{sphinxVerbatim}

To count rows:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{SELECT} \PYG{k}{COUNT}\PYG{p}{(}\PYG{o}{*}\PYG{p}{)}
\PYG{k}{FROM} \PYG{k}{table\PYGZus{}name}\PYG{p}{;}    \PYG{c+c1}{\PYGZhy{}\PYGZhy{} counts all the rows}
\PYG{k}{SELECT} \PYG{k}{COUNT}\PYG{p}{(}\PYG{k}{column\PYGZus{}name}\PYG{p}{)}
\PYG{k}{FROM} \PYG{k}{table\PYGZus{}name}\PYG{p}{;}    \PYG{c+c1}{\PYGZhy{}\PYGZhy{} counts the non\PYGZhy{}}
     \PYG{c+c1}{\PYGZhy{}\PYGZhy{} missing values in just that column}
\PYG{k}{SELECT} \PYG{k}{COUNT}\PYG{p}{(}\PYG{k}{DISTINCT} \PYG{k}{column\PYGZus{}name}\PYG{p}{)}
\PYG{k}{FROM} \PYG{k}{table\PYGZus{}name}\PYG{p}{;}    \PYG{c+c1}{\PYGZhy{}\PYGZhy{} \PYGZsh{} of unique entries}
\end{sphinxVerbatim}

If a result is huge, you may want just the first few lines:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{SELECT} \PYG{k}{column} \PYG{k}{FROM} \PYG{k}{table\PYGZus{}name}
\PYG{k}{LIMIT} \PYG{l+m+mi}{10}\PYG{p}{;}           \PYG{c+c1}{\PYGZhy{}\PYGZhy{} only return 10 rows}
\end{sphinxVerbatim}


\subsubsection{Chapter 2: Filtering rows}
\label{\detokenize{big-cheat-sheet:chapter-2-filtering-rows}}
(selecting a subset of the rows using the \sphinxcode{\sphinxupquote{WHERE}} keyword)

Using the comparison operators \sphinxcode{\sphinxupquote{<}}, \sphinxcode{\sphinxupquote{>}}, \sphinxcode{\sphinxupquote{=}}, \sphinxcode{\sphinxupquote{<=}}, \sphinxcode{\sphinxupquote{>=}}, and \sphinxcode{\sphinxupquote{<>}}, plus the inclusive range filter \sphinxcode{\sphinxupquote{BETWEEN}}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{SELECT} \PYG{o}{*} \PYG{k}{FROM} \PYG{k}{table\PYGZus{}name}
\PYG{k}{WHERE} \PYG{n}{quantity} \PYG{o}{\PYGZgt{}}\PYG{o}{=} \PYG{l+m+mi}{100}\PYG{p}{;}  \PYG{c+c1}{\PYGZhy{}\PYGZhy{} numeric filter}
\PYG{k}{SELECT} \PYG{o}{*} \PYG{k}{FROM} \PYG{k}{table\PYGZus{}name}
\PYG{k}{WHERE} \PYG{n}{name} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}Jeff\PYGZsq{}}\PYG{p}{;}    \PYG{c+c1}{\PYGZhy{}\PYGZhy{} string filter}
\end{sphinxVerbatim}

Using range and set filters:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{SELECT} \PYG{n}{title}\PYG{p}{,}\PYG{n}{release\PYGZus{}year} \PYG{k}{FROM} \PYG{n}{films}
\PYG{k}{WHERE} \PYG{n}{release\PYGZus{}year} \PYG{k}{BETWEEN} \PYG{l+m+mi}{1990} \PYG{k}{AND} \PYG{l+m+mi}{1999}\PYG{p}{;}
                        \PYG{c+c1}{\PYGZhy{}\PYGZhy{} range filter}
\PYG{k}{SELECT} \PYG{o}{*} \PYG{k}{FROM} \PYG{n}{employees}
\PYG{k}{WHERE} \PYG{k}{role} \PYG{k}{IN} \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}Engineer\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}Sales\PYGZsq{}}\PYG{p}{)}\PYG{p}{;}
                        \PYG{c+c1}{\PYGZhy{}\PYGZhy{} set filter}
\end{sphinxVerbatim}

Finding rows where specific columns have missing values:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{SELECT} \PYG{o}{*} \PYG{k}{FROM} \PYG{n}{employees}
\PYG{k}{WHERE} \PYG{k}{role} \PYG{k}{IS} \PYG{k}{NULL}\PYG{p}{;}
\end{sphinxVerbatim}

Combining filters with \sphinxcode{\sphinxupquote{AND}}, \sphinxcode{\sphinxupquote{OR}}, and parentheses:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{SELECT} \PYG{o}{*} \PYG{k}{FROM} \PYG{k}{table\PYGZus{}name}
\PYG{k}{WHERE} \PYG{n}{quantity} \PYG{o}{\PYGZgt{}}\PYG{o}{=} \PYG{l+m+mi}{100}
  \PYG{k}{AND} \PYG{n}{name} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}Jeff\PYGZsq{}}\PYG{p}{;}    \PYG{c+c1}{\PYGZhy{}\PYGZhy{} one combination}
\PYG{k}{SELECT} \PYG{n}{title}\PYG{p}{,}\PYG{n}{release\PYGZus{}year} \PYG{k}{FROM} \PYG{n}{films}
\PYG{k}{WHERE} \PYG{n}{release\PYGZus{}year} \PYG{o}{\PYGZgt{}}\PYG{o}{=} \PYG{l+m+mi}{1990}
  \PYG{k}{AND} \PYG{n}{release\PYGZus{}year} \PYG{o}{\PYGZlt{}}\PYG{o}{=} \PYG{l+m+mi}{1999}
  \PYG{k}{AND} \PYG{p}{(} \PYG{k}{language} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}French\PYGZsq{}}
     \PYG{k}{OR} \PYG{k}{language} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}Spanish\PYGZsq{}} \PYG{p}{)}
  \PYG{k}{AND} \PYG{n}{gross} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{2000000}\PYG{p}{;}  \PYG{c+c1}{\PYGZhy{}\PYGZhy{} many}
\end{sphinxVerbatim}

Using wildcards (\sphinxcode{\sphinxupquote{\%}} and \sphinxcode{\sphinxupquote{\_}}) to filter strings with \sphinxcode{\sphinxupquote{LIKE}}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{SELECT} \PYG{o}{*} \PYG{k}{FROM} \PYG{n}{employees}
\PYG{k}{WHERE} \PYG{n}{name} \PYG{k}{LIKE} \PYG{l+s+s1}{\PYGZsq{}Mac\PYGZpc{}\PYGZsq{}}\PYG{p}{;} \PYG{c+c1}{\PYGZhy{}\PYGZhy{} e.g., MacEwan}
\PYG{k}{SELECT} \PYG{o}{*} \PYG{k}{FROM} \PYG{n}{employees}
\PYG{k}{WHERE} \PYG{n}{id} \PYG{k}{NOT} \PYG{k}{LIKE} \PYG{l+s+s1}{\PYGZsq{}\PYGZpc{}00\PYGZsq{}}\PYG{p}{;}\PYG{c+c1}{\PYGZhy{}\PYGZhy{} e.g., 352800}
\PYG{k}{SELECT} \PYG{o}{*} \PYG{k}{FROM} \PYG{n}{employees}
\PYG{k}{WHERE} \PYG{n}{name} \PYG{k}{LIKE} \PYG{l+s+s1}{\PYGZsq{}D\PYGZus{}n\PYGZsq{}}\PYG{p}{;}  \PYG{c+c1}{\PYGZhy{}\PYGZhy{} e.g., Dan, Don}
\end{sphinxVerbatim}


\subsubsection{Chapter 3: Aggregate Functions}
\label{\detokenize{big-cheat-sheet:chapter-3-aggregate-functions}}
We’ve seen this function before; it is an aggregator:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{SELECT} \PYG{k}{COUNT}\PYG{p}{(}\PYG{o}{*}\PYG{p}{)}
\PYG{k}{FROM} \PYG{k}{table\PYGZus{}name}\PYG{p}{;}    \PYG{c+c1}{\PYGZhy{}\PYGZhy{} counts all the rows}
\end{sphinxVerbatim}

Some other aggregating functions:
\sphinxcode{\sphinxupquote{SUM}}, \sphinxcode{\sphinxupquote{AVG}}, \sphinxcode{\sphinxupquote{MIN}}, \sphinxcode{\sphinxupquote{MAX}}.
The resulting column name is the function name (e.g., \sphinxcode{\sphinxupquote{MAX}}).

To give a more descriptive name:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{SELECT} \PYG{k}{MIN}\PYG{p}{(}\PYG{n}{salary}\PYG{p}{)} \PYG{k}{AS} \PYG{n}{lowest\PYGZus{}salary}\PYG{p}{,}
       \PYG{k}{MAX}\PYG{p}{(}\PYG{n}{salary}\PYG{p}{)} \PYG{k}{AS} \PYG{n}{highest\PYGZus{}salary}
\PYG{k}{FROM} \PYG{n}{employees}\PYG{p}{;}
\end{sphinxVerbatim}

You can also do arithmetic on columns:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{SELECT} \PYG{n}{budget}\PYG{o}{/}\PYG{l+m+mi}{1000} \PYG{k}{AS} \PYG{n}{budget\PYGZus{}in\PYGZus{}thousands}
\PYG{k}{FROM} \PYG{n}{projects}\PYG{p}{;}      \PYG{c+c1}{\PYGZhy{}\PYGZhy{} convert a column}
\PYG{k}{SELECT} \PYG{n}{hours\PYGZus{}worked} \PYG{o}{*} \PYG{n}{hourly\PYGZus{}pay}
\PYG{k}{FROM} \PYG{n}{work\PYGZus{}log} \PYG{k}{WHERE} \PYG{n+nb}{date} \PYG{o}{\PYGZgt{}} \PYG{l+s+s1}{\PYGZsq{}2019\PYGZhy{}09\PYGZhy{}01\PYGZsq{}}\PYG{p}{;}
                    \PYG{c+c1}{\PYGZhy{}\PYGZhy{} create a column}
\PYG{k}{SELECT} \PYG{k}{count}\PYG{p}{(}\PYG{n}{start\PYGZus{}date}\PYG{p}{)}\PYG{o}{*}\PYG{l+m+mi}{100}\PYG{p}{.}\PYG{l+m+mi}{0}\PYG{o}{/}\PYG{k}{count}\PYG{p}{(}\PYG{o}{*}\PYG{p}{)}
\PYG{k}{FROM} \PYG{k}{table\PYGZus{}name}\PYG{p}{;}    \PYG{c+c1}{\PYGZhy{}\PYGZhy{} percent not missing}
\end{sphinxVerbatim}


\subsubsection{Chapter 4: Sorting and grouping}
\label{\detokenize{big-cheat-sheet:chapter-4-sorting-and-grouping}}
Sorting happens only after selecting:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{SELECT} \PYG{o}{*} \PYG{k}{FROM} \PYG{n}{employees}
\PYG{k}{ORDER} \PYG{k}{BY} \PYG{n}{name}\PYG{p}{;}      \PYG{c+c1}{\PYGZhy{}\PYGZhy{} ascending order}
\PYG{k}{SELECT} \PYG{o}{*} \PYG{k}{FROM} \PYG{n}{employees}
\PYG{k}{ORDER} \PYG{k}{BY} \PYG{n}{name} \PYG{k}{DESC}\PYG{p}{;} \PYG{c+c1}{\PYGZhy{}\PYGZhy{} descending order}
\PYG{k}{SELECT} \PYG{n}{name}\PYG{p}{,}\PYG{n}{salary} \PYG{k}{FROM} \PYG{n}{employees}
\PYG{k}{ORDER} \PYG{k}{BY} \PYG{k}{role}\PYG{p}{,}\PYG{n}{name}\PYG{p}{;} \PYG{c+c1}{\PYGZhy{}\PYGZhy{} multiple columns}
\end{sphinxVerbatim}

Grouping happens after selecting but before sorting.  It is used when you want to apply an aggregate function like \sphinxcode{\sphinxupquote{COUNT}} or \sphinxcode{\sphinxupquote{AVG}} not across the whole result set, but to groups within it.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZhy{}\PYGZhy{} Compute average salary by role:}
\PYG{k}{SELECT} \PYG{k}{role}\PYG{p}{,}\PYG{k}{AVG}\PYG{p}{(}\PYG{n}{salary}\PYG{p}{)} \PYG{k}{FROM} \PYG{n}{employees}
\PYG{k}{GROUP} \PYG{k}{BY} \PYG{k}{role}\PYG{p}{;}
\PYG{c+c1}{\PYGZhy{}\PYGZhy{} How many people are in each division?}
\PYG{c+c1}{\PYGZhy{}\PYGZhy{} (sorting results by division name)}
\PYG{k}{SELECT} \PYG{n}{division}\PYG{p}{,}\PYG{k}{COUNT}\PYG{p}{(}\PYG{o}{*}\PYG{p}{)} \PYG{k}{FROM} \PYG{n}{employees}
\PYG{k}{GROUP} \PYG{k}{BY} \PYG{n}{division}
\PYG{k}{ORDER} \PYG{k}{BY} \PYG{n}{division}\PYG{p}{;}
\end{sphinxVerbatim}

Every selected column except the one(s) you’re aggregating must appear in your \sphinxcode{\sphinxupquote{GROUP BY}}.

To filter by a condition (like with \sphinxcode{\sphinxupquote{WHERE}} but now applied to each group) use the \sphinxcode{\sphinxupquote{HAVING}} keyword:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZhy{}\PYGZhy{} Same as above, but omit tiny divisions:}
\PYG{k}{SELECT} \PYG{n}{division}\PYG{p}{,}\PYG{k}{COUNT}\PYG{p}{(}\PYG{o}{*}\PYG{p}{)} \PYG{k}{FROM} \PYG{n}{employees}
\PYG{k}{GROUP} \PYG{k}{BY} \PYG{n}{division}
\PYG{k}{HAVING} \PYG{k}{COUNT}\PYG{p}{(}\PYG{o}{*}\PYG{p}{)} \PYG{o}{\PYGZgt{}}\PYG{o}{=} \PYG{l+m+mi}{10}
\PYG{k}{ORDER} \PYG{k}{BY} \PYG{n}{division}\PYG{p}{;}
\end{sphinxVerbatim}


\bigskip\hrule\bigskip



\section{Additional Useful References}
\label{\detokenize{big-cheat-sheet:additional-useful-references}}

\subsection{Python Data Science Toolbox, Part 2}
\label{\detokenize{big-cheat-sheet:python-data-science-toolbox-part-2}}

\subsubsection{Chapter 1: Using iterators in PythonLand}
\label{\detokenize{big-cheat-sheet:chapter-1-using-iterators-in-pythonland}}
To convert an iterable to an iterator and use it:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{my\PYGZus{}iterable} \PYG{o}{=} \PYG{p}{[} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{one}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{two}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{three}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{]}  \PYG{c+c1}{\PYGZsh{} example}
\PYG{n}{my\PYGZus{}iterator} \PYG{o}{=} \PYG{n+nb}{iter}\PYG{p}{(} \PYG{n}{my\PYGZus{}iterable} \PYG{p}{)}
\PYG{n}{first\PYGZus{}value} \PYG{o}{=} \PYG{n+nb}{next}\PYG{p}{(} \PYG{n}{my\PYGZus{}iterator} \PYG{p}{)}        \PYG{c+c1}{\PYGZsh{} \PYGZsq{}one\PYGZsq{}}
\PYG{n}{second\PYGZus{}value} \PYG{o}{=} \PYG{n+nb}{next}\PYG{p}{(} \PYG{n}{my\PYGZus{}iterator} \PYG{p}{)}       \PYG{c+c1}{\PYGZsh{} \PYGZsq{}two\PYGZsq{}}
\PYG{c+c1}{\PYGZsh{} and so on}
\end{sphinxVerbatim}

To attach indices to the elements of an iterable:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{my\PYGZus{}iterable} \PYG{o}{=} \PYG{p}{[} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{one}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{two}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{three}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{]}  \PYG{c+c1}{\PYGZsh{} example}
\PYG{n}{with\PYGZus{}indices} \PYG{o}{=} \PYG{n+nb}{enumerate}\PYG{p}{(} \PYG{n}{my\PYGZus{}iterable} \PYG{p}{)}
\PYG{n}{my\PYGZus{}iterator} \PYG{o}{=} \PYG{n+nb}{iter}\PYG{p}{(} \PYG{n}{with\PYGZus{}indices} \PYG{p}{)}
\PYG{n}{first\PYGZus{}value} \PYG{o}{=} \PYG{n+nb}{next}\PYG{p}{(} \PYG{n}{my\PYGZus{}iterator} \PYG{p}{)}        \PYG{c+c1}{\PYGZsh{} (0,\PYGZsq{}one\PYGZsq{})}
\PYG{n}{second\PYGZus{}value} \PYG{o}{=} \PYG{n+nb}{next}\PYG{p}{(} \PYG{n}{my\PYGZus{}iterator} \PYG{p}{)}       \PYG{c+c1}{\PYGZsh{} (1,\PYGZsq{}two\PYGZsq{})}
\PYG{c+c1}{\PYGZsh{} and so on; see also \PYGZdq{}Looping Constructs\PYGZdq{} earlier}
\end{sphinxVerbatim}

To join iterables into tuples, use \sphinxcode{\sphinxupquote{zip}}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{iterable1} \PYG{o}{=} \PYG{n+nb}{range}\PYG{p}{(} \PYG{l+m+mi}{5} \PYG{p}{)}
\PYG{n}{iterable2} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{five!}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{iterable3} \PYG{o}{=} \PYG{p}{[} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{How}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{are}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{you}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{today}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{?}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{]}
\PYG{n+nb}{all} \PYG{o}{=} \PYG{n+nb}{zip}\PYG{p}{(} \PYG{n}{iterable1}\PYG{p}{,} \PYG{n}{iterable2}\PYG{p}{,} \PYG{n}{iterable3} \PYG{p}{)}
\PYG{n+nb}{next}\PYG{p}{(} \PYG{n+nb}{all} \PYG{p}{)}     \PYG{c+c1}{\PYGZsh{} (0,\PYGZsq{}f\PYGZsq{},\PYGZsq{}How\PYGZsq{})}
\PYG{n+nb}{next}\PYG{p}{(} \PYG{n+nb}{all} \PYG{p}{)}     \PYG{c+c1}{\PYGZsh{} (1,\PYGZsq{}i\PYGZsq{},\PYGZsq{}are\PYGZsq{})}
\PYG{c+c1}{\PYGZsh{} and so on, or use this syntax:}
\PYG{k}{for} \PYG{n}{x}\PYG{p}{,} \PYG{n}{y} \PYG{o+ow}{in} \PYG{n+nb}{zip}\PYG{p}{(} \PYG{n}{iterable1}\PYG{p}{,} \PYG{n}{iterable2} \PYG{p}{)}\PYG{p}{:}
    \PYG{n}{do\PYGZus{}something\PYGZus{}with}\PYG{p}{(} \PYG{n}{x}\PYG{p}{,} \PYG{n}{y} \PYG{p}{)}
\end{sphinxVerbatim}

Think of \sphinxcode{\sphinxupquote{zip}} as converting a list of rows into a list of columns, a “matrix transpose,” which is its own inverse:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{row1} \PYG{o}{=} \PYG{p}{[} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3} \PYG{p}{]}
\PYG{n}{row2} \PYG{o}{=} \PYG{p}{[} \PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{6} \PYG{p}{]}
\PYG{n}{cols} \PYG{o}{=} \PYG{n+nb}{zip}\PYG{p}{(} \PYG{n}{row1}\PYG{p}{,} \PYG{n}{row2} \PYG{p}{)}     \PYG{c+c1}{\PYGZsh{} swap rows and columns}
\PYG{n+nb}{print}\PYG{p}{(} \PYG{o}{*}\PYG{n}{cols} \PYG{p}{)}               \PYG{c+c1}{\PYGZsh{} (1,4) (2,5) (3,6)}
\PYG{n}{cols} \PYG{o}{=} \PYG{n+nb}{zip}\PYG{p}{(} \PYG{n}{row1}\PYG{p}{,} \PYG{n}{row2} \PYG{p}{)}     \PYG{c+c1}{\PYGZsh{} restart iterator}
\PYG{n}{undo1}\PYG{p}{,} \PYG{n}{undo2} \PYG{o}{=} \PYG{n+nb}{zip}\PYG{p}{(} \PYG{o}{*}\PYG{n}{cols} \PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} swap rows/cols again}
\PYG{n+nb}{print}\PYG{p}{(} \PYG{n}{undo1}\PYG{p}{,} \PYG{n}{undo2} \PYG{p}{)}        \PYG{c+c1}{\PYGZsh{} (1,2,3) (4,5,6)}
\end{sphinxVerbatim}

Pandas can read CSV files into DataFrames in chunks, creating an iterable out of a file too large for memory:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k}{for} \PYG{n}{chunk} \PYG{o+ow}{in} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(} \PYG{n}{filename}\PYG{p}{,} \PYG{n}{chunksize}\PYG{o}{=}\PYG{l+m+mi}{100} \PYG{p}{)}\PYG{p}{:}
    \PYG{n}{process\PYGZus{}one\PYGZus{}chunk}\PYG{p}{(} \PYG{n}{chunk} \PYG{p}{)}
\end{sphinxVerbatim}


\subsubsection{Chapter 2: List comprehensions and generators}
\label{\detokenize{big-cheat-sheet:chapter-2-list-comprehensions-and-generators}}
List comprehensions build a list from an output expression and a \sphinxcode{\sphinxupquote{for}} clause:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{[} \PYG{n}{n}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2} \PYG{k}{for} \PYG{n}{n} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{6}\PYG{p}{)} \PYG{p}{]}      \PYG{c+c1}{\PYGZsh{} == [9,16,25]}
\end{sphinxVerbatim}

You can nest list comprehensions:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{[} \PYG{p}{(}\PYG{n}{i}\PYG{p}{,}\PYG{n}{j}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)} \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{4}\PYG{p}{)} \PYG{p}{]}
  \PYG{c+c1}{\PYGZsh{} == [(0,0),(0,1),(0,2),(0,3),}
  \PYG{c+c1}{\PYGZsh{}     (1,0),(1,1),(1,2),(1,3),}
  \PYG{c+c1}{\PYGZsh{}     (2,0),(2,1),(2,2),(2,3)]}
\end{sphinxVerbatim}

You can put conditions on the \sphinxcode{\sphinxupquote{for}} clause:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{[} \PYG{p}{(}\PYG{n}{i}\PYG{p}{,}\PYG{n}{j}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)} \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)}
        \PYG{k}{if} \PYG{n}{i} \PYG{o}{+} \PYG{n}{j} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{2} \PYG{p}{]}  \PYG{c+c1}{\PYGZsh{} == [ (1,2), (2,1), (2,2) ]}
\end{sphinxVerbatim}

You can put conditions in the output expression:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{some\PYGZus{}data} \PYG{o}{=} \PYG{p}{[} \PYG{l+m+mf}{0.65}\PYG{p}{,} \PYG{l+m+mf}{9.12}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{3.1}\PYG{p}{,} \PYG{l+m+mf}{2.8}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{50.6} \PYG{p}{]}
\PYG{p}{[} \PYG{n}{x} \PYG{k}{if} \PYG{n}{x} \PYG{o}{\PYGZgt{}}\PYG{o}{=} \PYG{l+m+mi}{0} \PYG{k}{else} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{NEG}\PYG{l+s+s1}{\PYGZsq{}} \PYG{k}{for} \PYG{n}{x} \PYG{o+ow}{in} \PYG{n}{some\PYGZus{}data} \PYG{p}{]}
  \PYG{c+c1}{\PYGZsh{} == [ 0.65, 9.12, \PYGZsq{}NEG\PYGZsq{}, 2.8, \PYGZsq{}NEG\PYGZsq{} ]}
\end{sphinxVerbatim}

A dict comprehension creates a dictionary from an output expression in \sphinxcode{\sphinxupquote{key:value}} form, plus a \sphinxcode{\sphinxupquote{for}} clause:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}} \PYG{n}{a}\PYG{p}{:} \PYG{n}{a}\PYG{o}{.}\PYG{n}{capitalize}\PYG{p}{(}\PYG{p}{)} \PYG{k}{for} \PYG{n}{a} \PYG{o+ow}{in} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{one}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{two}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{three}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{p}{\PYGZcb{}}
  \PYG{c+c1}{\PYGZsh{} == \PYGZob{} \PYGZsq{}one\PYGZsq{}:\PYGZsq{}One\PYGZsq{}, \PYGZsq{}two\PYGZsq{}:\PYGZsq{}Two\PYGZsq{}, \PYGZsq{}three\PYGZsq{}:\PYGZsq{}Three\PYGZsq{} \PYGZcb{}}
\end{sphinxVerbatim}

Just like list comprehensions, but with parentheses:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{g} \PYG{o}{=} \PYG{p}{(} \PYG{n}{n}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2} \PYG{k}{for} \PYG{n}{n} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{6}\PYG{p}{)} \PYG{p}{)}
\PYG{n+nb}{next}\PYG{p}{(} \PYG{n}{g} \PYG{p}{)}                         \PYG{c+c1}{\PYGZsh{} == 9}
\PYG{n+nb}{next}\PYG{p}{(} \PYG{n}{g} \PYG{p}{)}                         \PYG{c+c1}{\PYGZsh{} == 16}
\PYG{n+nb}{next}\PYG{p}{(} \PYG{n}{g} \PYG{p}{)}                         \PYG{c+c1}{\PYGZsh{} == 25}
\end{sphinxVerbatim}

You can build generators with functions and \sphinxcode{\sphinxupquote{yield}}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{just\PYGZus{}like\PYGZus{}range} \PYG{p}{(} \PYG{n}{a}\PYG{p}{,} \PYG{n}{b} \PYG{p}{)}\PYG{p}{:}
    \PYG{n}{counter} \PYG{o}{=} \PYG{n}{a}
    \PYG{k}{while} \PYG{n}{counter} \PYG{o}{\PYGZlt{}} \PYG{n}{b}\PYG{p}{:}
        \PYG{k}{yield} \PYG{n}{counter}
        \PYG{n}{counter} \PYG{o}{+}\PYG{o}{=} \PYG{l+m+mi}{1}
\PYG{n+nb}{list}\PYG{p}{(} \PYG{n}{just\PYGZus{}like\PYGZus{}range}\PYG{p}{(} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{9} \PYG{p}{)} \PYG{p}{)}   \PYG{c+c1}{\PYGZsh{} == [5,6,7,8]}
\end{sphinxVerbatim}


\subsection{Introduction to Data Visualization with Python}
\label{\detokenize{big-cheat-sheet:id1}}

\subsubsection{Chapter 2: Plotting 2D arrays}
\label{\detokenize{big-cheat-sheet:chapter-2-plotting-2d-arrays}}
To plot a bivariate function using colors:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} choose the sampling points in both axes:}
\PYG{n}{u} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(} \PYG{n}{xmin}\PYG{p}{,} \PYG{n}{xmax}\PYG{p}{,} \PYG{n}{num\PYGZus{}xpoints} \PYG{p}{)}
\PYG{n}{v} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(} \PYG{n}{ymin}\PYG{p}{,} \PYG{n}{ymax}\PYG{p}{,} \PYG{n}{num\PYGZus{}ypoints} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} create pairs from these axes:}
\PYG{n}{x}\PYG{p}{,} \PYG{n}{y} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{meshgrid}\PYG{p}{(} \PYG{n}{u}\PYG{p}{,} \PYG{n}{v} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} broadcast a function across those points:}
\PYG{n}{z} \PYG{o}{=} \PYG{n}{x}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2} \PYG{o}{\PYGZhy{}} \PYG{n}{y}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}
\PYG{c+c1}{\PYGZsh{} plot it in color:}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{pcolor}\PYG{p}{(} \PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{z} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{p}{)}       \PYG{c+c1}{\PYGZsh{} optional but helpful}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{axis}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{tight}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} remove whitespace}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} optionally, the pcolor call can take a color}
\PYG{c+c1}{\PYGZsh{} map parameter, one of a host of palettes, e.g.:}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{pcolor}\PYG{p}{(} \PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{z}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{autumn}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\end{sphinxVerbatim}

To make a contour plot instead of a color map plot:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} replace the pcolor line with this:}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{contour}\PYG{p}{(} \PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{z} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{contour}\PYG{p}{(} \PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{z}\PYG{p}{,} \PYG{l+m+mi}{50} \PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} choose num. contours}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{contourf}\PYG{p}{(} \PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{z} \PYG{p}{)}     \PYG{c+c1}{\PYGZsh{} fill the contours}
\end{sphinxVerbatim}

To make a bivariate histogram:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} for rectangular bins:}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{hist2d}\PYG{p}{(} \PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{bins}\PYG{o}{=}\PYG{p}{(}\PYG{n}{xbins}\PYG{p}{,}\PYG{n}{ybins}\PYG{p}{)} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} with optional x and y ranges:}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{hist2d}\PYG{p}{(} \PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{bins}\PYG{o}{=}\PYG{p}{(}\PYG{n}{xbins}\PYG{p}{,}\PYG{n}{ybins}\PYG{p}{)}\PYG{p}{,}
            \PYG{n+nb}{range}\PYG{o}{=}\PYG{p}{(}\PYG{p}{(}\PYG{n}{xmin}\PYG{p}{,}\PYG{n}{xmax}\PYG{p}{)}\PYG{p}{,}\PYG{p}{(}\PYG{n}{ymin}\PYG{p}{,}\PYG{n}{ymax}\PYG{p}{)}\PYG{p}{)} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} for hexagonal bins:}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{hexbin}\PYG{p}{(} \PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,}
            \PYG{n}{gridsize}\PYG{o}{=}\PYG{p}{(}\PYG{n}{num\PYGZus{}x\PYGZus{}hexes}\PYG{p}{,}\PYG{n}{num\PYGZus{}y\PYGZus{}hexes}\PYG{p}{)} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} with optional x and y ranges:}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{hexbin}\PYG{p}{(} \PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,}
            \PYG{n}{gridsize}\PYG{o}{=}\PYG{p}{(}\PYG{n}{num\PYGZus{}x\PYGZus{}hexes}\PYG{p}{,}\PYG{n}{num\PYGZus{}y\PYGZus{}hexes}\PYG{p}{)}\PYG{p}{,}
            \PYG{n}{extent}\PYG{o}{=}\PYG{p}{(}\PYG{n}{xmin}\PYG{p}{,}\PYG{n}{xmax}\PYG{p}{,}\PYG{n}{ymin}\PYG{p}{,}\PYG{n}{ymax}\PYG{p}{)} \PYG{p}{)}
\end{sphinxVerbatim}

To display an image from a file:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{image} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{imread}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{filename.png}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(} \PYG{n}{image} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{axis}\PYG{p}{(} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{off}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}           \PYG{c+c1}{\PYGZsh{} axes don\PYGZsq{}t apply here}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} to collapse a color image to grayscale:}
\PYG{n}{gray\PYGZus{}img} \PYG{o}{=} \PYG{n}{image}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{2} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(} \PYG{n}{gray\PYGZus{}img}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gray}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} to alter the aspect ratio:}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(} \PYG{n}{gray\PYGZus{}img}\PYG{p}{,} \PYG{n}{aspect}\PYG{o}{=}\PYG{n}{height}\PYG{o}{/}\PYG{n}{width} \PYG{p}{)}
\end{sphinxVerbatim}


\chapter{Anaconda Installation}
\label{\detokenize{anaconda-installation:anaconda-installation}}\label{\detokenize{anaconda-installation::doc}}
Anaconda is a tool that installs Python together with the \sphinxcode{\sphinxupquote{conda}} package
manager and several related apps, tools, and packages.  It’s one of the
easiest ways to get Python installed on your system and ready to use for
data work.

These instructions are written primarily for Windows, with Mac
instructions in parentheses.


\section{Visit the Anaconda website}
\label{\detokenize{anaconda-installation:visit-the-anaconda-website}}
It is at this URL:
\sphinxhref{http://www.anaconda.com/distribution}{www.anaconda.com/distribution}

It looks like this:




\section{Choose your OS}
\label{\detokenize{anaconda-installation:choose-your-os}}
Scroll down on that same website and click the Windows link to indicate
that you want to download the installer for Windows.



(Mac users obviously click the macOS link instead.)


\section{Download the Installer}
\label{\detokenize{anaconda-installation:download-the-installer}}
Click the download button for the Python 3.7 distribution of Anaconda,
as shown on the left below.




\section{Run the Installer}
\label{\detokenize{anaconda-installation:run-the-installer}}
Run the installer once it’s downloaded, probably by clicking the
downloaded file in your browser’s list of downloaded files, usually at
the bottom left of the window.



(For Mac users, this will be a .pkg file instead of an .exe.)

Accept all the default choices during installation. This may take up to
10 minutes.



(For Mac users, the installer will look slightly different than the one
above.)

After this, you may wish to {\hyperref[\detokenize{vs-code-installation::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{install VS Code}}}} as well.


\chapter{VS Code for Python Installation}
\label{\detokenize{vs-code-installation:vs-code-for-python-installation}}\label{\detokenize{vs-code-installation::doc}}
This assumes you have {\hyperref[\detokenize{anaconda-installation::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{installed Anaconda}}}} already.


\section{Open the Anaconda Navigator}
\label{\detokenize{vs-code-installation:open-the-anaconda-navigator}}
Start Menu > Anaconda3 > Anaconda Navigator(On Mac: Finder > Applications > Anaconda Navigator.app.)




\section{Find and Install the VS Code application}
\label{\detokenize{vs-code-installation:find-and-install-the-vs-code-application}}
Scroll down in the list of applications until you find VS Code (Visual
Studio Code, by Microsoft).



Click the Install button beneath it.

Once you have installed VS Code, its application icon will change to
contain a “Launch” button.



Click that button now to launch VS Code.


\section{Installing and Configuring Code Runner}
\label{\detokenize{vs-code-installation:installing-and-configuring-code-runner}}
VS Code is all ready to let you edit Python code, but there’s an
Extension to VS Code, called Code Runner, that makes it more convenient
to run Python code when testing your work. Let’s install it now.

Click the Extensions button on the left of the VS Code window.  It is the bottom button shown below, which looks like four squares:



Then search for the Code Runner Extension as shown below and click its
install button.  (The install button is green and is just below and to the right of hte large orange “.run” icon.)



The Code Runner Extension in the Extensions list will then have a
settings gear icon, as shown below.



Click that gear icon to bring up the settings menu for Code Runner, as shown below.



Choose “Configure Extension Settings,” the bottom item on that menu.

It will bring up a settings window as shown below.



Scroll down until you find the settings for saving files before running,
and check both boxes, as shown here.



Your Code Runner Extension is correctly configured. Try it out as shown
on the next page.


\section{Testing your Installation}
\label{\detokenize{vs-code-installation:testing-your-installation}}
Let’s verify now that you can successfully run Python code.

Create a new file:





Save the file and give it the name \sphinxcode{\sphinxupquote{test.py}} to indicate that it is a Python file.



Enter the following small amount of Python code in the new, empty file.

\sphinxstyleemphasis{Be sure to press Enter after the code to start a new line!}



Save the file again.

Run the file by clicking the small Run icon (which looks like a “Play”
triangle) on the top right of the window.  (If you don’t see this button,
check your work above—did you save the file with a \sphinxcode{\sphinxupquote{.py}} extension?)



You should see the following output at the bottom of the window,
indicating that your code was run, and produced the output \sphinxcode{\sphinxupquote{4}} (which is
the result of 2+2, of course).



(The first line means that the “python” command was run on the \sphinxcode{\sphinxupquote{test.py}}
file you saved. The final line means the process ended with error code
0, which means no errors, and took 0.102 seconds in total.)

You have a successful Python installation that you can run from Visual
Studio!


\chapter{GB213 Review in Python}
\label{\detokenize{GB213-review-in-Python:gb213-review-in-python}}\label{\detokenize{GB213-review-in-Python::doc}}

\section{We’re not covering everything}
\label{\detokenize{GB213-review-in-Python:we-re-not-covering-everything}}
We’re omitting basic probability issues like experiments, sample spaces, discrete probabilities, combinations, and permutations.  At the end we’ll provide links to example topics.  But everything else we’ll cover at least briefly.

We begin by importing the necessary modules.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{import} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{stats} \PYG{k}{as} \PYG{n+nn}{stats}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}
\PYG{o}{\PYGZpc{}}\PYG{n}{matplotlib} \PYG{n}{inline}
\end{sphinxVerbatim}


\bigskip\hrule\bigskip



\section{Discrete Random Variables}
\label{\detokenize{GB213-review-in-Python:discrete-random-variables}}
(For continuous random variables, {\hyperref[\detokenize{GB213-review-in-Python:continuous-random-variables}]{\emph{see further below}}}.

Discrete random variables taken on a finite number of different values.  For example, a Bernoulli trial is either 0 or 1 (usually meaning failure and success, respectively).  You can create random variables using \sphinxcode{\sphinxupquote{scipy.stats}} as follows.


\subsection{Creating them}
\label{\detokenize{GB213-review-in-Python:creating-them}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{b1} \PYG{o}{=} \PYG{n}{stats}\PYG{o}{.}\PYG{n}{bernoulli}\PYG{p}{(} \PYG{l+m+mf}{0.25} \PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} probability of success}
\PYG{n}{b2} \PYG{o}{=} \PYG{n}{stats}\PYG{o}{.}\PYG{n}{binom}\PYG{p}{(} \PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mf}{0.5} \PYG{p}{)}   \PYG{c+c1}{\PYGZsh{} number of trials, prob. of success on each}
\end{sphinxVerbatim}


\subsection{Computing probabilities from a Discrete Random Variable}
\label{\detokenize{GB213-review-in-Python:computing-probabilities-from-a-discrete-random-variable}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{b1}\PYG{o}{.}\PYG{n}{pmf}\PYG{p}{(} \PYG{l+m+mi}{0} \PYG{p}{)}\PYG{p}{,} \PYG{n}{b1}\PYG{o}{.}\PYG{n}{pmf}\PYG{p}{(} \PYG{l+m+mi}{1} \PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} stands for \PYGZdq{}probability mass function\PYGZdq{}}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(0.75, 0.25)
\end{sphinxVerbatim}

The same code works for any random variable, not just \sphinxcode{\sphinxupquote{b1}}.


\subsection{Generating values from a Discrete Random Variable}
\label{\detokenize{GB213-review-in-Python:generating-values-from-a-discrete-random-variable}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{b1}\PYG{o}{.}\PYG{n}{rvs}\PYG{p}{(} \PYG{l+m+mi}{10} \PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} asks for 10 random values (rvs)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
array([0, 1, 0, 0, 0, 0, 0, 0, 1, 1])
\end{sphinxVerbatim}

The same code works for any random variable, not just \sphinxcode{\sphinxupquote{b1}}.


\subsection{Computing statistics about a Discrete Random Variable}
\label{\detokenize{GB213-review-in-Python:computing-statistics-about-a-discrete-random-variable}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{b1}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{b1}\PYG{o}{.}\PYG{n}{var}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{b1}\PYG{o}{.}\PYG{n}{std}\PYG{p}{(}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} mean, variance, standard deviation}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(0.25, 0.1875, 0.4330127018922193)
\end{sphinxVerbatim}

The same code works for any random variable, not just \sphinxcode{\sphinxupquote{b1}}.


\subsection{Plotting the Distribution of a Discrete Random Variable}
\label{\detokenize{GB213-review-in-Python:plotting-the-distribution-of-a-discrete-random-variable}}
Here’s a function you can use to plot (almost) any discrete probability distribution.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{plot\PYGZus{}discrete\PYGZus{}distribution} \PYG{p}{(} \PYG{n}{rv} \PYG{p}{)}\PYG{p}{:}
    \PYG{n}{xmin}\PYG{p}{,} \PYG{n}{xmax} \PYG{o}{=} \PYG{n}{rv}\PYG{o}{.}\PYG{n}{ppf}\PYG{p}{(} \PYG{l+m+mf}{0.0001} \PYG{p}{)}\PYG{p}{,} \PYG{n}{rv}\PYG{o}{.}\PYG{n}{ppf}\PYG{p}{(} \PYG{l+m+mf}{0.9999} \PYG{p}{)}
    \PYG{n}{x} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(} \PYG{n}{xmin}\PYG{p}{,} \PYG{n}{xmax}\PYG{o}{+}\PYG{l+m+mi}{1} \PYG{p}{)}
    \PYG{n}{y} \PYG{o}{=} \PYG{n}{rv}\PYG{o}{.}\PYG{n}{pmf}\PYG{p}{(} \PYG{n}{x} \PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(} \PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{o}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{vlines}\PYG{p}{(} \PYG{n}{x}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{y} \PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylim}\PYG{p}{(} \PYG{n}{bottom}\PYG{o}{=}\PYG{l+m+mi}{0} \PYG{p}{)}
\end{sphinxVerbatim}

Example use:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plot\PYGZus{}discrete\PYGZus{}distribution}\PYG{p}{(} \PYG{n}{b2} \PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{GB213-review-in-Python_15_0}.png}


\bigskip\hrule\bigskip



\section{Continuous Random Variables}
\label{\detokenize{GB213-review-in-Python:continuous-random-variables}}
(For discrete random variables, {\hyperref[\detokenize{GB213-review-in-Python:discrete-random-variables}]{\emph{see further above}}}.

Continuous random variables take on an infinite number of different values, sometimes in a certain range (like the uniform distribution on \([0,1]\), for example) and sometimes over the whole real number line (like the normal distribution, for example).


\subsection{Creating them}
\label{\detokenize{GB213-review-in-Python:id1}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} for uniform on the interval [a,b]: use loc=a, scale=b\PYGZhy{}a}
\PYG{n}{u} \PYG{o}{=} \PYG{n}{stats}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(} \PYG{n}{loc}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mi}{2} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} for normal use loc=mean, scale=standard deviation}
\PYG{n}{n} \PYG{o}{=} \PYG{n}{stats}\PYG{o}{.}\PYG{n}{norm}\PYG{p}{(} \PYG{n}{loc}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mi}{5} \PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} for t, same as normal, plus df=degrees of freedom}
\PYG{n}{t} \PYG{o}{=} \PYG{n}{stats}\PYG{o}{.}\PYG{n}{t}\PYG{p}{(} \PYG{n}{df}\PYG{o}{=}\PYG{l+m+mi}{15}\PYG{p}{,} \PYG{n}{loc}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mi}{5} \PYG{p}{)}
\end{sphinxVerbatim}


\subsection{Computing probabilities from a Continuous Random Variable}
\label{\detokenize{GB213-review-in-Python:computing-probabilities-from-a-continuous-random-variable}}
For a continuous random variable, you cannot compute the probability that it will equal a precise number, because such a probability is always zero.  But you can compute the probability that the value falls within a certain interval on the number line.

To do so for an interval \([a,b]\), compute the total probability accumulated up to \(a\) and subtract it from that up to \(b\), as follows.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{a}\PYG{p}{,} \PYG{n}{b} \PYG{o}{=} \PYG{l+m+mi}{95}\PYG{p}{,} \PYG{l+m+mi}{100}  \PYG{c+c1}{\PYGZsh{} or any values}
\PYG{n}{n}\PYG{o}{.}\PYG{n}{cdf}\PYG{p}{(} \PYG{n}{b} \PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{n}{n}\PYG{o}{.}\PYG{n}{cdf}\PYG{p}{(} \PYG{n}{a} \PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} probability of being in that interval}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
0.3413447460685429
\end{sphinxVerbatim}

The same code works for any continuous random variable, not just \sphinxcode{\sphinxupquote{n}}.


\subsection{Generating values from a Continuous Random Variable}
\label{\detokenize{GB213-review-in-Python:generating-values-from-a-continuous-random-variable}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{n}\PYG{o}{.}\PYG{n}{rvs}\PYG{p}{(} \PYG{l+m+mi}{10} \PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} same as for discrete random variables}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
array([ 94.22395563,  95.84480775,  99.42675177, 104.249009  ,
       107.50141932,  97.44272331,  96.45849703,  98.61017231,
        99.10615245,  97.85552034])
\end{sphinxVerbatim}


\subsection{Plotting the Distribution of a Continuous Random Variable}
\label{\detokenize{GB213-review-in-Python:plotting-the-distribution-of-a-continuous-random-variable}}
Here’s a function you can use to plot the center 99.98\% of any continuous probability distribution.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{plot\PYGZus{}continuous\PYGZus{}distribution} \PYG{p}{(} \PYG{n}{rv} \PYG{p}{)}\PYG{p}{:}
    \PYG{n}{xmin}\PYG{p}{,} \PYG{n}{xmax} \PYG{o}{=} \PYG{n}{rv}\PYG{o}{.}\PYG{n}{ppf}\PYG{p}{(} \PYG{l+m+mf}{0.0001} \PYG{p}{)}\PYG{p}{,} \PYG{n}{rv}\PYG{o}{.}\PYG{n}{ppf}\PYG{p}{(} \PYG{l+m+mf}{0.9999} \PYG{p}{)}
    \PYG{n}{x} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(} \PYG{n}{xmin}\PYG{p}{,} \PYG{n}{xmax}\PYG{p}{,} \PYG{l+m+mi}{100} \PYG{p}{)}
    \PYG{n}{y} \PYG{o}{=} \PYG{n}{rv}\PYG{o}{.}\PYG{n}{pdf}\PYG{p}{(} \PYG{n}{x} \PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(} \PYG{n}{x}\PYG{p}{,} \PYG{n}{y} \PYG{p}{)}
\end{sphinxVerbatim}

Example use:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plot\PYGZus{}continuous\PYGZus{}distribution}\PYG{p}{(} \PYG{n}{n} \PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{GB213-review-in-Python_26_0}.png}


\bigskip\hrule\bigskip



\section{Confidence Intervals}
\label{\detokenize{GB213-review-in-Python:confidence-intervals}}
Recall from GB213 that certain assumptions about normality must hold in order for you to do statistical inference.  We do not cover those here; refer to your GB213 text or notes.

Here we cover a confidence interval for the sample mean using confidence level \(\alpha\), which must be between 0 and 1 (typically 0.95).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{α} \PYG{o}{=} \PYG{l+m+mf}{0.95}
\PYG{c+c1}{\PYGZsh{} normally you\PYGZsq{}de have data; for this example, I make some up:}
\PYG{n}{data} \PYG{o}{=} \PYG{p}{[} \PYG{l+m+mi}{435}\PYG{p}{,}\PYG{l+m+mi}{542}\PYG{p}{,}\PYG{l+m+mi}{435}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{,}\PYG{l+m+mi}{54}\PYG{p}{,}\PYG{l+m+mi}{43}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{,}\PYG{l+m+mi}{43}\PYG{p}{,}\PYG{l+m+mi}{543}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{,}\PYG{l+m+mi}{432}\PYG{p}{,}\PYG{l+m+mi}{43}\PYG{p}{,}\PYG{l+m+mi}{36}\PYG{p}{,}\PYG{l+m+mi}{7}\PYG{p}{,}\PYG{l+m+mi}{876}\PYG{p}{,}\PYG{l+m+mi}{65}\PYG{p}{,}\PYG{l+m+mi}{5} \PYG{p}{]}
\PYG{n}{est\PYGZus{}mean} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(} \PYG{n}{data} \PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} estimate for the population mean}
\PYG{n}{sem} \PYG{o}{=} \PYG{n}{stats}\PYG{o}{.}\PYG{n}{sem}\PYG{p}{(} \PYG{n}{data} \PYG{p}{)}     \PYG{c+c1}{\PYGZsh{} standard error for the sample mean}
\PYG{c+c1}{\PYGZsh{} margin of error:}
\PYG{n}{moe} \PYG{o}{=} \PYG{n}{sem} \PYG{o}{*} \PYG{n}{stats}\PYG{o}{.}\PYG{n}{t}\PYG{o}{.}\PYG{n}{ppf}\PYG{p}{(} \PYG{p}{(} \PYG{l+m+mi}{1} \PYG{o}{+} \PYG{n}{α} \PYG{p}{)} \PYG{o}{/} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n+nb}{len}\PYG{p}{(} \PYG{n}{data} \PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{1} \PYG{p}{)}
\PYG{p}{(} \PYG{n}{est\PYGZus{}mean} \PYG{o}{\PYGZhy{}} \PYG{n}{moe}\PYG{p}{,} \PYG{n}{est\PYGZus{}mean} \PYG{o}{+} \PYG{n}{moe} \PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} confidence interval}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(70.29847811072423, 350.0544630657464)
\end{sphinxVerbatim}


\bigskip\hrule\bigskip



\section{Hypothesis Testing}
\label{\detokenize{GB213-review-in-Python:hypothesis-testing}}
Again, in GB213 you learned what assumptions must hold in order to do a hypothesis test, which I do not review here.

Let \(H_0\) be the null hypothesis, the currently held belief.  Let \(H_a\) be the alternative, which would result in some change in our beliefs or actions.

We assume some chosen value \(0\leq\alpha\leq1\), which is the probability of a Type I error (false positive, finding we should reject \(H_0\) when it’s actually true).


\subsection{Two\sphinxhyphen{}sided test for \protect\(H_0:\mu=\bar{x}\protect\)}
\label{\detokenize{GB213-review-in-Python:two-sided-test-for-h-0-mu-bar-x}}
Say we have a population whose mean \(\mu\) is known to be 10.  We take a sample \(x_1,\ldots,x_n\) and compute its mean, \(\bar{x}\).  We then ask whether this sample is significantly different from the population at large, that is, is \(\mu=\bar{x}\)?  We can do a two\sphinxhyphen{}sided test of \(H_0:\mu=\bar{x}\) as follows.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{α} \PYG{o}{=} \PYG{l+m+mf}{0.05}
\PYG{n}{μ} \PYG{o}{=} \PYG{l+m+mi}{10}
\PYG{n}{sample} \PYG{o}{=} \PYG{p}{[} \PYG{l+m+mi}{9}\PYG{p}{,} \PYG{l+m+mi}{12}\PYG{p}{,} \PYG{l+m+mi}{14}\PYG{p}{,} \PYG{l+m+mi}{8}\PYG{p}{,} \PYG{l+m+mi}{13} \PYG{p}{]}
\PYG{n}{t\PYGZus{}statistic}\PYG{p}{,} \PYG{n}{p\PYGZus{}value} \PYG{o}{=} \PYG{n}{stats}\PYG{o}{.}\PYG{n}{ttest\PYGZus{}1samp}\PYG{p}{(} \PYG{n}{sample}\PYG{p}{,} \PYG{n}{μ} \PYG{p}{)}
\PYG{n}{reject\PYGZus{}H0} \PYG{o}{=} \PYG{n}{p\PYGZus{}value} \PYG{o}{\PYGZlt{}} \PYG{n}{α}
\PYG{n}{α}\PYG{p}{,} \PYG{n}{p\PYGZus{}value}\PYG{p}{,} \PYG{n}{reject\PYGZus{}H0}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(0.05, 0.35845634462296455, False)
\end{sphinxVerbatim}

The output above says that the data does NOT give us enough information to reject the null hypothesis.  So we should continue to assume that the sample is like the population, and \(\mu=\bar{x}\).


\subsection{Two\sphinxhyphen{}sided test for \protect\(H_0: \bar{x}_1=\bar{x}_2\protect\)}
\label{\detokenize{GB213-review-in-Python:two-sided-test-for-h-0-bar-x-1-bar-x-2}}
What if we had wanted to do a test for whether two independent samples had the same mean?  We can ask that question as follows.  (Here we assume they have equal variances, but you can turn that assumption off with a third parameter to \sphinxcode{\sphinxupquote{ttest\_ind}}.)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{α} \PYG{o}{=} \PYG{l+m+mf}{0.05}
\PYG{n}{sample1} \PYG{o}{=} \PYG{p}{[} \PYG{l+m+mi}{6}\PYG{p}{,} \PYG{l+m+mi}{9}\PYG{p}{,} \PYG{l+m+mi}{7}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{9} \PYG{p}{]}
\PYG{n}{sample2} \PYG{o}{=} \PYG{p}{[} \PYG{l+m+mi}{12}\PYG{p}{,} \PYG{l+m+mi}{14}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{17}\PYG{p}{,} \PYG{l+m+mi}{9} \PYG{p}{]}
\PYG{n}{t\PYGZus{}statistics}\PYG{p}{,} \PYG{n}{p\PYGZus{}value} \PYG{o}{=} \PYG{n}{stats}\PYG{o}{.}\PYG{n}{ttest\PYGZus{}ind}\PYG{p}{(} \PYG{n}{sample1}\PYG{p}{,} \PYG{n}{sample2} \PYG{p}{)}
\PYG{n}{reject\PYGZus{}H0} \PYG{o}{=} \PYG{n}{p\PYGZus{}value} \PYG{o}{\PYGZlt{}} \PYG{n}{α}
\PYG{n}{α}\PYG{p}{,} \PYG{n}{p\PYGZus{}value}\PYG{p}{,} \PYG{n}{reject\PYGZus{}H0}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(0.05, 0.02815503832602318, True)
\end{sphinxVerbatim}

The output above says that the two samples DO give us enough information to reject the null hypothesis.  So the data suggest that the two samples have different means.


\bigskip\hrule\bigskip



\section{Linear Regression}
\label{\detokenize{GB213-review-in-Python:linear-regression}}

\subsection{Creating a linear model of data}
\label{\detokenize{GB213-review-in-Python:creating-a-linear-model-of-data}}
Normally you would have data that you wanted to model.  But in this example notebook, I have to make up some data first.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(} \PYG{p}{\PYGZob{}}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{height}\PYG{l+s+s2}{\PYGZdq{}} \PYG{p}{:} \PYG{p}{[} \PYG{l+m+mi}{393}\PYG{p}{,} \PYG{l+m+mi}{453}\PYG{p}{,} \PYG{l+m+mi}{553}\PYG{p}{,} \PYG{l+m+mi}{679}\PYG{p}{,} \PYG{l+m+mi}{729}\PYG{p}{,} \PYG{l+m+mi}{748}\PYG{p}{,} \PYG{l+m+mi}{817} \PYG{p}{]}\PYG{p}{,}  \PYG{c+c1}{\PYGZsh{} completely made up}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{width}\PYG{l+s+s2}{\PYGZdq{}}  \PYG{p}{:} \PYG{p}{[}  \PYG{l+m+mi}{24}\PYG{p}{,}  \PYG{l+m+mi}{25}\PYG{p}{,}  \PYG{l+m+mi}{27}\PYG{p}{,}  \PYG{l+m+mi}{36}\PYG{p}{,}  \PYG{l+m+mi}{55}\PYG{p}{,}  \PYG{l+m+mi}{68}\PYG{p}{,}  \PYG{l+m+mi}{84} \PYG{p}{]}   \PYG{c+c1}{\PYGZsh{} also totally pretend}
\PYG{p}{\PYGZcb{}} \PYG{p}{)}
\end{sphinxVerbatim}

As with all the content of this document, the assumptions required to make the technique applicable are not covered in detail, but in this case we at least review them briefly.  To ensure that linear regression is applicable, one should verify:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
We have two columns of numerical data of the same length.

\item {} 
We have made a scatter plot and observed a seeming linear relationship.

\item {} 
We know that there is no autocorrelation.

\item {} 
We will check later that the residuals are normally distributed.

\item {} 
We will check later that the residuals are homoscedastic.

\end{enumerate}

To create a linear model, use \sphinxcode{\sphinxupquote{scipy}} as follows.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{model} \PYG{o}{=} \PYG{n}{stats}\PYG{o}{.}\PYG{n}{linregress}\PYG{p}{(} \PYG{n}{df}\PYG{o}{.}\PYG{n}{height}\PYG{p}{,} \PYG{n}{df}\PYG{o}{.}\PYG{n}{width} \PYG{p}{)}
\PYG{n}{model}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
LinregressResult(slope=0.1327195637885226, intercept=\PYGZhy{}37.32141898334582, rvalue=0.8949574425541466, pvalue=0.006486043236692156, stderr=0.029588975845594334)
\end{sphinxVerbatim}

A linear model is usually written like so:
\begin{equation*}
\begin{split} y = \beta_0 + \beta_1 x \end{split}
\end{equation*}
The slope is \(\beta_1\) and the intercept is \(\beta_0\).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{β0} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{intercept}
\PYG{n}{β1} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{slope}
\PYG{n}{β0}\PYG{p}{,} \PYG{n}{β1}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(\PYGZhy{}37.32141898334582, 0.1327195637885226)
\end{sphinxVerbatim}

From the output above, our model would therefore be the following (with some rounding for simplicity):
\begin{equation*}
\begin{split} y = -37.32 + 0.132x \end{split}
\end{equation*}
To know how good it is, we often ask about the \(R^2\) value.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{R} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{rvalue}
\PYG{n}{R}\PYG{p}{,} \PYG{n}{R}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(0.8949574425541466, 0.8009488239830586)
\end{sphinxVerbatim}

In this case, \(R^2\) would be approximately \(0.895^2\), or about \(0.801\).  Thus our model explains about 80.1\% of the variability in the data.


\subsection{Visualizing the model}
\label{\detokenize{GB213-review-in-Python:visualizing-the-model}}
The Seaborn visualization package provides a handy tool for making scatterplots with linear models overlaid.  The light blue shading is a confidence band we will not cover.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{lmplot}\PYG{p}{(} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{height}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{width}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{df} \PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{GB213-review-in-Python_45_0}.png}


\bigskip\hrule\bigskip



\section{Other Topics}
\label{\detokenize{GB213-review-in-Python:other-topics}}

\subsection{ANOVA}
\label{\detokenize{GB213-review-in-Python:anova}}
Analysis of variance is an optional topic your GB213 class may or may not have covered, depending on scheduling and instructor choices.  If you covered it in GB213 and would like to see how to do it in Python, check out \sphinxhref{https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.f\_oneway.html}{the Scipy documentation for \sphinxcode{\sphinxupquote{f\_oneway}}}.


\subsection{\protect\(\chi^2\protect\) Tests}
\label{\detokenize{GB213-review-in-Python:chi-2-tests}}
Chi\sphinxhyphen{}squared (\(\chi^2\)) tests are another optional GB213 topic that your class may or may not have covered.  If you are familiar with it and would like to see how to do it in Python, check out \sphinxhref{https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chisquare.html}{the Scipy documentation for \sphinxcode{\sphinxupquote{chisquare}}}.







\renewcommand{\indexname}{Index}
\printindex
\end{document}